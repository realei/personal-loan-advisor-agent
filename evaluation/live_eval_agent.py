"""
å®æ—¶è¯„ä¼°Agent - ä½œä¸ºå·¥å…·é›†æˆåˆ°ä¸»Agentä¸­
å¯ä»¥å®æ—¶è¯„ä¼°Agentçš„è¡¨ç°å¹¶æä¾›åé¦ˆ
"""

from typing import Dict, List, Any, Optional
from dataclasses import dataclass, asdict
from enum import Enum
import json
from datetime import datetime
from pymongo import MongoClient
from deepeval.metrics import (
    AnswerRelevancyMetric,
    FaithfulnessMetric,
    ContextualRelevancyMetric
)
from deepeval.test_case import LLMTestCase
from agno.tools import tool
from evaluation.mongodb_storage import (
    EvaluationStorage,
    LiveEvalResult,
    MetricResult,
    MetricStatus,
    EvalType
)


# ============= è¯„ä¼°å·¥å…·å®šä¹‰ =============

@tool(name="evaluate_last_response", show_result=True)
def evaluate_last_response(
    session_id: Optional[str] = None,
    metric_types: List[str] = ["relevancy", "faithfulness"]
) -> str:
    """
    è¯„ä¼°æœ€è¿‘ä¸€æ¬¡å“åº”çš„è´¨é‡

    Args:
        session_id: ä¼šè¯IDï¼Œå¦‚æœä¸æä¾›åˆ™ä½¿ç”¨å½“å‰ä¼šè¯
        metric_types: è¦è¯„ä¼°çš„æŒ‡æ ‡ç±»å‹åˆ—è¡¨

    Returns:
        è¯„ä¼°æŠ¥å‘Šçš„æ ¼å¼åŒ–å­—ç¬¦ä¸²
    """
    evaluator = LiveEvaluator()

    # è·å–æœ€è¿‘çš„å“åº”
    last_run = evaluator.get_last_run(session_id)

    if not last_run:
        return "âŒ æ— æ³•æ‰¾åˆ°æœ€è¿‘çš„å“åº”æ•°æ®"

    # æ‰§è¡Œè¯„ä¼°
    results = evaluator.evaluate_run(last_run, metric_types)

    # æ ¼å¼åŒ–æŠ¥å‘Šï¼ˆåŒ…å«ä¿å­˜åˆ°MongoDBï¼‰
    return evaluator.format_report(results, last_run)


@tool(name="evaluate_session_quality", show_result=True)
def evaluate_session_quality(
    session_id: Optional[str] = None,
    include_metrics: bool = True,
    include_tools: bool = True
) -> str:
    """
    è¯„ä¼°æ•´ä¸ªä¼šè¯çš„è´¨é‡

    Args:
        session_id: ä¼šè¯ID
        include_metrics: æ˜¯å¦åŒ…å«æ€§èƒ½æŒ‡æ ‡
        include_tools: æ˜¯å¦åŒ…å«å·¥å…·ä½¿ç”¨åˆ†æ

    Returns:
        ä¼šè¯è´¨é‡è¯„ä¼°æŠ¥å‘Š
    """
    evaluator = LiveEvaluator()

    # è·å–ä¼šè¯æ•°æ®
    session_data = evaluator.get_session_data(session_id)

    if not session_data:
        return "âŒ æ— æ³•æ‰¾åˆ°ä¼šè¯æ•°æ®"

    # æ‰§è¡Œä¼šè¯çº§è¯„ä¼°
    report = evaluator.evaluate_session(
        session_data,
        include_metrics=include_metrics,
        include_tools=include_tools
    )

    return report


@tool(name="query_evaluation_history", show_result=True)
def query_evaluation_history(
    hours: int = 24,
    eval_type: str = "all",
    limit: int = 10
) -> str:
    """
    æŸ¥è¯¢è¯„ä¼°å†å²è®°å½•

    Args:
        hours: æŸ¥çœ‹æœ€è¿‘Nå°æ—¶çš„è®°å½•
        eval_type: è¯„ä¼°ç±»å‹ (all/ci/live)
        limit: è¿”å›è®°å½•æ•°é‡é™åˆ¶

    Returns:
        è¯„ä¼°å†å²æŠ¥å‘Š
    """
    evaluator = LiveEvaluator()
    storage = evaluator.storage

    report = "## ğŸ“œ è¯„ä¼°å†å²æŸ¥è¯¢\n\n"
    report += f"æ—¶é—´èŒƒå›´: æœ€è¿‘{hours}å°æ—¶\n"
    report += f"ç±»å‹: {eval_type}\n\n"

    # æŸ¥è¯¢å®æ—¶è¯„ä¼°
    if eval_type in ["all", "live"]:
        live_results = storage.get_recent_live_results(hours=hours, limit=limit)
        report += f"### å®æ—¶è¯„ä¼°è®°å½• ({len(live_results)}æ¡)\n\n"

        for i, result in enumerate(live_results[:5], 1):
            report += f"{i}. **è¯„ä¼°ID**: {result.get('eval_id', 'N/A')[:8]}\n"
            report += f"   - æ—¶é—´: {result.get('evaluated_at')}\n"
            report += f"   - æ€»ä½“è¯„åˆ†: {result.get('overall_score', 0):.2%}\n"
            report += f"   - Token: {result.get('total_tokens', 'N/A')}\n"

            # æ˜¾ç¤ºæŒ‡æ ‡ç»“æœ
            if result.get('metrics_results'):
                report += "   - æŒ‡æ ‡:\n"
                for metric in result['metrics_results'][:3]:
                    status = "âœ…" if metric.get('passed') else "âŒ"
                    report += f"     * {metric.get('metric_name')}: {status} {metric.get('score', 0):.2%}\n"
            report += "\n"

    # æŸ¥è¯¢CIæµ‹è¯•
    if eval_type in ["all", "ci"]:
        ci_results = storage.get_latest_ci_results(limit=limit)
        report += f"### CIæµ‹è¯•è®°å½• ({len(ci_results)}æ¡)\n\n"

        for i, result in enumerate(ci_results[:3], 1):
            report += f"{i}. **æµ‹è¯•è¿è¡Œ**: {result.get('test_run_id', 'N/A')[:8]}\n"
            report += f"   - æ—¶é—´: {result.get('started_at')}\n"
            report += f"   - åˆ†æ”¯: {result.get('git_branch', 'unknown')}\n"
            report += f"   - é€šè¿‡ç‡: {result.get('passed_tests', 0)}/{result.get('total_tests', 0)}\n"
            report += f"   - è€—æ—¶: {result.get('duration_seconds', 0):.1f}ç§’\n\n"

    # è·å–ç»Ÿè®¡ä¿¡æ¯
    stats = storage.get_metrics_statistics()
    if stats:
        report += "### ğŸ“Š æŒ‡æ ‡ç»Ÿè®¡\n\n"
        for metric_name, metric_stats in list(stats.items())[:5]:
            report += f"- **{metric_name}**:\n"
            report += f"  - å¹³å‡åˆ†: {metric_stats['avg_score']:.2%}\n"
            report += f"  - é€šè¿‡ç‡: {metric_stats['pass_rate']:.1%}\n"

    return report


@tool(name="benchmark_agent_performance", show_result=True)
def benchmark_agent_performance(
    time_range_hours: int = 24,
    compare_with_baseline: bool = True
) -> str:
    """
    å¯¹Agentè¿›è¡Œæ€§èƒ½åŸºå‡†æµ‹è¯•

    Args:
        time_range_hours: åˆ†æçš„æ—¶é—´èŒƒå›´ï¼ˆå°æ—¶ï¼‰
        compare_with_baseline: æ˜¯å¦ä¸åŸºå‡†çº¿æ¯”è¾ƒ

    Returns:
        æ€§èƒ½åŸºå‡†æµ‹è¯•æŠ¥å‘Š
    """
    evaluator = LiveEvaluator()

    # è·å–æŒ‡å®šæ—¶é—´èŒƒå›´çš„æ•°æ®
    benchmark_data = evaluator.collect_benchmark_data(time_range_hours)

    # ç”ŸæˆåŸºå‡†æŠ¥å‘Š
    report = evaluator.generate_benchmark_report(
        benchmark_data,
        compare_with_baseline=compare_with_baseline
    )

    return report


# ============= è¯„ä¼°å™¨å®ç° =============

@dataclass
class RunData:
    """è¿è¡Œæ•°æ®"""
    session_id: str
    run_id: str
    input: str
    output: str
    tool_calls: List[Dict]
    tool_responses: List[str]
    metrics: Dict[str, Any]
    timestamp: datetime


@dataclass
class EvaluationScore:
    """è¯„ä¼°åˆ†æ•°"""
    metric_name: str
    score: float
    passed: bool
    details: Optional[str] = None


class LiveEvaluator:
    """å®æ—¶è¯„ä¼°å™¨"""

    def __init__(self, mongodb_url: str = "mongodb://admin:password123@localhost:27017/",
                 db_name: str = "loan_advisor"):
        self.client = MongoClient(mongodb_url)
        self.db = self.client[db_name]
        self.sessions_collection = self.db["agno_sessions"]

        # åˆå§‹åŒ–å­˜å‚¨ç®¡ç†å™¨
        self.storage = EvaluationStorage(mongodb_url, db_name)

        # åˆå§‹åŒ–DeepEvalæŒ‡æ ‡
        self.metrics = {
            "relevancy": AnswerRelevancyMetric(threshold=0.7),
            "faithfulness": FaithfulnessMetric(threshold=0.8),
            "contextual": ContextualRelevancyMetric(threshold=0.7)
        }

        # åŸºå‡†çº¿å®šä¹‰
        self.baseline = {
            "relevancy": 0.75,
            "faithfulness": 0.80,
            "tool_accuracy": 0.90,
            "avg_response_time": 3.0,
            "avg_tokens": 2000
        }

    def get_last_run(self, session_id: Optional[str] = None) -> Optional[RunData]:
        """è·å–æœ€è¿‘çš„è¿è¡Œæ•°æ®"""
        filter_query = {}
        if session_id:
            filter_query["session_id"] = session_id

        # è·å–æœ€æ–°çš„ä¼šè¯
        session = self.sessions_collection.find_one(
            filter_query,
            sort=[("updated_at", -1)]
        )

        if not session or not session.get("runs"):
            return None

        # è·å–æœ€åä¸€ä¸ªrun
        last_run = session["runs"][-1]

        # æå–æ•°æ®
        tool_calls = []
        tool_responses = []

        for msg in last_run.get("messages", []):
            if msg.get("role") == "assistant" and msg.get("tool_calls"):
                tool_calls.extend(msg["tool_calls"])
            elif msg.get("role") == "tool":
                tool_responses.append(msg.get("content", ""))

        return RunData(
            session_id=session["session_id"],
            run_id=last_run.get("run_id"),
            input=last_run.get("input", ""),
            output=last_run.get("content", ""),
            tool_calls=tool_calls,
            tool_responses=tool_responses,
            metrics=last_run.get("metrics", {}),
            timestamp=datetime.fromtimestamp(last_run.get("created_at", 0))
        )

    def evaluate_run(self, run_data: RunData, metric_types: List[str]) -> List[EvaluationScore]:
        """è¯„ä¼°å•ä¸ªè¿è¡Œ"""
        results = []

        # åˆ›å»ºDeepEvalæµ‹è¯•ç”¨ä¾‹
        test_case = LLMTestCase(
            input=run_data.input,
            actual_output=run_data.output,
            context=run_data.tool_responses if run_data.tool_responses else None
        )

        # æ‰§è¡Œé€‰å®šçš„è¯„ä¼°
        for metric_type in metric_types:
            if metric_type in self.metrics:
                metric = self.metrics[metric_type]
                metric.measure(test_case)

                results.append(EvaluationScore(
                    metric_name=metric_type,
                    score=metric.score,
                    passed=metric.is_successful(),
                    details=metric.reason
                ))

        # æ·»åŠ è‡ªå®šä¹‰è¯„ä¼°
        if "tool_accuracy" in metric_types:
            tool_score = self._evaluate_tool_accuracy(run_data)
            results.append(tool_score)

        return results

    def _evaluate_tool_accuracy(self, run_data: RunData) -> EvaluationScore:
        """è¯„ä¼°å·¥å…·ä½¿ç”¨å‡†ç¡®æ€§"""
        # ç®€åŒ–çš„å·¥å…·å‡†ç¡®æ€§è¯„ä¼°
        expected_tools_map = {
            "è´·æ¬¾": ["check_loan_eligibility", "calculate_loan_payment"],
            "æœˆä¾›": ["calculate_loan_payment"],
            "èµ„æ ¼": ["check_loan_eligibility"]
        }

        actual_tools = [
            tc["function"]["name"]
            for tc in run_data.tool_calls
            if "function" in tc
        ]

        # æ ¹æ®è¾“å…¥åˆ¤æ–­æœŸæœ›çš„å·¥å…·
        expected_tools = []
        for keyword, tools in expected_tools_map.items():
            if keyword in run_data.input:
                expected_tools.extend(tools)

        expected_tools = list(set(expected_tools))

        # è®¡ç®—å‡†ç¡®ç‡
        if not expected_tools:
            score = 1.0
        else:
            correct = len(set(actual_tools) & set(expected_tools))
            score = correct / len(expected_tools)

        return EvaluationScore(
            metric_name="tool_accuracy",
            score=score,
            passed=score >= 0.8,
            details=f"Used: {actual_tools}, Expected: {expected_tools}"
        )

    def format_report(self, results: List[EvaluationScore], run_data: Optional[RunData] = None) -> str:
        """æ ¼å¼åŒ–è¯„ä¼°æŠ¥å‘Šå¹¶ä¿å­˜åˆ°MongoDB"""
        report = "## ğŸ“Š å“åº”è´¨é‡è¯„ä¼°æŠ¥å‘Š\n\n"

        # æ€»ä½“è¯„åˆ†
        avg_score = sum(r.score for r in results) / len(results) if results else 0
        report += f"**æ€»ä½“è¯„åˆ†**: {avg_score:.2%}\n\n"

        # è¯¦ç»†æŒ‡æ ‡
        report += "### è¯¦ç»†æŒ‡æ ‡:\n"
        metrics_results = []
        recommendations = []

        for result in results:
            status = "âœ…" if result.passed else "âŒ"
            report += f"- **{result.metric_name}**: {status} {result.score:.2%}\n"
            if result.details:
                report += f"  - {result.details}\n"

            # è½¬æ¢ä¸ºMetricResultç”¨äºå­˜å‚¨
            metrics_results.append(MetricResult(
                metric_name=result.metric_name,
                metric_type="deepeval" if result.metric_name in ["relevancy", "faithfulness", "contextual"] else "custom",
                score=result.score,
                threshold=0.7,  # é»˜è®¤é˜ˆå€¼
                status=MetricStatus.PASSED if result.passed else MetricStatus.FAILED,
                passed=result.passed,
                reason=result.details
            ))

            # æ”¶é›†å»ºè®®
            if not result.passed:
                if result.metric_name == "relevancy":
                    recommendations.append("æé«˜å›ç­”ä¸é—®é¢˜çš„ç›¸å…³æ€§")
                elif result.metric_name == "faithfulness":
                    recommendations.append("ç¡®ä¿å›ç­”åŸºäºå·¥å…·è¿”å›çš„äº‹å®")
                elif result.metric_name == "tool_accuracy":
                    recommendations.append("ä¼˜åŒ–å·¥å…·é€‰æ‹©ç­–ç•¥")

        # å»ºè®®
        report += "\n### ğŸ’¡ æ”¹è¿›å»ºè®®:\n"
        for rec in recommendations:
            report += f"- {rec}\n"

        # ä¿å­˜åˆ°MongoDB
        if run_data:
            live_result = LiveEvalResult(
                session_id=run_data.session_id,
                run_id=run_data.run_id,
                input_text=run_data.input,
                output_text=run_data.output,
                context=run_data.tool_responses,
                metrics_results=metrics_results,
                overall_score=avg_score,
                tool_calls=run_data.tool_calls,
                response_time=run_data.metrics.get("duration") if run_data.metrics else None,
                total_tokens=run_data.metrics.get("total_tokens") if run_data.metrics else None,
                recommendations=recommendations
            )

            eval_id = self.storage.save_live_result(live_result)
            report += f"\n\nğŸ“ è¯„ä¼°ç»“æœå·²ä¿å­˜: {eval_id}\n"

        return report

    def get_session_data(self, session_id: Optional[str]) -> Optional[Dict]:
        """è·å–ä¼šè¯æ•°æ®"""
        filter_query = {}
        if session_id:
            filter_query["session_id"] = session_id

        return self.sessions_collection.find_one(
            filter_query,
            sort=[("updated_at", -1)]
        )

    def evaluate_session(self, session_data: Dict,
                        include_metrics: bool = True,
                        include_tools: bool = True) -> str:
        """è¯„ä¼°æ•´ä¸ªä¼šè¯"""
        report = "## ğŸ“ˆ ä¼šè¯è´¨é‡åˆ†ææŠ¥å‘Š\n\n"

        runs = session_data.get("runs", [])
        report += f"**ä¼šè¯ID**: {session_data.get('session_id')}\n"
        report += f"**äº¤äº’æ¬¡æ•°**: {len(runs)}\n\n"

        if include_metrics and runs:
            # æ€§èƒ½æŒ‡æ ‡åˆ†æ
            total_tokens = sum(r.get("metrics", {}).get("total_tokens", 0) for r in runs)
            avg_response_time = sum(r.get("metrics", {}).get("duration", 0) for r in runs) / len(runs)

            report += "### âš¡ æ€§èƒ½æŒ‡æ ‡:\n"
            report += f"- æ€»Tokenæ¶ˆè€—: {total_tokens}\n"
            report += f"- å¹³å‡å“åº”æ—¶é—´: {avg_response_time:.2f}ç§’\n"
            report += f"- Tokenæ•ˆç‡: {total_tokens / len(runs):.0f} tokens/äº¤äº’\n\n"

        if include_tools and runs:
            # å·¥å…·ä½¿ç”¨åˆ†æ
            all_tools = []
            for run in runs:
                for msg in run.get("messages", []):
                    if msg.get("role") == "assistant" and msg.get("tool_calls"):
                        for tc in msg["tool_calls"]:
                            if "function" in tc:
                                all_tools.append(tc["function"]["name"])

            if all_tools:
                tool_counts = {}
                for tool in all_tools:
                    tool_counts[tool] = tool_counts.get(tool, 0) + 1

                report += "### ğŸ”§ å·¥å…·ä½¿ç”¨ç»Ÿè®¡:\n"
                for tool, count in sorted(tool_counts.items(), key=lambda x: x[1], reverse=True):
                    report += f"- {tool}: {count}æ¬¡\n"

        return report

    def collect_benchmark_data(self, time_range_hours: int) -> Dict:
        """æ”¶é›†åŸºå‡†æµ‹è¯•æ•°æ®"""
        from datetime import datetime, timedelta

        cutoff_time = datetime.now() - timedelta(hours=time_range_hours)
        cutoff_timestamp = cutoff_time.timestamp()

        # æŸ¥è¯¢æŒ‡å®šæ—¶é—´èŒƒå›´çš„ä¼šè¯
        sessions = self.sessions_collection.find({
            "updated_at": {"$gte": cutoff_timestamp}
        })

        # èšåˆæ•°æ®
        data = {
            "total_sessions": 0,
            "total_runs": 0,
            "total_tokens": 0,
            "total_duration": 0,
            "tool_usage": {},
            "scores": []
        }

        for session in sessions:
            data["total_sessions"] += 1
            for run in session.get("runs", []):
                data["total_runs"] += 1
                metrics = run.get("metrics", {})
                data["total_tokens"] += metrics.get("total_tokens", 0)
                data["total_duration"] += metrics.get("duration", 0)

        return data

    def generate_benchmark_report(self, benchmark_data: Dict,
                                 compare_with_baseline: bool = True) -> str:
        """ç”ŸæˆåŸºå‡†æµ‹è¯•æŠ¥å‘Š"""
        report = "## ğŸ¯ Agentæ€§èƒ½åŸºå‡†æµ‹è¯•æŠ¥å‘Š\n\n"

        if benchmark_data["total_runs"] == 0:
            return report + "âŒ æ²¡æœ‰è¶³å¤Ÿçš„æ•°æ®è¿›è¡ŒåŸºå‡†æµ‹è¯•\n"

        # è®¡ç®—å¹³å‡å€¼
        avg_tokens = benchmark_data["total_tokens"] / benchmark_data["total_runs"]
        avg_duration = benchmark_data["total_duration"] / benchmark_data["total_runs"]

        report += "### ğŸ“Š æ€§èƒ½ç»Ÿè®¡:\n"
        report += f"- æ€»ä¼šè¯æ•°: {benchmark_data['total_sessions']}\n"
        report += f"- æ€»äº¤äº’æ¬¡æ•°: {benchmark_data['total_runs']}\n"
        report += f"- å¹³å‡Tokenæ¶ˆè€—: {avg_tokens:.0f}\n"
        report += f"- å¹³å‡å“åº”æ—¶é—´: {avg_duration:.2f}ç§’\n\n"

        if compare_with_baseline:
            report += "### ğŸ“ˆ ä¸åŸºå‡†çº¿å¯¹æ¯”:\n"

            # Tokenå¯¹æ¯”
            token_diff = (avg_tokens - self.baseline["avg_tokens"]) / self.baseline["avg_tokens"] * 100
            token_status = "ğŸŸ¢" if avg_tokens <= self.baseline["avg_tokens"] else "ğŸ”´"
            report += f"- Tokenæ¶ˆè€—: {token_status} {token_diff:+.1f}% (åŸºå‡†: {self.baseline['avg_tokens']})\n"

            # å“åº”æ—¶é—´å¯¹æ¯”
            time_diff = (avg_duration - self.baseline["avg_response_time"]) / self.baseline["avg_response_time"] * 100
            time_status = "ğŸŸ¢" if avg_duration <= self.baseline["avg_response_time"] else "ğŸ”´"
            report += f"- å“åº”æ—¶é—´: {time_status} {time_diff:+.1f}% (åŸºå‡†: {self.baseline['avg_response_time']}s)\n"

        return report


# ============= é›†æˆåˆ°Agent =============

def create_eval_tools():
    """åˆ›å»ºè¯„ä¼°å·¥å…·åˆ—è¡¨ï¼Œå¯ä»¥æ·»åŠ åˆ°ä¸»Agentä¸­"""
    return [
        evaluate_last_response,
        evaluate_session_quality,
        query_evaluation_history,
        benchmark_agent_performance
    ]


# ============= ç‹¬ç«‹è¿è¡Œç¤ºä¾‹ =============

if __name__ == "__main__":
    # ç¤ºä¾‹ï¼šè¯„ä¼°æœ€è¿‘çš„å“åº”
    result = evaluate_last_response(metric_types=["relevancy", "faithfulness", "tool_accuracy"])
    print(result)

    # ç¤ºä¾‹ï¼šè¯„ä¼°ä¼šè¯è´¨é‡
    session_report = evaluate_session_quality(include_metrics=True, include_tools=True)
    print(session_report)

    # ç¤ºä¾‹ï¼šåŸºå‡†æµ‹è¯•
    benchmark = benchmark_agent_performance(time_range_hours=24)
    print(benchmark)