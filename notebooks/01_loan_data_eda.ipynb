{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ¦ è´·æ¬¾å®¡æ‰¹æ•°æ®é›† - å®Œæ•´æŽ¢ç´¢æ€§æ•°æ®åˆ†æž (EDA)\n",
    "\n",
    "## ðŸ“‹ ç›®æ ‡\n",
    "1. ç†è§£æ•°æ®é›†ç»“æž„å’Œè´¨é‡\n",
    "2. åˆ†æžå„ä¸ªç‰¹å¾çš„åˆ†å¸ƒ\n",
    "3. æŽ¢ç´¢ç‰¹å¾ä¸Žè´·æ¬¾æ‰¹å‡†çš„å…³ç³»\n",
    "4. å‘çŽ°ä¸šåŠ¡æ´žå¯Ÿ\n",
    "5. ä¸ºæœºå™¨å­¦ä¹ å»ºæ¨¡åšå‡†å¤‡\n",
    "\n",
    "## ðŸ“Š æ•°æ®æ¥æº\n",
    "- **Dataset**: Loan Approval Prediction Dataset\n",
    "- **Source**: Kaggle (architsharma01)\n",
    "- **Size**: ~4,000-45,000 records\n",
    "- **Task**: Binary Classification (Approved/Rejected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯¼å…¥å¿…è¦çš„åº“\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# è®¾ç½®å¯è§†åŒ–é£Žæ ¼\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "# ä¸­æ–‡å­—ä½“æ”¯æŒ (å¦‚æžœéœ€è¦)\n",
    "# plt.rcParams['font.sans-serif'] = ['Arial Unicode MS']  # Mac\n",
    "# plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "print(\"âœ… åº“å¯¼å…¥æˆåŠŸï¼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“‚ 1. æ•°æ®åŠ è½½ä¸ŽåŸºæœ¬ä¿¡æ¯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ•°æ®è·¯å¾„\n",
    "data_path = Path(\"../data/raw\")\n",
    "\n",
    "# æŸ¥æ‰¾CSVæ–‡ä»¶\n",
    "csv_files = list(data_path.glob(\"*.csv\"))\n",
    "print(f\"æ‰¾åˆ°çš„CSVæ–‡ä»¶: {csv_files}\")\n",
    "\n",
    "if not csv_files:\n",
    "    print(\"âš ï¸ æœªæ‰¾åˆ°æ•°æ®æ–‡ä»¶ï¼\")\n",
    "    print(\"è¯·æŒ‰ç…§ DATA_SETUP.md çš„æŒ‡å¼•ä¸‹è½½æ•°æ®é›†\")\n",
    "    print(\"\\nå¿«é€Ÿä¸‹è½½å‘½ä»¤:\")\n",
    "    print(\"  python scripts/download_data.py\")\n",
    "else:\n",
    "    # åŠ è½½ç¬¬ä¸€ä¸ªCSVæ–‡ä»¶\n",
    "    df = pd.read_csv(csv_files[0])\n",
    "    print(f\"\\nâœ… æˆåŠŸåŠ è½½æ•°æ®: {csv_files[0].name}\")\n",
    "    print(f\"ðŸ“Š æ•°æ®é›†å¤§å°: {df.shape[0]} è¡Œ, {df.shape[1]} åˆ—\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æŸ¥çœ‹å‰å‡ è¡Œæ•°æ®\n",
    "print(\"ðŸ“‹ å‰5è¡Œæ•°æ®é¢„è§ˆ:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ•°æ®é›†åŸºæœ¬ä¿¡æ¯\n",
    "print(\"â„¹ï¸ æ•°æ®é›†è¯¦ç»†ä¿¡æ¯:\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ•°å€¼åž‹ç‰¹å¾çš„ç»Ÿè®¡æè¿°\n",
    "print(\"ðŸ“ˆ æ•°å€¼åž‹ç‰¹å¾ç»Ÿè®¡æè¿°:\")\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åˆ†ç±»åž‹ç‰¹å¾çš„ç»Ÿè®¡æè¿°\n",
    "print(\"ðŸ“Š åˆ†ç±»åž‹ç‰¹å¾ç»Ÿè®¡æè¿°:\")\n",
    "df.describe(include='object')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ” 2. æ•°æ®è´¨é‡æ£€æŸ¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç¼ºå¤±å€¼åˆ†æž\n",
    "print(\"ðŸ•µï¸ ç¼ºå¤±å€¼æ£€æŸ¥:\")\n",
    "missing_data = pd.DataFrame({\n",
    "    'åˆ—å': df.columns,\n",
    "    'ç¼ºå¤±å€¼æ•°é‡': df.isnull().sum().values,\n",
    "    'ç¼ºå¤±å€¼ç™¾åˆ†æ¯”': (df.isnull().sum().values / len(df) * 100).round(2)\n",
    "})\n",
    "missing_data = missing_data[missing_data['ç¼ºå¤±å€¼æ•°é‡'] > 0].sort_values('ç¼ºå¤±å€¼æ•°é‡', ascending=False)\n",
    "\n",
    "if len(missing_data) > 0:\n",
    "    print(missing_data.to_string(index=False))\n",
    "else:\n",
    "    print(\"âœ… æ²¡æœ‰ç¼ºå¤±å€¼ï¼\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# é‡å¤å€¼æ£€æŸ¥\n",
    "duplicates = df.duplicated().sum()\n",
    "print(f\"\\nðŸ“ é‡å¤è¡Œæ•°: {duplicates}\")\n",
    "if duplicates > 0:\n",
    "    print(f\"âš ï¸ é‡å¤è¡Œå æ¯”: {(duplicates/len(df)*100):.2f}%\")\n",
    "else:\n",
    "    print(\"âœ… æ²¡æœ‰é‡å¤è¡Œï¼\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ•°æ®ç±»åž‹æ£€æŸ¥\n",
    "print(\"\\nðŸ”¤ å„åˆ—æ•°æ®ç±»åž‹:\")\n",
    "data_types = pd.DataFrame({\n",
    "    'åˆ—å': df.columns,\n",
    "    'æ•°æ®ç±»åž‹': df.dtypes.values,\n",
    "    'å”¯ä¸€å€¼æ•°é‡': [df[col].nunique() for col in df.columns],\n",
    "    'ç¤ºä¾‹å€¼': [str(df[col].iloc[0]) for col in df.columns]\n",
    "})\n",
    "print(data_types.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ 3. ç›®æ ‡å˜é‡åˆ†æž - Loan Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å‡è®¾ç›®æ ‡å˜é‡æ˜¯ 'loan_status' æˆ–ç±»ä¼¼çš„åˆ—å\n",
    "# æ ¹æ®å®žé™…æ•°æ®è°ƒæ•´åˆ—å\n",
    "target_col = None\n",
    "for possible_name in ['loan_status', 'Loan_Status', 'status', 'approval_status', 'approved']:\n",
    "    if possible_name in df.columns:\n",
    "        target_col = possible_name\n",
    "        break\n",
    "\n",
    "if target_col:\n",
    "    print(f\"ðŸŽ¯ ç›®æ ‡å˜é‡: {target_col}\")\n",
    "    print(f\"\\nç›®æ ‡å˜é‡åˆ†å¸ƒ:\")\n",
    "    target_counts = df[target_col].value_counts()\n",
    "    print(target_counts)\n",
    "    \n",
    "    # å¯è§†åŒ–\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # è®¡æ•°å›¾\n",
    "    target_counts.plot(kind='bar', ax=axes[0], color=['#2ecc71', '#e74c3c'])\n",
    "    axes[0].set_title(f'{target_col} Distribution (Count)', fontsize=14, fontweight='bold')\n",
    "    axes[0].set_xlabel('Status', fontsize=12)\n",
    "    axes[0].set_ylabel('Count', fontsize=12)\n",
    "    axes[0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # æ·»åŠ æ•°å€¼æ ‡ç­¾\n",
    "    for i, v in enumerate(target_counts):\n",
    "        axes[0].text(i, v + 50, str(v), ha='center', fontweight='bold')\n",
    "    \n",
    "    # é¥¼å›¾\n",
    "    target_pct = df[target_col].value_counts(normalize=True) * 100\n",
    "    axes[1].pie(target_pct, labels=target_pct.index, autopct='%1.1f%%',\n",
    "                colors=['#2ecc71', '#e74c3c'], startangle=90)\n",
    "    axes[1].set_title(f'{target_col} Distribution (Percentage)', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nâœ… è´·æ¬¾æ‰¹å‡†çŽ‡: {target_pct.iloc[0]:.2f}%\")\n",
    "    print(f\"âŒ è´·æ¬¾æ‹’ç»çŽ‡: {target_pct.iloc[1]:.2f}%\")\n",
    "    \n",
    "    # æ£€æŸ¥ç±»åˆ«ä¸å¹³è¡¡\n",
    "    imbalance_ratio = target_counts.max() / target_counts.min()\n",
    "    print(f\"\\nâš–ï¸ ç±»åˆ«ä¸å¹³è¡¡æ¯”ä¾‹: {imbalance_ratio:.2f}:1\")\n",
    "    if imbalance_ratio > 2:\n",
    "        print(\"âš ï¸ æ•°æ®é›†å­˜åœ¨ç±»åˆ«ä¸å¹³è¡¡ï¼Œå»ºè®®ä½¿ç”¨SMOTEæˆ–è°ƒæ•´ç±»åˆ«æƒé‡\")\n",
    "else:\n",
    "    print(\"âš ï¸ æœªæ‰¾åˆ°ç›®æ ‡å˜é‡åˆ—ï¼Œè¯·æ£€æŸ¥æ•°æ®é›†\")\n",
    "    print(f\"å¯ç”¨åˆ—: {df.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“Š 4. æ•°å€¼åž‹ç‰¹å¾åˆ†å¸ƒåˆ†æž"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¯†åˆ«æ•°å€¼åž‹åˆ—\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "print(f\"ðŸ“Š æ•°å€¼åž‹ç‰¹å¾ ({len(numeric_cols)}ä¸ª): {numeric_cols}\")\n",
    "\n",
    "# ä¸ºæ¯ä¸ªæ•°å€¼åž‹ç‰¹å¾åˆ›å»ºåˆ†å¸ƒå›¾\n",
    "if len(numeric_cols) > 0:\n",
    "    n_cols = 3\n",
    "    n_rows = (len(numeric_cols) + n_cols - 1) // n_cols\n",
    "    \n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(18, n_rows * 4))\n",
    "    axes = axes.flatten() if n_rows > 1 else [axes] if n_rows == 1 and n_cols == 1 else axes\n",
    "    \n",
    "    for idx, col in enumerate(numeric_cols):\n",
    "        if idx < len(axes):\n",
    "            axes[idx].hist(df[col].dropna(), bins=30, edgecolor='black', alpha=0.7)\n",
    "            axes[idx].set_title(f'{col} Distribution', fontweight='bold')\n",
    "            axes[idx].set_xlabel(col)\n",
    "            axes[idx].set_ylabel('Frequency')\n",
    "            \n",
    "            # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯\n",
    "            mean_val = df[col].mean()\n",
    "            median_val = df[col].median()\n",
    "            axes[idx].axvline(mean_val, color='red', linestyle='--', linewidth=2, label=f'Mean: {mean_val:.2f}')\n",
    "            axes[idx].axvline(median_val, color='green', linestyle='--', linewidth=2, label=f'Median: {median_val:.2f}')\n",
    "            axes[idx].legend()\n",
    "    \n",
    "    # éšè—å¤šä½™çš„å­å›¾\n",
    "    for idx in range(len(numeric_cols), len(axes)):\n",
    "        axes[idx].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç®±çº¿å›¾æ£€æµ‹å¼‚å¸¸å€¼\n",
    "if len(numeric_cols) > 0:\n",
    "    n_cols = 3\n",
    "    n_rows = (len(numeric_cols) + n_cols - 1) // n_cols\n",
    "    \n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(18, n_rows * 4))\n",
    "    axes = axes.flatten() if n_rows > 1 else [axes] if n_rows == 1 and n_cols == 1 else axes\n",
    "    \n",
    "    for idx, col in enumerate(numeric_cols):\n",
    "        if idx < len(axes):\n",
    "            sns.boxplot(data=df, y=col, ax=axes[idx], color='skyblue')\n",
    "            axes[idx].set_title(f'{col} - Outlier Detection', fontweight='bold')\n",
    "            axes[idx].set_ylabel(col)\n",
    "            \n",
    "            # è®¡ç®—å¼‚å¸¸å€¼æ•°é‡\n",
    "            Q1 = df[col].quantile(0.25)\n",
    "            Q3 = df[col].quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            outliers = df[(df[col] < Q1 - 1.5 * IQR) | (df[col] > Q3 + 1.5 * IQR)][col]\n",
    "            axes[idx].text(0, df[col].max(), f'Outliers: {len(outliers)}', \n",
    "                          ha='center', fontsize=10, bbox=dict(boxstyle='round', facecolor='wheat'))\n",
    "    \n",
    "    # éšè—å¤šä½™çš„å­å›¾\n",
    "    for idx in range(len(numeric_cols), len(axes)):\n",
    "        axes[idx].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ·ï¸ 5. åˆ†ç±»åž‹ç‰¹å¾åˆ†æž"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¯†åˆ«åˆ†ç±»åž‹åˆ—\n",
    "categorical_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "# æŽ’é™¤ç›®æ ‡å˜é‡\n",
    "if target_col and target_col in categorical_cols:\n",
    "    categorical_cols.remove(target_col)\n",
    "\n",
    "print(f\"ðŸ·ï¸ åˆ†ç±»åž‹ç‰¹å¾ ({len(categorical_cols)}ä¸ª): {categorical_cols}\")\n",
    "\n",
    "if len(categorical_cols) > 0:\n",
    "    n_cols = 2\n",
    "    n_rows = (len(categorical_cols) + n_cols - 1) // n_cols\n",
    "    \n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(16, n_rows * 5))\n",
    "    axes = axes.flatten() if n_rows > 1 else [axes] if n_rows == 1 and n_cols == 1 else axes\n",
    "    \n",
    "    for idx, col in enumerate(categorical_cols):\n",
    "        if idx < len(axes):\n",
    "            value_counts = df[col].value_counts()\n",
    "            value_counts.plot(kind='barh', ax=axes[idx], color=sns.color_palette('Set2'))\n",
    "            axes[idx].set_title(f'{col} Distribution', fontweight='bold', fontsize=12)\n",
    "            axes[idx].set_xlabel('Count', fontsize=10)\n",
    "            axes[idx].set_ylabel(col, fontsize=10)\n",
    "            \n",
    "            # æ·»åŠ æ•°å€¼æ ‡ç­¾\n",
    "            for i, v in enumerate(value_counts):\n",
    "                axes[idx].text(v + 10, i, str(v), va='center', fontweight='bold')\n",
    "    \n",
    "    # éšè—å¤šä½™çš„å­å›¾\n",
    "    for idx in range(len(categorical_cols), len(axes)):\n",
    "        axes[idx].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ”— 6. ç‰¹å¾ç›¸å…³æ€§åˆ†æž"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ•°å€¼åž‹ç‰¹å¾ç›¸å…³æ€§çƒ­å›¾\n",
    "if len(numeric_cols) > 1:\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    correlation_matrix = df[numeric_cols].corr()\n",
    "    \n",
    "    sns.heatmap(correlation_matrix, annot=True, fmt='.2f', cmap='coolwarm', \n",
    "                center=0, square=True, linewidths=1, cbar_kws={\"shrink\": 0.8})\n",
    "    plt.title('Feature Correlation Matrix', fontsize=16, fontweight='bold', pad=20)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # æ‰¾å‡ºé«˜åº¦ç›¸å…³çš„ç‰¹å¾å¯¹\n",
    "    print(\"\\nðŸ” é«˜åº¦ç›¸å…³çš„ç‰¹å¾å¯¹ (|r| > 0.7):\")\n",
    "    high_corr_pairs = []\n",
    "    for i in range(len(correlation_matrix.columns)):\n",
    "        for j in range(i+1, len(correlation_matrix.columns)):\n",
    "            if abs(correlation_matrix.iloc[i, j]) > 0.7:\n",
    "                high_corr_pairs.append((\n",
    "                    correlation_matrix.columns[i],\n",
    "                    correlation_matrix.columns[j],\n",
    "                    correlation_matrix.iloc[i, j]\n",
    "                ))\n",
    "    \n",
    "    if high_corr_pairs:\n",
    "        for feat1, feat2, corr in high_corr_pairs:\n",
    "            print(f\"  {feat1} <-> {feat2}: {corr:.3f}\")\n",
    "    else:\n",
    "        print(\"  æ²¡æœ‰å‘çŽ°é«˜åº¦ç›¸å…³çš„ç‰¹å¾å¯¹\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ 7. ç‰¹å¾ä¸Žç›®æ ‡å˜é‡çš„å…³ç³»åˆ†æž"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ•°å€¼åž‹ç‰¹å¾ä¸Žç›®æ ‡å˜é‡çš„å…³ç³»\n",
    "if target_col and len(numeric_cols) > 0:\n",
    "    n_cols = 3\n",
    "    n_rows = (len(numeric_cols) + n_cols - 1) // n_cols\n",
    "    \n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(18, n_rows * 4))\n",
    "    axes = axes.flatten() if n_rows > 1 else [axes] if n_rows == 1 and n_cols == 1 else axes\n",
    "    \n",
    "    for idx, col in enumerate(numeric_cols):\n",
    "        if idx < len(axes):\n",
    "            sns.boxplot(data=df, x=target_col, y=col, ax=axes[idx], palette='Set2')\n",
    "            axes[idx].set_title(f'{col} by {target_col}', fontweight='bold')\n",
    "            axes[idx].set_xlabel(target_col)\n",
    "            axes[idx].set_ylabel(col)\n",
    "            axes[idx].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # éšè—å¤šä½™çš„å­å›¾\n",
    "    for idx in range(len(numeric_cols), len(axes)):\n",
    "        axes[idx].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åˆ†ç±»åž‹ç‰¹å¾ä¸Žç›®æ ‡å˜é‡çš„å…³ç³»\n",
    "if target_col and len(categorical_cols) > 0:\n",
    "    n_cols = 2\n",
    "    n_rows = (len(categorical_cols) + n_cols - 1) // n_cols\n",
    "    \n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(16, n_rows * 5))\n",
    "    axes = axes.flatten() if n_rows > 1 else [axes] if n_rows == 1 and n_cols == 1 else axes\n",
    "    \n",
    "    for idx, col in enumerate(categorical_cols):\n",
    "        if idx < len(axes):\n",
    "            # åˆ›å»ºäº¤å‰è¡¨\n",
    "            ct = pd.crosstab(df[col], df[target_col], normalize='index') * 100\n",
    "            ct.plot(kind='bar', ax=axes[idx], stacked=False, \n",
    "                   color=['#2ecc71', '#e74c3c'], alpha=0.8)\n",
    "            axes[idx].set_title(f'{target_col} Rate by {col}', fontweight='bold')\n",
    "            axes[idx].set_xlabel(col)\n",
    "            axes[idx].set_ylabel('Percentage (%)')\n",
    "            axes[idx].legend(title=target_col)\n",
    "            axes[idx].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # éšè—å¤šä½™çš„å­å›¾\n",
    "    for idx in range(len(categorical_cols), len(axes)):\n",
    "        axes[idx].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ’¡ 8. ä¸šåŠ¡æ´žå¯Ÿåˆ†æž"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ ¹æ®å®žé™…åˆ—åè°ƒæ•´ä»¥ä¸‹åˆ†æž\n",
    "# è¿™é‡Œæä¾›é€šç”¨çš„ä¸šåŠ¡åˆ†æžæ¡†æž¶\n",
    "\n",
    "print(\"ðŸ’¡ ä¸šåŠ¡æ´žå¯Ÿåˆ†æž\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 1. è´·æ¬¾é‡‘é¢åˆ†æž\n",
    "loan_amount_cols = [col for col in df.columns if 'loan' in col.lower() and 'amount' in col.lower()]\n",
    "if loan_amount_cols:\n",
    "    loan_col = loan_amount_cols[0]\n",
    "    print(f\"\\n1ï¸âƒ£ è´·æ¬¾é‡‘é¢åˆ†æž ({loan_col}):\")\n",
    "    print(f\"   å¹³å‡è´·æ¬¾é¢: ${df[loan_col].mean():,.2f}\")\n",
    "    print(f\"   ä¸­ä½æ•°è´·æ¬¾é¢: ${df[loan_col].median():,.2f}\")\n",
    "    print(f\"   æœ€å¤§è´·æ¬¾é¢: ${df[loan_col].max():,.2f}\")\n",
    "    print(f\"   æœ€å°è´·æ¬¾é¢: ${df[loan_col].min():,.2f}\")\n",
    "\n",
    "# 2. æ”¶å…¥åˆ†æž\n",
    "income_cols = [col for col in df.columns if 'income' in col.lower()]\n",
    "if income_cols:\n",
    "    income_col = income_cols[0]\n",
    "    print(f\"\\n2ï¸âƒ£ æ”¶å…¥åˆ†æž ({income_col}):\")\n",
    "    print(f\"   å¹³å‡æ”¶å…¥: ${df[income_col].mean():,.2f}\")\n",
    "    print(f\"   ä¸­ä½æ•°æ”¶å…¥: ${df[income_col].median():,.2f}\")\n",
    "    \n",
    "    # å¦‚æžœåŒæ—¶æœ‰è´·æ¬¾é‡‘é¢ï¼Œè®¡ç®—è´·æ¬¾æ”¶å…¥æ¯”\n",
    "    if loan_amount_cols:\n",
    "        loan_to_income = df[loan_col] / df[income_col]\n",
    "        print(f\"   å¹³å‡è´·æ¬¾/æ”¶å…¥æ¯”: {loan_to_income.mean():.2f}\")\n",
    "\n",
    "# 3. ä¿¡ç”¨åˆ†æ•°åˆ†æž\n",
    "credit_cols = [col for col in df.columns if 'credit' in col.lower() or 'score' in col.lower()]\n",
    "if credit_cols:\n",
    "    credit_col = credit_cols[0]\n",
    "    print(f\"\\n3ï¸âƒ£ ä¿¡ç”¨åˆ†æ•°åˆ†æž ({credit_col}):\")\n",
    "    print(f\"   å¹³å‡ä¿¡ç”¨åˆ†: {df[credit_col].mean():.0f}\")\n",
    "    print(f\"   ä¸­ä½æ•°ä¿¡ç”¨åˆ†: {df[credit_col].median():.0f}\")\n",
    "    \n",
    "    # ä¿¡ç”¨åˆ†ç­‰çº§åˆ†å¸ƒ\n",
    "    def credit_category(score):\n",
    "        if pd.isna(score):\n",
    "            return 'Unknown'\n",
    "        elif score >= 750:\n",
    "            return 'Excellent (750+)'\n",
    "        elif score >= 700:\n",
    "            return 'Good (700-749)'\n",
    "        elif score >= 650:\n",
    "            return 'Fair (650-699)'\n",
    "        elif score >= 600:\n",
    "            return 'Poor (600-649)'\n",
    "        else:\n",
    "            return 'Very Poor (<600)'\n",
    "    \n",
    "    credit_categories = df[credit_col].apply(credit_category)\n",
    "    print(f\"\\n   ä¿¡ç”¨ç­‰çº§åˆ†å¸ƒ:\")\n",
    "    print(credit_categories.value_counts().to_string())\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ‰¹å‡†çŽ‡æŒ‰ä¸åŒç»´åº¦åˆ†æž\n",
    "if target_col:\n",
    "    print(\"\\nðŸ“Š ä¸åŒç»´åº¦çš„è´·æ¬¾æ‰¹å‡†çŽ‡åˆ†æž\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # æŒ‰ä¿¡ç”¨åˆ†ç­‰çº§\n",
    "    if credit_cols:\n",
    "        credit_col = credit_cols[0]\n",
    "        credit_categories = df[credit_col].apply(credit_category)\n",
    "        approval_by_credit = pd.crosstab(credit_categories, df[target_col], normalize='index') * 100\n",
    "        print(\"\\næŒ‰ä¿¡ç”¨ç­‰çº§çš„æ‰¹å‡†çŽ‡:\")\n",
    "        print(approval_by_credit.to_string())\n",
    "    \n",
    "    # æŒ‰å°±ä¸šçŠ¶æ€\n",
    "    employment_cols = [col for col in df.columns if 'employ' in col.lower()]\n",
    "    if employment_cols:\n",
    "        emp_col = employment_cols[0]\n",
    "        approval_by_emp = pd.crosstab(df[emp_col], df[target_col], normalize='index') * 100\n",
    "        print(\"\\næŒ‰å°±ä¸šçŠ¶æ€çš„æ‰¹å‡†çŽ‡:\")\n",
    "        print(approval_by_emp.to_string())\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“ 9. å…³é”®å‘çŽ°æ€»ç»“"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ðŸ“ EDA å…³é”®å‘çŽ°æ€»ç»“\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nâœ… æ•°æ®è´¨é‡:\")\n",
    "print(f\"   - æ•°æ®é›†å¤§å°: {df.shape[0]} è¡Œ, {df.shape[1]} åˆ—\")\n",
    "print(f\"   - ç¼ºå¤±å€¼: {df.isnull().sum().sum()} ä¸ª\")\n",
    "print(f\"   - é‡å¤è¡Œ: {df.duplicated().sum()} è¡Œ\")\n",
    "\n",
    "if target_col:\n",
    "    print(f\"\\nðŸŽ¯ ç›®æ ‡å˜é‡ ({target_col}):\")\n",
    "    print(f\"   - ç±»åˆ«åˆ†å¸ƒ: {df[target_col].value_counts().to_dict()}\")\n",
    "    approval_rate = (df[target_col].value_counts(normalize=True).iloc[0] * 100)\n",
    "    print(f\"   - æ‰¹å‡†çŽ‡: {approval_rate:.2f}%\")\n",
    "\n",
    "print(f\"\\nðŸ“Š ç‰¹å¾æ¦‚å†µ:\")\n",
    "print(f\"   - æ•°å€¼åž‹ç‰¹å¾: {len(numeric_cols)} ä¸ª\")\n",
    "print(f\"   - åˆ†ç±»åž‹ç‰¹å¾: {len(categorical_cols)} ä¸ª\")\n",
    "\n",
    "print(\"\\nðŸ’¡ å»ºè®®:\")\n",
    "print(\"   1. ç‰¹å¾å·¥ç¨‹:\")\n",
    "print(\"      - åˆ›å»ºè´·æ¬¾/æ”¶å…¥æ¯”ç‰¹å¾\")\n",
    "print(\"      - å¯¹åˆ†ç±»å˜é‡è¿›è¡Œç¼–ç  (One-Hot æˆ– Label Encoding)\")\n",
    "print(\"      - è€ƒè™‘ç‰¹å¾æ ‡å‡†åŒ–/å½’ä¸€åŒ–\")\n",
    "print(\"   2. æ¨¡åž‹é€‰æ‹©:\")\n",
    "print(\"      - XGBoost (æŽ¨è): é€‚åˆè¡¨æ ¼æ•°æ®ï¼Œå¯å¤„ç†éžçº¿æ€§å…³ç³»\")\n",
    "print(\"      - Random Forest: ç¨³å¥ä¸”å¯è§£é‡Š\")\n",
    "print(\"      - Logistic Regression: ä½œä¸ºåŸºçº¿æ¨¡åž‹\")\n",
    "print(\"   3. è¯„ä¼°æŒ‡æ ‡:\")\n",
    "print(\"      - Accuracy, Precision, Recall, F1-Score\")\n",
    "print(\"      - ROC-AUC: è¯„ä¼°åˆ†ç±»å™¨æ€§èƒ½\")\n",
    "print(\"      - æ··æ·†çŸ©é˜µ: ç†è§£é”™è¯¯ç±»åž‹\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"âœ… EDA å®Œæˆï¼å¯ä»¥å¼€å§‹æ¨¡åž‹è®­ç»ƒäº†ã€‚\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ’¾ 10. ä¿å­˜å¤„ç†åŽçš„æ•°æ®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä¿å­˜æ¸…æ´—åŽçš„æ•°æ®åˆ° processed ç›®å½•\n",
    "processed_dir = Path(\"../data/processed\")\n",
    "processed_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "output_path = processed_dir / \"loan_data_cleaned.csv\"\n",
    "df.to_csv(output_path, index=False)\n",
    "print(f\"âœ… æ¸…æ´—åŽçš„æ•°æ®å·²ä¿å­˜åˆ°: {output_path}\")\n",
    "print(f\"ðŸ“Š ä¿å­˜çš„æ•°æ®å¤§å°: {df.shape[0]} è¡Œ, {df.shape[1]} åˆ—\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
