{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepEval: Evaluating LangChain Agents\n",
    "\n",
    "**LLM-as-Judge Evaluation for AI Agents**\n",
    "\n",
    "[![DeepEval](https://img.shields.io/badge/DeepEval-Latest-purple.svg)](https://docs.confident-ai.com/)\n",
    "[![LangChain](https://img.shields.io/badge/LangChain-0.2+-green.svg)](https://python.langchain.com/)\n",
    "\n",
    "---\n",
    "\n",
    "## What This Notebook Covers\n",
    "\n",
    "1. **Golden → Dataset → Evaluation** workflow with DeepEval\n",
    "2. **LLM-as-Judge metrics**: Answer Relevancy, Faithfulness, Hallucination\n",
    "3. **Multi-type loan agent**: Personal, Mortgage, Auto loans\n",
    "4. **Tool output as context**: Using tool results for evaluation\n",
    "\n",
    "### Loan Advisor Agent Tools\n",
    "\n",
    "| Tool | Description |\n",
    "|------|-------------|\n",
    "| `calculate_personal_loan` | Personal loan payment calculation |\n",
    "| `calculate_mortgage` | Home loan with down payment & LTV |\n",
    "| `calculate_auto_loan` | Car loan with trade-in support |\n",
    "| `check_loan_eligibility` | Credit & income eligibility check |\n",
    "| `check_affordability` | DTI-based affordability analysis |\n",
    "| `compare_loan_options` | Compare different loan terms |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (uncomment for Kaggle/Colab)\n",
    "# !pip install -q deepeval langchain langchain-openai langgraph pandas python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from typing import Dict, Any\n",
    "\n",
    "# =============================================================================\n",
    "# API Key Setup (Kaggle / Colab / Local)\n",
    "# =============================================================================\n",
    "\n",
    "def setup_api_key():\n",
    "    \"\"\"Load API key from various sources.\"\"\"\n",
    "    if os.getenv(\"OPENAI_API_KEY\"):\n",
    "        return \"environment\"\n",
    "    \n",
    "    try:  # Kaggle\n",
    "        from kaggle_secrets import UserSecretsClient\n",
    "        os.environ[\"OPENAI_API_KEY\"] = UserSecretsClient().get_secret(\"OPENAI_API_KEY\")\n",
    "        return \"Kaggle Secrets\"\n",
    "    except: pass\n",
    "    \n",
    "    try:  # Colab\n",
    "        from google.colab import userdata\n",
    "        os.environ[\"OPENAI_API_KEY\"] = userdata.get(\"OPENAI_API_KEY\")\n",
    "        return \"Colab Secrets\"\n",
    "    except: pass\n",
    "    \n",
    "    try:  # Local .env\n",
    "        from dotenv import load_dotenv\n",
    "        load_dotenv()\n",
    "        if os.getenv(\"OPENAI_API_KEY\"):\n",
    "            return \".env file\"\n",
    "    except: pass\n",
    "    \n",
    "    return None\n",
    "\n",
    "source = setup_api_key()\n",
    "if source:\n",
    "    print(f\"API key loaded from {source}\")\n",
    "else:\n",
    "    raise ValueError(\"OPENAI_API_KEY not found. Set via Kaggle/Colab Secrets or .env file.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import Loan Advisor Tools\n",
    "\n",
    "Tools are defined in `langchain_tools.py` - a separate module for cleaner organization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import tools from external module\n",
    "from langchain_tools import (\n",
    "    get_all_tools,\n",
    "    get_tool_descriptions,\n",
    "    calculate_personal_loan,\n",
    "    calculate_mortgage,\n",
    "    calculate_auto_loan,\n",
    "    check_loan_eligibility,\n",
    "    check_affordability,\n",
    "    compare_loan_options,\n",
    ")\n",
    "\n",
    "tools = get_all_tools()\n",
    "print(f\"Loaded {len(tools)} tools:\\n\")\n",
    "print(get_tool_descriptions())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create LangChain Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langchain_core.messages import HumanMessage, AIMessage, ToolMessage\n",
    "\n",
    "# Create agent\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "SYSTEM_PROMPT = \"\"\"You are a Loan Advisor assistant helping users with:\n",
    "- Personal loans, mortgages (home loans), and auto (car) loans\n",
    "- Payment calculations, eligibility checks, and affordability analysis\n",
    "Use the provided tools for accurate calculations. Be clear and helpful.\"\"\"\n",
    "\n",
    "agent = create_react_agent(llm, tools, prompt=SYSTEM_PROMPT)\n",
    "print(\"Agent created with 6 loan advisor tools\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agent runner - extracts output and tool context\n",
    "class AgentRunner:\n",
    "    def __init__(self, agent):\n",
    "        self.agent = agent\n",
    "    \n",
    "    def run(self, query: str) -> Dict[str, Any]:\n",
    "        result = self.agent.invoke({\"messages\": [HumanMessage(content=query)]})\n",
    "        messages = result[\"messages\"]\n",
    "        \n",
    "        # Extract final output\n",
    "        actual_output = next(\n",
    "            (m.content for m in reversed(messages) if isinstance(m, AIMessage) and m.content),\n",
    "            \"\"\n",
    "        )\n",
    "        \n",
    "        # Extract tool calls and results (retrieval context)\n",
    "        tools_called = []\n",
    "        retrieval_context = []\n",
    "        \n",
    "        for msg in messages:\n",
    "            if isinstance(msg, AIMessage) and msg.tool_calls:\n",
    "                tools_called.extend([tc.get(\"name\", \"\") for tc in msg.tool_calls if isinstance(tc, dict)])\n",
    "            if isinstance(msg, ToolMessage):\n",
    "                retrieval_context.append(msg.content)\n",
    "        \n",
    "        return {\n",
    "            \"actual_output\": actual_output,\n",
    "            \"tools_called\": tools_called,\n",
    "            \"retrieval_context\": retrieval_context,\n",
    "        }\n",
    "\n",
    "runner = AgentRunner(agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Define Test Goldens\n",
    "\n",
    "Goldens = test templates with expected behavior. We cover all loan types and tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GOLDENS = [\n",
    "    # --- Personal Loan ---\n",
    "    {\n",
    "        \"id\": \"personal_loan_basic\",\n",
    "        \"category\": \"personal\",\n",
    "        \"input\": \"Calculate monthly payment for a $25,000 personal loan at 10% interest for 48 months.\",\n",
    "        \"expected_tools\": [\"calculate_personal_loan\"],\n",
    "        \"expected_keywords\": [\"634\", \"monthly\", \"payment\"],\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"personal_loan_comparison\",\n",
    "        \"category\": \"personal\",\n",
    "        \"input\": \"Compare a $20,000 personal loan at 9% interest for 36, 48, and 60 months.\",\n",
    "        \"expected_tools\": [\"compare_loan_options\"],\n",
    "        \"expected_keywords\": [\"36\", \"48\", \"60\", \"interest\"],\n",
    "    },\n",
    "    \n",
    "    # --- Mortgage (Home Loan) ---\n",
    "    {\n",
    "        \"id\": \"mortgage_basic\",\n",
    "        \"category\": \"mortgage\",\n",
    "        \"input\": \"Calculate mortgage payment for a $500,000 home with 20% down payment at 6.5% for 30 years.\",\n",
    "        \"expected_tools\": [\"calculate_mortgage\"],\n",
    "        \"expected_keywords\": [\"2,528\", \"monthly\", \"down payment\", \"400,000\"],\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"mortgage_low_down\",\n",
    "        \"category\": \"mortgage\",\n",
    "        \"input\": \"What's the monthly payment for a $400,000 house with only 10% down at 7% for 30 years? Will I need PMI?\",\n",
    "        \"expected_tools\": [\"calculate_mortgage\"],\n",
    "        \"expected_keywords\": [\"LTV\", \"PMI\", \"monthly\"],\n",
    "    },\n",
    "    \n",
    "    # --- Auto Loan (Car Loan) ---\n",
    "    {\n",
    "        \"id\": \"auto_loan_basic\",\n",
    "        \"category\": \"auto\",\n",
    "        \"input\": \"Calculate car loan payment for a $35,000 vehicle with $5,000 down at 5.9% for 60 months.\",\n",
    "        \"expected_tools\": [\"calculate_auto_loan\"],\n",
    "        \"expected_keywords\": [\"581\", \"monthly\", \"30,000\"],\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"auto_loan_trade_in\",\n",
    "        \"category\": \"auto\",\n",
    "        \"input\": \"I want to buy a $40,000 car. I have a trade-in worth $8,000 and can put $2,000 down. What's my payment at 6% for 72 months?\",\n",
    "        \"expected_tools\": [\"calculate_auto_loan\"],\n",
    "        \"expected_keywords\": [\"trade\", \"monthly\", \"30,000\"],\n",
    "    },\n",
    "    \n",
    "    # --- Eligibility ---\n",
    "    {\n",
    "        \"id\": \"eligibility_good_credit\",\n",
    "        \"category\": \"eligibility\",\n",
    "        \"input\": \"Check my loan eligibility: age 35, income $8,000/month, credit score 750, full-time employed, requesting $50,000 personal loan.\",\n",
    "        \"expected_tools\": [\"check_loan_eligibility\"],\n",
    "        \"expected_keywords\": [\"eligible\", \"Excellent\", \"750\"],\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"eligibility_low_credit\",\n",
    "        \"category\": \"eligibility\",\n",
    "        \"input\": \"Am I eligible for a mortgage? Age 28, income $5,000/month, credit score 580, self-employed, want $300,000.\",\n",
    "        \"expected_tools\": [\"check_loan_eligibility\"],\n",
    "        \"expected_keywords\": [\"not\", \"credit\", \"580\"],\n",
    "    },\n",
    "    \n",
    "    # --- Affordability ---\n",
    "    {\n",
    "        \"id\": \"affordability_ok\",\n",
    "        \"category\": \"affordability\",\n",
    "        \"input\": \"Can I afford a $30,000 car loan at 6% for 60 months? I earn $6,000/month with $500 existing debt.\",\n",
    "        \"expected_tools\": [\"check_affordability\"],\n",
    "        \"expected_keywords\": [\"affordable\", \"DTI\"],\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"affordability_stretched\",\n",
    "        \"category\": \"affordability\",\n",
    "        \"input\": \"Monthly income $4,000, existing debt $1,500. Can I afford a $25,000 loan at 8% for 48 months?\",\n",
    "        \"expected_tools\": [\"check_affordability\"],\n",
    "        \"expected_keywords\": [\"DTI\", \"exceed\", \"not\"],\n",
    "    },\n",
    "]\n",
    "\n",
    "print(f\"Defined {len(GOLDENS)} test cases across categories:\")\n",
    "categories = {}\n",
    "for g in GOLDENS:\n",
    "    cat = g[\"category\"]\n",
    "    categories[cat] = categories.get(cat, 0) + 1\n",
    "for cat, count in categories.items():\n",
    "    print(f\"  - {cat}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Run Agent on All Test Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval.dataset import EvaluationDataset, Golden\n",
    "\n",
    "print(\"Running agent on all test cases...\\n\")\n",
    "\n",
    "goldens_with_output = []\n",
    "\n",
    "for g in GOLDENS:\n",
    "    print(f\"[{g['category']}] {g['id']}\")\n",
    "    \n",
    "    result = runner.run(g[\"input\"])\n",
    "    print(f\"  Tools: {result['tools_called']}\")\n",
    "    \n",
    "    golden = Golden(\n",
    "        input=g[\"input\"],\n",
    "        actual_output=result[\"actual_output\"],\n",
    "        retrieval_context=result[\"retrieval_context\"],\n",
    "        additional_metadata={\n",
    "            \"test_id\": g[\"id\"],\n",
    "            \"category\": g[\"category\"],\n",
    "            \"expected_tools\": g[\"expected_tools\"],\n",
    "            \"actual_tools\": result[\"tools_called\"],\n",
    "            \"expected_keywords\": g[\"expected_keywords\"],\n",
    "        }\n",
    "    )\n",
    "    goldens_with_output.append(golden)\n",
    "\n",
    "dataset = EvaluationDataset(goldens=goldens_with_output)\n",
    "print(f\"\\nCreated dataset with {len(dataset.goldens)} test cases\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Configure DeepEval Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval import evaluate\n",
    "from deepeval.metrics import AnswerRelevancyMetric, FaithfulnessMetric, HallucinationMetric\n",
    "from deepeval.test_case import LLMTestCase\n",
    "\n",
    "EVAL_MODEL = \"gpt-4o-mini\"\n",
    "\n",
    "metrics = [\n",
    "    AnswerRelevancyMetric(threshold=0.7, model=EVAL_MODEL),\n",
    "    FaithfulnessMetric(threshold=0.7, model=EVAL_MODEL),\n",
    "    HallucinationMetric(threshold=0.5, model=EVAL_MODEL),\n",
    "]\n",
    "\n",
    "print(\"Metrics configured:\")\n",
    "for m in metrics:\n",
    "    print(f\"  - {m.__class__.__name__} (threshold: {m.threshold})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Goldens to TestCases\n",
    "test_cases = [\n",
    "    LLMTestCase(\n",
    "        input=g.input,\n",
    "        actual_output=g.actual_output,\n",
    "        context=g.retrieval_context,           # For Hallucination\n",
    "        retrieval_context=g.retrieval_context,  # For Faithfulness\n",
    "    )\n",
    "    for g in dataset.goldens\n",
    "]\n",
    "print(f\"Created {len(test_cases)} test cases\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Run Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Running DeepEval evaluation...\\n\")\n",
    "\n",
    "results = evaluate(\n",
    "    test_cases=test_cases,\n",
    "    metrics=metrics,\n",
    ")\n",
    "\n",
    "print(\"\\nEvaluation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Analyze Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "results_data = []\n",
    "\n",
    "for tc, golden in zip(test_cases, dataset.goldens):\n",
    "    meta = golden.additional_metadata\n",
    "    row = {\"test_id\": meta[\"test_id\"], \"category\": meta[\"category\"]}\n",
    "    \n",
    "    for metric in metrics:\n",
    "        metric.measure(tc)\n",
    "        name = metric.__class__.__name__.replace(\"Metric\", \"\")\n",
    "        row[name] = metric.score\n",
    "        row[f\"{name}_pass\"] = metric.is_successful()\n",
    "    \n",
    "    results_data.append(row)\n",
    "\n",
    "df = pd.DataFrame(results_data)\n",
    "print(\"RESULTS BY TEST CASE\\n\")\n",
    "print(df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary by category\n",
    "print(\"\\nAVERAGE SCORES BY CATEGORY\\n\")\n",
    "score_cols = [c for c in df.columns if not c.endswith(\"_pass\") and c not in [\"test_id\", \"category\"]]\n",
    "summary = df.groupby(\"category\")[score_cols].mean().round(2)\n",
    "print(summary.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overall pass rates\n",
    "print(\"\\nOVERALL PASS RATES\\n\")\n",
    "pass_cols = [c for c in df.columns if c.endswith(\"_pass\")]\n",
    "for col in pass_cols:\n",
    "    rate = df[col].mean()\n",
    "    print(f\"  {col.replace('_pass', '')}: {rate:.1%}\")\n",
    "\n",
    "overall = df[pass_cols].all(axis=1).mean()\n",
    "print(f\"\\n  Overall (all metrics pass): {overall:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Tool Call Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"TOOL CALL VALIDATION\\n\")\n",
    "\n",
    "tool_results = []\n",
    "for golden in dataset.goldens:\n",
    "    meta = golden.additional_metadata\n",
    "    expected = set(meta[\"expected_tools\"])\n",
    "    actual = set(meta[\"actual_tools\"])\n",
    "    match = expected.issubset(actual)\n",
    "    \n",
    "    print(f\"[{'PASS' if match else 'FAIL'}] {meta['test_id']}\")\n",
    "    if not match:\n",
    "        print(f\"      Expected: {list(expected)}, Got: {list(actual)}\")\n",
    "    \n",
    "    tool_results.append(match)\n",
    "\n",
    "print(f\"\\nTool Accuracy: {sum(tool_results)/len(tool_results):.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Key Takeaways\n",
    "\n",
    "### DeepEval Evaluation Flow\n",
    "\n",
    "```\n",
    "Goldens (templates) → Agent Execution → LLMTestCase → Metrics → Results\n",
    "                           ↓\n",
    "                    Tool Outputs = retrieval_context\n",
    "```\n",
    "\n",
    "### Metrics Explained\n",
    "\n",
    "| Metric | Measures | Uses Context? |\n",
    "|--------|----------|---------------|\n",
    "| **AnswerRelevancy** | Is response relevant to question? | No |\n",
    "| **Faithfulness** | Is response grounded in context? | Yes |\n",
    "| **Hallucination** | Does response contain made-up info? | Yes |\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "1. **Tool outputs → retrieval_context**: Ground truth for Faithfulness/Hallucination\n",
    "2. **Separate tools module**: Keep notebook focused on evaluation\n",
    "3. **Category-based Goldens**: Cover different loan types and scenarios\n",
    "4. **Validate tool calls**: Ensure agent uses correct tools\n",
    "\n",
    "---\n",
    "\n",
    "**Resources**: [DeepEval Docs](https://docs.confident-ai.com/) | [LangChain](https://python.langchain.com/) | [LangGraph](https://langchain-ai.github.io/langgraph/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
