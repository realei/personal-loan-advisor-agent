{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepEval: Evaluating AI Agents with LLM-as-Judge\n",
    "\n",
    "**A Complete Guide to Golden, Dataset, and Metrics**\n",
    "\n",
    "[![DeepEval](https://img.shields.io/badge/DeepEval-Latest-purple.svg)](https://docs.confident-ai.com/)\n",
    "[![Python 3.11+](https://img.shields.io/badge/python-3.11+-blue.svg)](https://www.python.org/downloads/)\n",
    "\n",
    "---\n",
    "\n",
    "## What is DeepEval?\n",
    "\n",
    "[DeepEval](https://docs.confident-ai.com/) is an open-source LLM evaluation framework that uses **LLM-as-Judge** methodology.\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "| Concept | Description |\n",
    "|---------|-------------|\n",
    "| **Golden** | Blueprint/template for test cases (input + expected data) |\n",
    "| **LLMTestCase** | Actual test case with LLM outputs |\n",
    "| **EvaluationDataset** | Collection of Goldens or TestCases |\n",
    "| **Metrics** | LLM-as-Judge evaluators (Relevancy, Faithfulness, etc.) |\n",
    "\n",
    "### DeepEval is Framework-Agnostic\n",
    "\n",
    "DeepEval works with **any** LLM framework:\n",
    "- Agno (this project)\n",
    "- LangChain\n",
    "- LlamaIndex\n",
    "- Custom implementations\n",
    "\n",
    "What matters is: `input` → `actual_output` → `context`\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Installation & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install DeepEval (uncomment if needed)\n",
    "# !pip install -q deepeval openai pandas python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from typing import List, Dict, Any, Optional\n",
    "from dataclasses import dataclass\n",
    "\n",
    "# DeepEval imports\n",
    "from deepeval import evaluate, assert_test\n",
    "from deepeval.dataset import EvaluationDataset, Golden\n",
    "from deepeval.metrics import (\n",
    "    AnswerRelevancyMetric,\n",
    "    FaithfulnessMetric,\n",
    "    HallucinationMetric,\n",
    "    GEval,\n",
    ")\n",
    "from deepeval.test_case import LLMTestCase, LLMTestCaseParams\n",
    "\n",
    "print(\"DeepEval imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# Verify API key\n",
    "assert os.getenv(\"OPENAI_API_KEY\"), \"Please set OPENAI_API_KEY\"\n",
    "\n",
    "# Configuration\n",
    "EVAL_MODEL = os.getenv(\"DEEPEVAL_MODEL\", \"gpt-4o-mini\")\n",
    "\n",
    "print(f\"Evaluation model: {EVAL_MODEL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Understanding Goldens vs Test Cases\n",
    "\n",
    "### Best Practice from DeepEval Documentation\n",
    "\n",
    "> \"Think of goldens as 'pending test cases' - they contain all the input data and expected results, but are missing the dynamic elements (actual_output, retrieval_context) that will be generated when your LLM processes them.\"\n",
    "\n",
    "**Workflow:**\n",
    "1. Define **Goldens** with inputs and expected behavior\n",
    "2. Run LLM/Agent to generate **actual_output**\n",
    "3. Convert to **LLMTestCase** for evaluation\n",
    "\n",
    "```\n",
    "Golden (template) → LLM Execution → LLMTestCase (with actual output) → Metrics Evaluation\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define Goldens (Test Templates)\n",
    "\n",
    "Following [DeepEval best practices](https://www.confident-ai.com/docs/llm-evaluation/core-concepts/test-cases-goldens-datasets):\n",
    "- Start with ~100 test cases\n",
    "- Include diverse real-world inputs\n",
    "- Vary complexity levels\n",
    "- Cover edge cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Goldens - templates for test cases\n",
    "# These contain input + expected behavior, NOT actual_output\n",
    "\n",
    "GOLDENS_DEFINITION = [\n",
    "    {\n",
    "        \"id\": \"loan_calculation_basic\",\n",
    "        \"input\": \"Calculate my monthly payment for a $50,000 loan at 5% annual interest for 36 months.\",\n",
    "        \"expected_output\": \"Monthly payment around $1,498.88\",\n",
    "        \"expected_tools\": [\"calculate_loan_payment\"],\n",
    "        \"expected_keywords\": [\"1,498\", \"1498\", \"monthly\"],\n",
    "        \"complexity\": \"simple\",\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"eligibility_check\",\n",
    "        \"input\": \"Check my loan eligibility: I'm 25 years old, monthly income $6000, credit score 720, requesting $30,000 loan. I work full-time.\",\n",
    "        \"expected_output\": \"Eligible for the loan with good credit score\",\n",
    "        \"expected_tools\": [\"check_loan_eligibility\"],\n",
    "        \"expected_keywords\": [\"eligible\", \"720\", \"good\"],\n",
    "        \"complexity\": \"simple\",\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"affordability_analysis\",\n",
    "        \"input\": \"Can I afford a $40,000 loan at 6% interest for 48 months? My monthly income is $5,500 and I have $500 in existing debt.\",\n",
    "        \"expected_output\": \"Affordability analysis with DTI ratio calculation\",\n",
    "        \"expected_tools\": [\"check_loan_affordability\"],\n",
    "        \"expected_keywords\": [\"afford\", \"DTI\", \"debt\"],\n",
    "        \"complexity\": \"moderate\",\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"edge_case_low_income\",\n",
    "        \"input\": \"I earn $2000/month. Can I get a $100,000 loan?\",\n",
    "        \"expected_output\": \"Not eligible or not affordable due to income constraints\",\n",
    "        \"expected_tools\": [\"check_loan_eligibility\", \"check_loan_affordability\"],\n",
    "        \"expected_keywords\": [\"income\", \"not\", \"afford\"],\n",
    "        \"complexity\": \"edge_case\",\n",
    "    },\n",
    "]\n",
    "\n",
    "print(f\"Defined {len(GOLDENS_DEFINITION)} Golden templates:\")\n",
    "for g in GOLDENS_DEFINITION:\n",
    "    print(f\"  [{g['complexity']}] {g['id']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Simulate Agent Execution\n",
    "\n",
    "In production, you would run your actual agent here.\n",
    "\n",
    "For this demo, we use **pre-collected responses** to show the evaluation workflow without requiring the full agent setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-collected agent responses (simulating agent execution)\n",
    "# In production: actual_output = agent.run(golden[\"input\"])\n",
    "\n",
    "AGENT_RESPONSES = {\n",
    "    \"loan_calculation_basic\": {\n",
    "        \"actual_output\": \"\"\"Based on your loan details:\n",
    "\n",
    "**Loan Amount**: $50,000.00\n",
    "**Interest Rate**: 5.00% per year\n",
    "**Term**: 36 months\n",
    "\n",
    "### Monthly Payment: $1,498.88\n",
    "\n",
    "**Total Payment**: $53,959.68\n",
    "**Total Interest**: $3,959.68\n",
    "**Interest as % of Principal**: 7.9%\"\"\",\n",
    "        \"tools_called\": [\"calculate_loan_payment\"],\n",
    "        \"retrieval_context\": [\n",
    "            json.dumps({\n",
    "                \"monthly_payment\": 1498.88,\n",
    "                \"total_payment\": 53959.68,\n",
    "                \"total_interest\": 3959.68,\n",
    "                \"loan_amount\": 50000,\n",
    "                \"annual_interest_rate\": 0.05,\n",
    "                \"loan_term_months\": 36\n",
    "            })\n",
    "        ]\n",
    "    },\n",
    "    \"eligibility_check\": {\n",
    "        \"actual_output\": \"\"\"Congratulations! Based on your profile, you ARE eligible for the $30,000 loan.\n",
    "\n",
    "**Your Profile**:\n",
    "- Age: 25 (meets requirement: 18-65)\n",
    "- Monthly Income: $6,000 (exceeds minimum: $3,000)\n",
    "- Credit Score: 720 (Good - above 670 threshold)\n",
    "- Employment: Full-time (stable)\n",
    "\n",
    "Your debt-to-income ratio is healthy at 15%, well below the 50% maximum.\"\"\",\n",
    "        \"tools_called\": [\"check_loan_eligibility\"],\n",
    "        \"retrieval_context\": [\n",
    "            json.dumps({\n",
    "                \"is_eligible\": True,\n",
    "                \"credit_rating\": \"Good\",\n",
    "                \"dti_ratio\": 0.15,\n",
    "                \"max_loan_amount\": 50000,\n",
    "                \"reasons\": [\"Age OK\", \"Income OK\", \"Credit OK\"]\n",
    "            })\n",
    "        ]\n",
    "    },\n",
    "    \"affordability_analysis\": {\n",
    "        \"actual_output\": \"\"\"Based on affordability analysis:\n",
    "\n",
    "**Loan Details**:\n",
    "- Amount: $40,000 at 6% for 48 months\n",
    "- Monthly Payment: $939.15\n",
    "\n",
    "**Your Financial Situation**:\n",
    "- Monthly Income: $5,500\n",
    "- Existing Debt: $500/month\n",
    "- New Loan Payment: $939.15/month\n",
    "- Total Debt: $1,439.15/month\n",
    "\n",
    "**DTI Ratio**: 26.2% (Below 50% threshold)\n",
    "\n",
    "Yes, this loan is AFFORDABLE for you.\"\"\",\n",
    "        \"tools_called\": [\"check_loan_affordability\"],\n",
    "        \"retrieval_context\": [\n",
    "            json.dumps({\n",
    "                \"is_affordable\": True,\n",
    "                \"monthly_payment\": 939.15,\n",
    "                \"current_dti\": 0.091,\n",
    "                \"new_dti\": 0.262,\n",
    "                \"max_dti\": 0.5\n",
    "            })\n",
    "        ]\n",
    "    },\n",
    "    \"edge_case_low_income\": {\n",
    "        \"actual_output\": \"\"\"Based on your income and requested loan amount:\n",
    "\n",
    "**Analysis**:\n",
    "- Monthly Income: $2,000\n",
    "- Requested Loan: $100,000\n",
    "\n",
    "Unfortunately, this loan is NOT affordable for you.\n",
    "\n",
    "The monthly payment would be approximately $2,300, which exceeds your income.\n",
    "Your DTI ratio would be over 100%, far exceeding the 50% maximum.\n",
    "\n",
    "**Recommendation**: Consider a smaller loan amount that fits within 50% of your income.\"\"\",\n",
    "        \"tools_called\": [\"check_loan_eligibility\", \"check_loan_affordability\"],\n",
    "        \"retrieval_context\": [\n",
    "            json.dumps({\n",
    "                \"is_eligible\": False,\n",
    "                \"is_affordable\": False,\n",
    "                \"reason\": \"Income too low for requested amount\",\n",
    "                \"projected_dti\": 1.15\n",
    "            })\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "\n",
    "print(f\"Loaded {len(AGENT_RESPONSES)} pre-collected agent responses\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Create Golden → Dataset Workflow\n",
    "\n",
    "This follows [DeepEval's recommended workflow](https://deepeval.com/docs/evaluation-datasets):\n",
    "\n",
    "1. Loop through goldens\n",
    "2. Generate actual_output from LLM/Agent\n",
    "3. Create test cases with proper context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_evaluation_dataset(\n",
    "    goldens_def: List[Dict],\n",
    "    agent_responses: Dict[str, Dict]\n",
    ") -> EvaluationDataset:\n",
    "    \"\"\"\n",
    "    Create EvaluationDataset from Golden definitions + Agent responses.\n",
    "    \n",
    "    This follows DeepEval best practice:\n",
    "    - Goldens define expected behavior\n",
    "    - actual_output is generated at evaluation time\n",
    "    - retrieval_context provides ground truth for Faithfulness/Hallucination\n",
    "    \"\"\"\n",
    "    goldens = []\n",
    "    \n",
    "    for golden_def in goldens_def:\n",
    "        test_id = golden_def[\"id\"]\n",
    "        \n",
    "        # Get agent response (in production: run agent here)\n",
    "        response = agent_responses.get(test_id, {})\n",
    "        \n",
    "        if not response:\n",
    "            print(f\"  Warning: No response for {test_id}, skipping\")\n",
    "            continue\n",
    "        \n",
    "        # Create Golden with actual output\n",
    "        golden = Golden(\n",
    "            input=golden_def[\"input\"],\n",
    "            actual_output=response[\"actual_output\"],\n",
    "            expected_output=golden_def.get(\"expected_output\", \"\"),\n",
    "            retrieval_context=response.get(\"retrieval_context\", []),\n",
    "            additional_metadata={\n",
    "                \"test_id\": test_id,\n",
    "                \"complexity\": golden_def.get(\"complexity\", \"unknown\"),\n",
    "                \"expected_tools\": golden_def.get(\"expected_tools\", []),\n",
    "                \"actual_tools_called\": response.get(\"tools_called\", []),\n",
    "                \"expected_keywords\": golden_def.get(\"expected_keywords\", []),\n",
    "            }\n",
    "        )\n",
    "        goldens.append(golden)\n",
    "    \n",
    "    return EvaluationDataset(goldens=goldens)\n",
    "\n",
    "\n",
    "# Create dataset\n",
    "print(\"Creating EvaluationDataset...\")\n",
    "dataset = create_evaluation_dataset(GOLDENS_DEFINITION, AGENT_RESPONSES)\n",
    "print(f\"Created dataset with {len(dataset.goldens)} test cases\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Convert Goldens to Test Cases\n",
    "\n",
    "DeepEval metrics require `LLMTestCase` objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def goldens_to_test_cases(dataset: EvaluationDataset) -> List[LLMTestCase]:\n",
    "    \"\"\"\n",
    "    Convert Goldens to LLMTestCase for metric evaluation.\n",
    "    \n",
    "    Key mappings:\n",
    "    - context: Used by HallucinationMetric\n",
    "    - retrieval_context: Used by FaithfulnessMetric\n",
    "    \"\"\"\n",
    "    return [\n",
    "        LLMTestCase(\n",
    "            input=g.input,\n",
    "            actual_output=g.actual_output,\n",
    "            expected_output=g.expected_output,\n",
    "            context=g.retrieval_context,          # For HallucinationMetric\n",
    "            retrieval_context=g.retrieval_context, # For FaithfulnessMetric\n",
    "        )\n",
    "        for g in dataset.goldens\n",
    "    ]\n",
    "\n",
    "\n",
    "test_cases = goldens_to_test_cases(dataset)\n",
    "print(f\"Converted to {len(test_cases)} LLMTestCase objects\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Define Evaluation Metrics\n",
    "\n",
    "### Standard Metrics\n",
    "\n",
    "| Metric | Purpose | Good Score |\n",
    "|--------|---------|------------|\n",
    "| **AnswerRelevancy** | Response addresses the question | >= 0.7 |\n",
    "| **Faithfulness** | Response grounded in context | >= 0.7 |\n",
    "| **Hallucination** | No made-up information | <= 0.5 |\n",
    "\n",
    "### Custom Metrics with G-Eval\n",
    "\n",
    "[G-Eval](https://deepeval.com/docs/metrics-llm-evals) allows custom evaluation criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard metrics\n",
    "standard_metrics = [\n",
    "    AnswerRelevancyMetric(threshold=0.7, model=EVAL_MODEL),\n",
    "    FaithfulnessMetric(threshold=0.7, model=EVAL_MODEL),\n",
    "    HallucinationMetric(threshold=0.5, model=EVAL_MODEL),\n",
    "]\n",
    "\n",
    "# Custom metric: Financial Accuracy (using G-Eval)\n",
    "financial_accuracy_metric = GEval(\n",
    "    name=\"Financial Accuracy\",\n",
    "    criteria=\"\"\"Evaluate whether the financial calculations and advice are accurate:\n",
    "    1. Are numerical calculations correct?\n",
    "    2. Are financial terms used correctly?\n",
    "    3. Is the advice financially sound?\"\"\",\n",
    "    evaluation_params=[LLMTestCaseParams.INPUT, LLMTestCaseParams.ACTUAL_OUTPUT],\n",
    "    threshold=0.7,\n",
    "    model=EVAL_MODEL\n",
    ")\n",
    "\n",
    "# Combine all metrics\n",
    "all_metrics = standard_metrics + [financial_accuracy_metric]\n",
    "\n",
    "print(\"Configured metrics:\")\n",
    "for m in all_metrics:\n",
    "    name = m.name if hasattr(m, 'name') else m.__class__.__name__\n",
    "    print(f\"  - {name} (threshold: {m.threshold})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Run Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Running DeepEval evaluation...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Run batch evaluation\n",
    "results = evaluate(\n",
    "    test_cases=test_cases,\n",
    "    metrics=all_metrics,\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Evaluation Complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Detailed Results Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Collect detailed results\n",
    "results_data = []\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"DETAILED RESULTS BY TEST CASE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for i, (tc, golden) in enumerate(zip(test_cases, dataset.goldens)):\n",
    "    test_id = golden.additional_metadata[\"test_id\"]\n",
    "    complexity = golden.additional_metadata[\"complexity\"]\n",
    "    \n",
    "    print(f\"\\n[{complexity.upper()}] {test_id}\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    row = {\n",
    "        \"test_id\": test_id,\n",
    "        \"complexity\": complexity,\n",
    "    }\n",
    "    \n",
    "    for metric in all_metrics:\n",
    "        metric.measure(tc)\n",
    "        metric_name = metric.name if hasattr(metric, 'name') else metric.__class__.__name__.replace(\"Metric\", \"\")\n",
    "        status = \"PASS\" if metric.is_successful() else \"FAIL\"\n",
    "        print(f\"  {metric_name}: {metric.score:.2f} ({status})\")\n",
    "        \n",
    "        row[metric_name] = metric.score\n",
    "        row[f\"{metric_name}_pass\"] = metric.is_successful()\n",
    "    \n",
    "    results_data.append(row)\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(results_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"SUMMARY STATISTICS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Score columns\n",
    "score_cols = [c for c in df.columns if not c.endswith('_pass') and c not in ['test_id', 'complexity']]\n",
    "\n",
    "print(\"\\nAverage Scores:\")\n",
    "for col in score_cols:\n",
    "    avg = df[col].mean()\n",
    "    print(f\"  {col}: {avg:.2f}\")\n",
    "\n",
    "# Pass rates\n",
    "print(\"\\nPass Rates:\")\n",
    "pass_cols = [c for c in df.columns if c.endswith('_pass')]\n",
    "for col in pass_cols:\n",
    "    rate = df[col].mean()\n",
    "    print(f\"  {col.replace('_pass', '')}: {rate:.1%}\")\n",
    "\n",
    "# Overall\n",
    "overall = df[pass_cols].all(axis=1).mean()\n",
    "print(f\"\\nOverall Pass Rate: {overall:.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results by complexity\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"RESULTS BY COMPLEXITY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for complexity in df['complexity'].unique():\n",
    "    subset = df[df['complexity'] == complexity]\n",
    "    print(f\"\\n{complexity.upper()} ({len(subset)} tests):\")\n",
    "    for col in score_cols:\n",
    "        avg = subset[col].mean()\n",
    "        print(f\"  {col}: {avg:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Tool Call Validation\n",
    "\n",
    "Verify the agent called the expected tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TOOL CALL VALIDATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "tool_results = []\n",
    "\n",
    "for golden in dataset.goldens:\n",
    "    meta = golden.additional_metadata\n",
    "    test_id = meta[\"test_id\"]\n",
    "    expected = set(meta.get(\"expected_tools\", []))\n",
    "    actual = set(meta.get(\"actual_tools_called\", []))\n",
    "    \n",
    "    # Check if expected tools were called\n",
    "    match = expected.issubset(actual)\n",
    "    status = \"PASS\" if match else \"FAIL\"\n",
    "    \n",
    "    print(f\"\\n{test_id}: {status}\")\n",
    "    print(f\"  Expected: {list(expected)}\")\n",
    "    print(f\"  Actual:   {list(actual)}\")\n",
    "    \n",
    "    tool_results.append({\n",
    "        \"test_id\": test_id,\n",
    "        \"expected\": list(expected),\n",
    "        \"actual\": list(actual),\n",
    "        \"match\": match\n",
    "    })\n",
    "\n",
    "# Summary\n",
    "tool_pass_rate = sum(r[\"match\"] for r in tool_results) / len(tool_results)\n",
    "print(f\"\\nTool Call Accuracy: {tool_pass_rate:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Keyword Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"KEYWORD VALIDATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "keyword_results = []\n",
    "\n",
    "for golden in dataset.goldens:\n",
    "    meta = golden.additional_metadata\n",
    "    test_id = meta[\"test_id\"]\n",
    "    expected_kw = meta.get(\"expected_keywords\", [])\n",
    "    output = golden.actual_output.lower()\n",
    "    \n",
    "    found = [kw for kw in expected_kw if kw.lower() in output]\n",
    "    match = len(found) > 0 if expected_kw else True\n",
    "    status = \"PASS\" if match else \"FAIL\"\n",
    "    \n",
    "    print(f\"\\n{test_id}: {status}\")\n",
    "    print(f\"  Expected: {expected_kw}\")\n",
    "    print(f\"  Found:    {found}\")\n",
    "    \n",
    "    keyword_results.append({\"test_id\": test_id, \"match\": match})\n",
    "\n",
    "kw_pass_rate = sum(r[\"match\"] for r in keyword_results) / len(keyword_results)\n",
    "print(f\"\\nKeyword Match Rate: {kw_pass_rate:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final results table\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"FINAL RESULTS TABLE\")\n",
    "print(\"=\" * 60)\n",
    "print(df.to_string(index=False))\n",
    "\n",
    "# Optionally save to CSV\n",
    "# df.to_csv(\"evaluation_results.csv\", index=False)\n",
    "# print(\"\\nResults saved to evaluation_results.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Key Takeaways\n",
    "\n",
    "### DeepEval Best Practices (Summary)\n",
    "\n",
    "1. **Framework-Agnostic**: Works with Agno, LangChain, or any LLM framework\n",
    "2. **Golden → TestCase Workflow**: Define templates, generate outputs at eval time\n",
    "3. **Context is Critical**: `retrieval_context` enables Faithfulness/Hallucination metrics\n",
    "4. **Multiple Metrics**: Combine standard + custom (G-Eval) metrics\n",
    "5. **Start with ~100 cases**: Diverse, varying complexity, include edge cases\n",
    "\n",
    "### For AI Agent Evaluation\n",
    "\n",
    "When evaluating agents that use tools:\n",
    "- **Tool outputs = retrieval_context** (ground truth)\n",
    "- Validate both LLM quality AND tool selection\n",
    "- Use additional_metadata to track tool calls\n",
    "\n",
    "---\n",
    "\n",
    "### Resources\n",
    "\n",
    "- [DeepEval Documentation](https://docs.confident-ai.com/)\n",
    "- [Evaluation Datasets Guide](https://deepeval.com/docs/evaluation-datasets)\n",
    "- [G-Eval Custom Metrics](https://deepeval.com/docs/metrics-llm-evals)\n",
    "- [AI Agent Evaluation Guide](https://deepeval.com/guides/guides-ai-agent-evaluation-metrics)\n",
    "- [DeepEval GitHub](https://github.com/confident-ai/deepeval)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
