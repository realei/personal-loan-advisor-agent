{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ¯ DeepEval: Evaluating LangChain Agents with LLM-as-Judge\n",
    "\n",
    "[![DeepEval](https://img.shields.io/badge/DeepEval-Latest-purple.svg)](https://docs.confident-ai.com/)\n",
    "[![LangChain](https://img.shields.io/badge/LangChain-0.2+-green.svg)](https://python.langchain.com/)\n",
    "[![Python](https://img.shields.io/badge/Python-3.11+-blue.svg)](https://python.org)\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“š What You'll Learn\n",
    "\n",
    "This notebook demonstrates how to evaluate AI agents using **DeepEval's LLM-as-Judge** approach:\n",
    "\n",
    "| Section | Topic |\n",
    "|---------|-------|\n",
    "| ğŸ”§ Setup | Environment and API configuration |\n",
    "| ğŸ› ï¸ Tools | LangChain tools for loan calculations |\n",
    "| ğŸ¤– Agent | Building a ReAct agent with LangGraph |\n",
    "| ğŸ“ Goldens | Test templates covering multiple scenarios |\n",
    "| ğŸ“Š Metrics | **Reference-based** vs **Referenceless** evaluation |\n",
    "| ğŸ”¬ Evaluation | Running DeepEval on agent outputs |\n",
    "| ğŸ“ˆ Analysis | Understanding and interpreting results |\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“ Key Concept: Reference vs Referenceless Metrics\n",
    "\n",
    "DeepEval metrics fall into two categories:\n",
    "\n",
    "### ğŸ“‹ Reference-Based Metrics\n",
    "> **Require ground truth** (`expected_output` or `retrieval_context`)\n",
    "\n",
    "| Metric | Required Parameter | What It Measures |\n",
    "|--------|-------------------|------------------|\n",
    "| `Faithfulness` | `retrieval_context` | Is output grounded in provided context? |\n",
    "| `Hallucination` | `context` | Does output contain fabricated info? |\n",
    "| `ContextualRecall` | `expected_output` + `retrieval_context` | Does context contain all needed info? |\n",
    "| `ContextualPrecision` | `expected_output` + `retrieval_context` | Is context focused and relevant? |\n",
    "\n",
    "### ğŸ†“ Referenceless Metrics  \n",
    "> **No ground truth needed** - evaluate output quality directly\n",
    "\n",
    "| Metric | Required Parameter | What It Measures |\n",
    "|--------|-------------------|------------------|\n",
    "| `AnswerRelevancy` | `input` only | Is response relevant to the question? |\n",
    "| `Toxicity` | `actual_output` only | Contains harmful content? |\n",
    "| `Bias` | `actual_output` only | Contains biased language? |\n",
    "| `TaskCompletion` | Agent trace | Did agent complete the task? |\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¦ Demo: Loan Advisor Agent\n",
    "\n",
    "We'll evaluate a **multi-type loan advisor** with 6 tools:\n",
    "\n",
    "| Tool | Loan Type | Description |\n",
    "|------|-----------|-------------|\n",
    "| `calculate_personal_loan` | ğŸ’³ Personal | Unsecured loan payment calculation |\n",
    "| `calculate_mortgage` | ğŸ  Mortgage | Home loan with LTV & PMI analysis |\n",
    "| `calculate_auto_loan` | ğŸš— Auto | Car loan with trade-in support |\n",
    "| `check_loan_eligibility` | âœ… All | Credit & income eligibility check |\n",
    "| `check_affordability` | ğŸ’° All | DTI-based affordability analysis |\n",
    "| `compare_loan_options` | ğŸ“Š All | Compare different loan terms |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ”§ Part 1: Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“¦ Install dependencies (uncomment for Kaggle/Colab)\n",
    "# !pip install -q deepeval langchain langchain-openai langgraph pandas python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os\nimport json\nfrom typing import Dict, Any\n\n# =============================================================================\n# ğŸ”‘ API Key Setup\n# =============================================================================\n# For Kaggle: Add your key to \"Add-ons\" â†’ \"Secrets\" â†’ \"OPENAI_API_KEY\"\n# For local/Colab: You'll be prompted to enter your key\n# =============================================================================\n\nif not os.getenv(\"OPENAI_API_KEY\"):\n    # Try Kaggle Secrets first (for auto-run when saving version)\n    try:\n        from kaggle_secrets import UserSecretsClient\n        os.environ[\"OPENAI_API_KEY\"] = UserSecretsClient().get_secret(\"OPENAI_API_KEY\")\n        print(\"âœ… API key loaded from Kaggle Secrets\")\n    except:\n        # Fall back to manual input\n        from getpass import getpass\n        os.environ[\"OPENAI_API_KEY\"] = getpass(\"ğŸ”‘ Enter your OpenAI API Key: \")\n        print(\"âœ… API key configured\")\nelse:\n    print(\"âœ… API key already set\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ› ï¸ Part 2: LangChain Tools Definition\n",
    "\n",
    "This notebook uses **6 LangChain tools** for loan calculations:\n",
    "\n",
    "| Tool | Type | Description |\n",
    "|------|------|-------------|\n",
    "| `calculate_personal_loan` | ğŸ’³ Personal | Unsecured loan payment calculation |\n",
    "| `calculate_mortgage` | ğŸ  Mortgage | Home loan with down payment & LTV ratio |\n",
    "| `calculate_auto_loan` | ğŸš— Auto | Car loan with trade-in support |\n",
    "| `check_loan_eligibility` | âœ… Check | Credit & income eligibility verification |\n",
    "| `check_affordability` | ğŸ’° Check | DTI-based affordability analysis |\n",
    "| `compare_loan_options` | ğŸ“Š Compare | Compare different loan terms side-by-side |\n",
    "\n",
    "> â¬‡ï¸ **Next cell:** Run once to create `langchain_tools.py`. You can **collapse it** after running (click the blue bar on the left in Kaggle/Jupyter)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“¦ Create langchain_tools.py (run once, then collapse this cell)\n",
    "# ğŸ‘† Click the blue bar on the left to collapse/expand in Kaggle/Jupyter\n",
    "\n",
    "TOOLS_CODE = '''\n",
    "import json\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "def _calculate_monthly_payment(principal: float, annual_rate: float, months: int) -> float:\n",
    "    if annual_rate == 0: return principal / months\n",
    "    monthly_rate = annual_rate / 12\n",
    "    return round(principal * (monthly_rate * (1 + monthly_rate) ** months) / ((1 + monthly_rate) ** months - 1), 2)\n",
    "\n",
    "def _calculate_totals(principal: float, monthly_payment: float, months: int) -> dict:\n",
    "    total_payment = monthly_payment * months\n",
    "    total_interest = total_payment - principal\n",
    "    return {\"total_payment\": round(total_payment, 2), \"total_interest\": round(total_interest, 2), \"interest_percentage\": round(total_interest / principal * 100, 2)}\n",
    "\n",
    "@tool\n",
    "def calculate_personal_loan(loan_amount: float, annual_interest_rate: float, loan_term_months: int) -> str:\n",
    "    \"\"\"Calculate monthly payment for a personal loan. Args: loan_amount (dollars), annual_interest_rate (decimal, e.g. 0.10), loan_term_months.\"\"\"\n",
    "    monthly = _calculate_monthly_payment(loan_amount, annual_interest_rate, loan_term_months)\n",
    "    return json.dumps({\"loan_type\": \"personal\", \"loan_amount\": loan_amount, \"annual_interest_rate\": annual_interest_rate, \"loan_term_months\": loan_term_months, \"monthly_payment\": monthly, **_calculate_totals(loan_amount, monthly, loan_term_months)})\n",
    "\n",
    "@tool\n",
    "def calculate_mortgage(home_price: float, down_payment_percent: float, annual_interest_rate: float, loan_term_years: int) -> str:\n",
    "    \"\"\"Calculate monthly payment for a mortgage. Args: home_price, down_payment_percent (decimal), annual_interest_rate (decimal), loan_term_years.\"\"\"\n",
    "    down_payment = home_price * down_payment_percent\n",
    "    loan_amount = home_price - down_payment\n",
    "    loan_term_months = loan_term_years * 12\n",
    "    ltv_ratio = loan_amount / home_price\n",
    "    monthly = _calculate_monthly_payment(loan_amount, annual_interest_rate, loan_term_months)\n",
    "    return json.dumps({\"loan_type\": \"mortgage\", \"home_price\": home_price, \"down_payment\": round(down_payment, 2), \"loan_amount\": round(loan_amount, 2), \"ltv_ratio\": round(ltv_ratio, 3), \"annual_interest_rate\": annual_interest_rate, \"loan_term_years\": loan_term_years, \"monthly_payment\": monthly, **_calculate_totals(loan_amount, monthly, loan_term_months), \"ltv_warning\": \"LTV exceeds 80%, PMI may be required\" if ltv_ratio > 0.80 else None})\n",
    "\n",
    "@tool\n",
    "def calculate_auto_loan(vehicle_price: float, down_payment: float, annual_interest_rate: float, loan_term_months: int, trade_in_value: float = 0) -> str:\n",
    "    \"\"\"Calculate monthly payment for an auto loan. Args: vehicle_price, down_payment (dollars), annual_interest_rate (decimal), loan_term_months, trade_in_value (optional).\"\"\"\n",
    "    effective_price = vehicle_price - trade_in_value\n",
    "    loan_amount = effective_price - down_payment\n",
    "    monthly = _calculate_monthly_payment(loan_amount, annual_interest_rate, loan_term_months)\n",
    "    return json.dumps({\"loan_type\": \"auto\", \"vehicle_price\": vehicle_price, \"trade_in_value\": trade_in_value, \"effective_price\": round(effective_price, 2), \"down_payment\": down_payment, \"loan_amount\": round(loan_amount, 2), \"monthly_payment\": monthly, **_calculate_totals(loan_amount, monthly, loan_term_months)})\n",
    "\n",
    "@tool\n",
    "def check_loan_eligibility(age: int, monthly_income: float, credit_score: int, employment_status: str, requested_loan_amount: float, loan_type: str = \"personal\") -> str:\n",
    "    \"\"\"Check loan eligibility. Args: age, monthly_income, credit_score (300-850), employment_status (full_time/part_time/self_employed/unemployed), requested_loan_amount, loan_type.\"\"\"\n",
    "    reasons, is_eligible = [], True\n",
    "    if age < 18 or age > 65: is_eligible, reasons = False, reasons + [f\"Age {age} outside 18-65 range\"]\n",
    "    else: reasons.append(\"Age requirement met\")\n",
    "    min_income = 3000 if loan_type == \"personal\" else 4000\n",
    "    if monthly_income < min_income: is_eligible, reasons = False, reasons + [f\"Income below ${min_income}\"]\n",
    "    else: reasons.append(f\"Income OK (${monthly_income:,.0f}/month)\")\n",
    "    min_scores = {\"personal\": 600, \"mortgage\": 620, \"auto\": 580}\n",
    "    min_score = min_scores.get(loan_type, 600)\n",
    "    credit_rating = \"Poor\" if credit_score < min_score else \"Fair\" if credit_score < 670 else \"Good\" if credit_score < 740 else \"Excellent\"\n",
    "    if credit_score < min_score: is_eligible, reasons = False, reasons + [f\"Credit {credit_score} below {min_score}\"]\n",
    "    else: reasons.append(f\"Credit {credit_rating} ({credit_score})\")\n",
    "    if employment_status == \"unemployed\": is_eligible, reasons = False, reasons + [\"Employment required\"]\n",
    "    else: reasons.append(f\"Employment OK ({employment_status})\")\n",
    "    max_dti = 0.43 if loan_type == \"mortgage\" else 0.50\n",
    "    dti = (requested_loan_amount / 36) / monthly_income\n",
    "    if dti > max_dti: is_eligible, reasons = False, reasons + [f\"DTI {dti:.1%} exceeds {max_dti:.0%}\"]\n",
    "    return json.dumps({\"loan_type\": loan_type, \"is_eligible\": is_eligible, \"credit_rating\": credit_rating, \"estimated_dti\": round(dti, 3), \"reasons\": reasons, \"max_recommended_loan\": round(monthly_income * max_dti * 36, 2)})\n",
    "\n",
    "@tool\n",
    "def check_affordability(monthly_income: float, existing_monthly_debt: float, proposed_loan_amount: float, annual_interest_rate: float, loan_term_months: int) -> str:\n",
    "    \"\"\"Check loan affordability based on DTI. Args: monthly_income, existing_monthly_debt, proposed_loan_amount, annual_interest_rate, loan_term_months.\"\"\"\n",
    "    new_payment = _calculate_monthly_payment(proposed_loan_amount, annual_interest_rate, loan_term_months)\n",
    "    new_dti = (existing_monthly_debt + new_payment) / monthly_income\n",
    "    assessment = \"Excellent\" if new_dti <= 0.30 else \"Good\" if new_dti <= 0.40 else \"Acceptable\" if new_dti <= 0.50 else \"Not recommended\"\n",
    "    return json.dumps({\"is_affordable\": new_dti <= 0.50, \"monthly_income\": monthly_income, \"existing_monthly_debt\": existing_monthly_debt, \"new_monthly_payment\": new_payment, \"total_monthly_debt\": round(existing_monthly_debt + new_payment, 2), \"current_dti\": round(existing_monthly_debt / monthly_income, 3), \"new_dti\": round(new_dti, 3), \"assessment\": assessment})\n",
    "\n",
    "@tool\n",
    "def compare_loan_options(loan_amount: float, annual_interest_rate: float, term_options: str = \"36,48,60\") -> str:\n",
    "    \"\"\"Compare loan options across different terms. Args: loan_amount, annual_interest_rate, term_options (comma-separated months).\"\"\"\n",
    "    terms = [int(t.strip()) for t in term_options.split(\",\")]\n",
    "    comparisons = []\n",
    "    for months in terms:\n",
    "        monthly = _calculate_monthly_payment(loan_amount, annual_interest_rate, months)\n",
    "        totals = _calculate_totals(loan_amount, monthly, months)\n",
    "        comparisons.append({\"term_months\": months, \"monthly_payment\": monthly, \"total_interest\": totals[\"total_interest\"]})\n",
    "    comparisons.sort(key=lambda x: x[\"term_months\"])\n",
    "    if len(comparisons) > 1:\n",
    "        for c in comparisons: c[\"interest_savings_vs_longest\"] = round(comparisons[-1][\"total_interest\"] - c[\"total_interest\"], 2)\n",
    "    return json.dumps({\"loan_amount\": loan_amount, \"annual_interest_rate\": annual_interest_rate, \"comparisons\": comparisons})\n",
    "\n",
    "def get_all_tools():\n",
    "    return [calculate_personal_loan, calculate_mortgage, calculate_auto_loan, check_loan_eligibility, check_affordability, compare_loan_options]\n",
    "\n",
    "def get_tool_descriptions() -> str:\n",
    "    return \"\\\\n\".join([f\"- **{t.name}**: {t.description.split(chr(46))[0]}\" for t in get_all_tools()])\n",
    "'''\n",
    "\n",
    "# Write to file\n",
    "with open(\"langchain_tools.py\", \"w\") as f:\n",
    "    f.write(TOOLS_CODE.strip())\n",
    "print(\"âœ… Created langchain_tools.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“¥ Import the tools we just created\n",
    "from langchain_tools import get_all_tools, get_tool_descriptions\n",
    "\n",
    "tools = get_all_tools()\n",
    "print(f\"âœ… Loaded {len(tools)} tools:\\n\")\n",
    "print(get_tool_descriptions())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¤– Part 3: Create LangChain ReAct Agent\n",
    "\n",
    "We use **LangGraph's `create_react_agent`** â€” a prebuilt ReAct (Reason + Act) agent that:\n",
    "1. Receives user input\n",
    "2. Decides which tool(s) to call\n",
    "3. Executes tools and observes results\n",
    "4. Generates final response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langchain_core.messages import HumanMessage, AIMessage, ToolMessage\n",
    "\n",
    "# ğŸ§  Create LLM\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "# ğŸ“‹ System prompt defines agent behavior\n",
    "SYSTEM_PROMPT = \"\"\"You are a Loan Advisor assistant helping users with:\n",
    "- Personal loans, mortgages (home loans), and auto (car) loans\n",
    "- Payment calculations, eligibility checks, and affordability analysis\n",
    "\n",
    "Use the provided tools for accurate calculations. Be clear and helpful.\n",
    "Always show the key numbers in your response.\"\"\"\n",
    "\n",
    "# ğŸ¤– Create ReAct agent\n",
    "agent = create_react_agent(llm, tools, prompt=SYSTEM_PROMPT)\n",
    "print(\"ğŸ¤– Agent created with 6 loan advisor tools\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸƒ Agent Runner - extracts outputs and tool context\n",
    "class AgentRunner:\n",
    "    \"\"\"Wrapper that runs agent and extracts outputs for evaluation.\"\"\"\n",
    "    \n",
    "    def __init__(self, agent):\n",
    "        self.agent = agent\n",
    "    \n",
    "    def run(self, query: str) -> Dict[str, Any]:\n",
    "        \"\"\"Run agent and extract actual_output + retrieval_context.\"\"\"\n",
    "        result = self.agent.invoke({\"messages\": [HumanMessage(content=query)]})\n",
    "        messages = result[\"messages\"]\n",
    "        \n",
    "        # ğŸ“¤ Extract final AI response\n",
    "        actual_output = next(\n",
    "            (m.content for m in reversed(messages) if isinstance(m, AIMessage) and m.content),\n",
    "            \"\"\n",
    "        )\n",
    "        \n",
    "        # ğŸ”§ Extract tool calls and results\n",
    "        tools_called = []\n",
    "        retrieval_context = []  # ğŸ’¡ Tool outputs become retrieval_context!\n",
    "        \n",
    "        for msg in messages:\n",
    "            if isinstance(msg, AIMessage) and msg.tool_calls:\n",
    "                tools_called.extend([tc.get(\"name\", \"\") for tc in msg.tool_calls if isinstance(tc, dict)])\n",
    "            if isinstance(msg, ToolMessage):\n",
    "                retrieval_context.append(msg.content)\n",
    "        \n",
    "        return {\n",
    "            \"actual_output\": actual_output,\n",
    "            \"tools_called\": tools_called,\n",
    "            \"retrieval_context\": retrieval_context,  # ğŸ¯ Key for Faithfulness/Hallucination metrics!\n",
    "        }\n",
    "\n",
    "runner = AgentRunner(agent)\n",
    "print(\"âœ… AgentRunner ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“ Part 4: Define Test Goldens\n",
    "\n",
    "**Goldens** = test templates that define:\n",
    "- `input`: User query to test\n",
    "- `expected_tools`: Which tools should be called\n",
    "- `expected_keywords`: Key terms that should appear in response\n",
    "\n",
    "We cover **5 categories** across all loan types:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GOLDENS = [\n",
    "    # ==========================================================================\n",
    "    # ğŸ’³ PERSONAL LOAN\n",
    "    # ==========================================================================\n",
    "    {\n",
    "        \"id\": \"personal_loan_basic\",\n",
    "        \"category\": \"ğŸ’³ Personal\",\n",
    "        \"input\": \"Calculate monthly payment for a $25,000 personal loan at 10% interest for 48 months.\",\n",
    "        \"expected_tools\": [\"calculate_personal_loan\"],\n",
    "        \"expected_keywords\": [\"634\", \"monthly\", \"payment\"],\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"personal_loan_comparison\",\n",
    "        \"category\": \"ğŸ’³ Personal\",\n",
    "        \"input\": \"Compare a $20,000 personal loan at 9% interest for 36, 48, and 60 months.\",\n",
    "        \"expected_tools\": [\"compare_loan_options\"],\n",
    "        \"expected_keywords\": [\"36\", \"48\", \"60\", \"interest\"],\n",
    "    },\n",
    "    \n",
    "    # ==========================================================================\n",
    "    # ğŸ  MORTGAGE (HOME LOAN)\n",
    "    # ==========================================================================\n",
    "    {\n",
    "        \"id\": \"mortgage_basic\",\n",
    "        \"category\": \"ğŸ  Mortgage\",\n",
    "        \"input\": \"Calculate mortgage payment for a $500,000 home with 20% down payment at 6.5% for 30 years.\",\n",
    "        \"expected_tools\": [\"calculate_mortgage\"],\n",
    "        \"expected_keywords\": [\"2,528\", \"monthly\", \"down payment\", \"400,000\"],\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"mortgage_low_down\",\n",
    "        \"category\": \"ğŸ  Mortgage\",\n",
    "        \"input\": \"What's the monthly payment for a $400,000 house with only 10% down at 7% for 30 years? Will I need PMI?\",\n",
    "        \"expected_tools\": [\"calculate_mortgage\"],\n",
    "        \"expected_keywords\": [\"LTV\", \"PMI\", \"monthly\"],\n",
    "    },\n",
    "    \n",
    "    # ==========================================================================\n",
    "    # ğŸš— AUTO LOAN (CAR LOAN)\n",
    "    # ==========================================================================\n",
    "    {\n",
    "        \"id\": \"auto_loan_basic\",\n",
    "        \"category\": \"ğŸš— Auto\",\n",
    "        \"input\": \"Calculate car loan payment for a $35,000 vehicle with $5,000 down at 5.9% for 60 months.\",\n",
    "        \"expected_tools\": [\"calculate_auto_loan\"],\n",
    "        \"expected_keywords\": [\"581\", \"monthly\", \"30,000\"],\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"auto_loan_trade_in\",\n",
    "        \"category\": \"ğŸš— Auto\",\n",
    "        \"input\": \"I want to buy a $40,000 car. I have a trade-in worth $8,000 and can put $2,000 down. What's my payment at 6% for 72 months?\",\n",
    "        \"expected_tools\": [\"calculate_auto_loan\"],\n",
    "        \"expected_keywords\": [\"trade\", \"monthly\", \"30,000\"],\n",
    "    },\n",
    "    \n",
    "    # ==========================================================================\n",
    "    # âœ… ELIGIBILITY CHECK\n",
    "    # ==========================================================================\n",
    "    {\n",
    "        \"id\": \"eligibility_good_credit\",\n",
    "        \"category\": \"âœ… Eligibility\",\n",
    "        \"input\": \"Check my loan eligibility: age 35, income $8,000/month, credit score 750, full-time employed, requesting $50,000 personal loan.\",\n",
    "        \"expected_tools\": [\"check_loan_eligibility\"],\n",
    "        \"expected_keywords\": [\"eligible\", \"Excellent\", \"750\"],\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"eligibility_low_credit\",\n",
    "        \"category\": \"âœ… Eligibility\",\n",
    "        \"input\": \"Am I eligible for a mortgage? Age 28, income $5,000/month, credit score 580, self-employed, want $300,000.\",\n",
    "        \"expected_tools\": [\"check_loan_eligibility\"],\n",
    "        \"expected_keywords\": [\"not\", \"credit\", \"580\"],\n",
    "    },\n",
    "    \n",
    "    # ==========================================================================\n",
    "    # ğŸ’° AFFORDABILITY ANALYSIS\n",
    "    # ==========================================================================\n",
    "    {\n",
    "        \"id\": \"affordability_ok\",\n",
    "        \"category\": \"ğŸ’° Affordability\",\n",
    "        \"input\": \"Can I afford a $30,000 car loan at 6% for 60 months? I earn $6,000/month with $500 existing debt.\",\n",
    "        \"expected_tools\": [\"check_affordability\"],\n",
    "        \"expected_keywords\": [\"affordable\", \"DTI\"],\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"affordability_stretched\",\n",
    "        \"category\": \"ğŸ’° Affordability\",\n",
    "        \"input\": \"Monthly income $4,000, existing debt $1,500. Can I afford a $25,000 loan at 8% for 48 months?\",\n",
    "        \"expected_tools\": [\"check_affordability\"],\n",
    "        \"expected_keywords\": [\"DTI\", \"exceed\", \"not\"],\n",
    "    },\n",
    "]\n",
    "\n",
    "# ğŸ“Š Summary\n",
    "print(f\"ğŸ“ Defined {len(GOLDENS)} test cases:\\n\")\n",
    "from collections import Counter\n",
    "categories = Counter(g[\"category\"] for g in GOLDENS)\n",
    "for cat, count in categories.items():\n",
    "    print(f\"  {cat}: {count} tests\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸƒ Part 5: Run Agent on All Test Cases\n",
    "\n",
    "Execute the agent for each Golden and collect:\n",
    "- `actual_output`: Agent's final response\n",
    "- `retrieval_context`: Tool outputs (used by Faithfulness/Hallucination metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval.dataset import EvaluationDataset, Golden\n",
    "\n",
    "print(\"ğŸƒ Running agent on all test cases...\\n\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "goldens_with_output = []\n",
    "\n",
    "for g in GOLDENS:\n",
    "    print(f\"\\n{g['category']} | {g['id']}\")\n",
    "    print(f\"  ğŸ“¥ Input: {g['input'][:50]}...\")\n",
    "    \n",
    "    result = runner.run(g[\"input\"])\n",
    "    print(f\"  ğŸ”§ Tools called: {result['tools_called']}\")\n",
    "    \n",
    "    # Create Golden with agent output\n",
    "    golden = Golden(\n",
    "        input=g[\"input\"],\n",
    "        actual_output=result[\"actual_output\"],\n",
    "        retrieval_context=result[\"retrieval_context\"],  # ğŸ’¡ Tool outputs!\n",
    "        additional_metadata={\n",
    "            \"test_id\": g[\"id\"],\n",
    "            \"category\": g[\"category\"],\n",
    "            \"expected_tools\": g[\"expected_tools\"],\n",
    "            \"actual_tools\": result[\"tools_called\"],\n",
    "            \"expected_keywords\": g[\"expected_keywords\"],\n",
    "        }\n",
    "    )\n",
    "    goldens_with_output.append(golden)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "dataset = EvaluationDataset(goldens=goldens_with_output)\n",
    "print(f\"\\nâœ… Created dataset with {len(dataset.goldens)} test cases\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“Š Part 6: Configure DeepEval Metrics\n",
    "\n",
    "We use **3 metrics** that represent both categories:\n",
    "\n",
    "| Metric | Type | Uses Context? | What It Measures |\n",
    "|--------|------|---------------|------------------|\n",
    "| `AnswerRelevancy` | ğŸ†“ Referenceless | âŒ No | Is response relevant to the question? |\n",
    "| `Faithfulness` | ğŸ“‹ Reference-based | âœ… Yes | Is response grounded in tool outputs? |\n",
    "| `Hallucination` | ğŸ“‹ Reference-based | âœ… Yes | Does response make things up? |\n",
    "\n",
    "> ğŸ’¡ **Why these metrics?**  \n",
    "> - `AnswerRelevancy`: Ensures agent addresses the user's question  \n",
    "> - `Faithfulness`: Ensures agent uses tool outputs correctly  \n",
    "> - `Hallucination`: Catches fabricated numbers or facts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval import evaluate\n",
    "from deepeval.metrics import AnswerRelevancyMetric, FaithfulnessMetric, HallucinationMetric\n",
    "from deepeval.test_case import LLMTestCase\n",
    "\n",
    "# ğŸ§  Judge model for evaluation\n",
    "EVAL_MODEL = \"gpt-4o-mini\"\n",
    "\n",
    "# ğŸ“Š Configure metrics\n",
    "metrics = [\n",
    "    # ğŸ†“ REFERENCELESS: Only needs input + actual_output\n",
    "    AnswerRelevancyMetric(threshold=0.7, model=EVAL_MODEL),\n",
    "    \n",
    "    # ğŸ“‹ REFERENCE-BASED: Need retrieval_context (tool outputs)\n",
    "    FaithfulnessMetric(threshold=0.7, model=EVAL_MODEL),\n",
    "    HallucinationMetric(threshold=0.5, model=EVAL_MODEL),\n",
    "]\n",
    "\n",
    "print(\"ğŸ“Š Metrics configured:\\n\")\n",
    "print(\"  ğŸ†“ Referenceless (no ground truth needed):\")\n",
    "print(f\"     â€¢ AnswerRelevancy (threshold: 0.7)\")\n",
    "print(\"\\n  ğŸ“‹ Reference-based (uses retrieval_context):\")\n",
    "print(f\"     â€¢ Faithfulness (threshold: 0.7)\")\n",
    "print(f\"     â€¢ Hallucination (threshold: 0.5)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ”„ Convert Goldens to LLMTestCases\n",
    "test_cases = [\n",
    "    LLMTestCase(\n",
    "        input=g.input,\n",
    "        actual_output=g.actual_output,\n",
    "        context=g.retrieval_context,           # ğŸ“‹ For Hallucination metric\n",
    "        retrieval_context=g.retrieval_context,  # ğŸ“‹ For Faithfulness metric\n",
    "    )\n",
    "    for g in dataset.goldens\n",
    "]\n",
    "\n",
    "print(f\"âœ… Created {len(test_cases)} LLMTestCases\")\n",
    "print(\"\\nğŸ’¡ Note: retrieval_context = tool outputs = ground truth for reference-based metrics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ”¬ Part 7: Run DeepEval Evaluation\n",
    "\n",
    "Now we run the LLM-as-Judge evaluation on all test cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ”¬ Running DeepEval evaluation...\")\n",
    "print(\"   This may take a few minutes as the judge LLM evaluates each test case.\\n\")\n",
    "\n",
    "results = evaluate(\n",
    "    test_cases=test_cases,\n",
    "    metrics=metrics,\n",
    ")\n",
    "\n",
    "print(\"\\nâœ… Evaluation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“ˆ Part 8: Analyze Results\n",
    "\n",
    "Let's examine the evaluation results in detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# ğŸ“Š Build results dataframe\n",
    "results_data = []\n",
    "\n",
    "for tc, golden in zip(test_cases, dataset.goldens):\n",
    "    meta = golden.additional_metadata\n",
    "    row = {\n",
    "        \"test_id\": meta[\"test_id\"], \n",
    "        \"category\": meta[\"category\"]\n",
    "    }\n",
    "    \n",
    "    # Evaluate each metric\n",
    "    for metric in metrics:\n",
    "        metric.measure(tc)\n",
    "        name = metric.__class__.__name__.replace(\"Metric\", \"\")\n",
    "        row[name] = round(metric.score, 2)\n",
    "        row[f\"{name}_pass\"] = \"âœ…\" if metric.is_successful() else \"âŒ\"\n",
    "    \n",
    "    results_data.append(row)\n",
    "\n",
    "df = pd.DataFrame(results_data)\n",
    "\n",
    "print(\"ğŸ“Š RESULTS BY TEST CASE\\n\")\n",
    "print(df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“Š Summary by category\n",
    "print(\"ğŸ“Š AVERAGE SCORES BY CATEGORY\\n\")\n",
    "score_cols = [\"AnswerRelevancy\", \"Faithfulness\", \"Hallucination\"]\n",
    "summary = df.groupby(\"category\")[score_cols].mean().round(2)\n",
    "print(summary.to_string())\n",
    "\n",
    "# ğŸ¨ Visual representation\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"ğŸ“ˆ CATEGORY PERFORMANCE\")\n",
    "print(\"=\" * 50)\n",
    "for cat in summary.index:\n",
    "    scores = summary.loc[cat]\n",
    "    avg = scores.mean()\n",
    "    bar = \"â–ˆ\" * int(avg * 20) + \"â–‘\" * (20 - int(avg * 20))\n",
    "    print(f\"\\n{cat}\")\n",
    "    print(f\"  {bar} {avg:.0%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“Š Overall pass rates\n",
    "print(\"ğŸ“Š OVERALL PASS RATES\\n\")\n",
    "\n",
    "pass_data = {\n",
    "    \"ğŸ†“ AnswerRelevancy\": (df[\"AnswerRelevancy_pass\"] == \"âœ…\").mean(),\n",
    "    \"ğŸ“‹ Faithfulness\": (df[\"Faithfulness_pass\"] == \"âœ…\").mean(),\n",
    "    \"ğŸ“‹ Hallucination\": (df[\"Hallucination_pass\"] == \"âœ…\").mean(),\n",
    "}\n",
    "\n",
    "for metric, rate in pass_data.items():\n",
    "    bar = \"â–ˆ\" * int(rate * 20) + \"â–‘\" * (20 - int(rate * 20))\n",
    "    status = \"âœ…\" if rate >= 0.8 else \"âš ï¸\" if rate >= 0.6 else \"âŒ\"\n",
    "    print(f\"  {metric}: {bar} {rate:.0%} {status}\")\n",
    "\n",
    "# Overall\n",
    "all_pass = (\n",
    "    (df[\"AnswerRelevancy_pass\"] == \"âœ…\") & \n",
    "    (df[\"Faithfulness_pass\"] == \"âœ…\") & \n",
    "    (df[\"Hallucination_pass\"] == \"âœ…\")\n",
    ").mean()\n",
    "print(f\"\\n  ğŸ¯ All metrics pass: {all_pass:.0%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ”§ Part 9: Tool Call Validation\n",
    "\n",
    "Beyond LLM-as-Judge metrics, we also validate that the agent used the correct tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ”§ TOOL CALL VALIDATION\\n\")\n",
    "\n",
    "tool_results = []\n",
    "for golden in dataset.goldens:\n",
    "    meta = golden.additional_metadata\n",
    "    expected = set(meta[\"expected_tools\"])\n",
    "    actual = set(meta[\"actual_tools\"])\n",
    "    match = expected.issubset(actual)\n",
    "    \n",
    "    status = \"âœ…\" if match else \"âŒ\"\n",
    "    print(f\"  {status} {meta['test_id']}\")\n",
    "    if not match:\n",
    "        print(f\"      Expected: {list(expected)}\")\n",
    "        print(f\"      Got: {list(actual)}\")\n",
    "    \n",
    "    tool_results.append(match)\n",
    "\n",
    "accuracy = sum(tool_results) / len(tool_results)\n",
    "print(f\"\\nğŸ¯ Tool Call Accuracy: {accuracy:.0%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“ Part 10: Key Takeaways\n",
    "\n",
    "### ğŸ”„ DeepEval Evaluation Flow\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚   Goldens   â”‚ â”€â”€â–º â”‚   Agent     â”‚ â”€â”€â–º â”‚ LLMTestCase â”‚ â”€â”€â–º â”‚  Metrics    â”‚\n",
    "â”‚ (templates) â”‚     â”‚ Execution   â”‚     â”‚             â”‚     â”‚             â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                          â”‚\n",
    "                          â–¼\n",
    "                   Tool Outputs = retrieval_context\n",
    "                   (ground truth for reference-based metrics)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“Š Metric Types Summary\n",
    "\n",
    "| Type | Metrics | Required Parameters | Use Case |\n",
    "|------|---------|---------------------|----------|\n",
    "| ğŸ†“ **Referenceless** | AnswerRelevancy, Toxicity, Bias | `input`, `actual_output` only | No labeled data needed |\n",
    "| ğŸ“‹ **Reference-based** | Faithfulness, Hallucination | + `retrieval_context` | Ground truth from tools |\n",
    "| ğŸ“‹ **Reference-based** | ContextualRecall, Precision | + `expected_output` | Need expected answers |\n",
    "\n",
    "---\n",
    "\n",
    "### âœ… Best Practices\n",
    "\n",
    "1. **ğŸ”§ Tool outputs â†’ retrieval_context**  \n",
    "   Use tool results as ground truth for Faithfulness/Hallucination\n",
    "\n",
    "2. **ğŸ“ Separate tools module**  \n",
    "   Keep notebook focused on evaluation logic\n",
    "\n",
    "3. **ğŸ·ï¸ Category-based Goldens**  \n",
    "   Organize tests by loan type/scenario for better analysis\n",
    "\n",
    "4. **ğŸ” Multi-level validation**  \n",
    "   Combine LLM-as-Judge metrics with deterministic tool validation\n",
    "\n",
    "5. **ğŸ“ˆ Track by category**  \n",
    "   Identify which scenarios need improvement\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“š Resources\n",
    "\n",
    "- [DeepEval Documentation](https://docs.confident-ai.com/)\n",
    "- [LangChain](https://python.langchain.com/)\n",
    "- [LangGraph](https://langchain-ai.github.io/langgraph/)\n",
    "- [DeepEval Metrics Guide](https://deepeval.com/docs/metrics-introduction)\n",
    "\n",
    "---\n",
    "\n",
    "**ğŸ‘ Found this helpful? Please upvote!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}