{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepEval: Evaluating LangChain Agents with LLM-as-Judge\n",
    "\n",
    "**A Complete Guide to Evaluating AI Agents**\n",
    "\n",
    "[![DeepEval](https://img.shields.io/badge/DeepEval-Latest-purple.svg)](https://docs.confident-ai.com/)\n",
    "[![LangChain](https://img.shields.io/badge/LangChain-0.2+-green.svg)](https://python.langchain.com/)\n",
    "[![Python 3.11+](https://img.shields.io/badge/python-3.11+-blue.svg)](https://www.python.org/downloads/)\n",
    "\n",
    "---\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook demonstrates how to:\n",
    "1. Build a **Loan Advisor Agent** using [LangChain](https://python.langchain.com/)\n",
    "2. Evaluate it with [DeepEval](https://docs.confident-ai.com/) LLM-as-Judge metrics\n",
    "3. Follow best practices for Golden â†’ Dataset â†’ Evaluation workflow\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "| Concept | Description |\n",
    "|---------|-------------|\n",
    "| **LangChain Agent** | ReAct agent with tools for loan calculations |\n",
    "| **DeepEval Golden** | Test template (input + expected behavior) |\n",
    "| **LLMTestCase** | Actual test with agent outputs |\n",
    "| **Metrics** | Answer Relevancy, Faithfulness, Hallucination |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "# !pip install -q deepeval langchain langchain-openai langgraph pandas python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os\nimport json\nfrom typing import List, Dict, Any, Annotated\n\n# =============================================================================\n# API Key Configuration\n# Supports: Local (.env), Kaggle Secrets, Google Colab Secrets\n# =============================================================================\n\ndef setup_api_key():\n    \"\"\"Setup OpenAI API key from various sources.\"\"\"\n    \n    # 1. Check if already set in environment\n    if os.getenv(\"OPENAI_API_KEY\"):\n        print(\"âœ“ API key loaded from environment\")\n        return True\n    \n    # 2. Try Kaggle Secrets\n    try:\n        from kaggle_secrets import UserSecretsClient\n        user_secrets = UserSecretsClient()\n        os.environ[\"OPENAI_API_KEY\"] = user_secrets.get_secret(\"OPENAI_API_KEY\")\n        print(\"âœ“ API key loaded from Kaggle Secrets\")\n        return True\n    except:\n        pass\n    \n    # 3. Try Google Colab Secrets\n    try:\n        from google.colab import userdata\n        os.environ[\"OPENAI_API_KEY\"] = userdata.get(\"OPENAI_API_KEY\")\n        print(\"âœ“ API key loaded from Colab Secrets\")\n        return True\n    except:\n        pass\n    \n    # 4. Try .env file (local development)\n    try:\n        from dotenv import load_dotenv\n        load_dotenv()\n        if os.getenv(\"OPENAI_API_KEY\"):\n            print(\"âœ“ API key loaded from .env file\")\n            return True\n    except:\n        pass\n    \n    return False\n\n# Setup API key\nif not setup_api_key():\n    print(\"âš ï¸  OPENAI_API_KEY not found!\")\n    print(\"   Please set it using one of these methods:\")\n    print(\"   - Kaggle: Add to Secrets (Add-ons â†’ Secrets)\")\n    print(\"   - Colab: Add to Secrets (ðŸ”‘ icon in sidebar)\")\n    print(\"   - Local: Create .env file with OPENAI_API_KEY=sk-...\")\n    raise ValueError(\"OPENAI_API_KEY is required\")\n\nprint(\"\\nEnvironment configured!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define Loan Advisor Tools\n",
    "\n",
    "Create tools for loan calculations using LangChain's `@tool` decorator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "\n",
    "@tool\n",
    "def calculate_loan_payment(\n",
    "    loan_amount: float,\n",
    "    annual_interest_rate: float,\n",
    "    loan_term_months: int\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Calculate monthly loan payment using standard amortization formula.\n",
    "    \n",
    "    Args:\n",
    "        loan_amount: The principal loan amount in dollars\n",
    "        annual_interest_rate: Annual interest rate as decimal (e.g., 0.05 for 5%)\n",
    "        loan_term_months: Loan term in months\n",
    "    \n",
    "    Returns:\n",
    "        JSON string with payment details\n",
    "    \"\"\"\n",
    "    # Monthly interest rate\n",
    "    monthly_rate = annual_interest_rate / 12\n",
    "    \n",
    "    # Calculate monthly payment using amortization formula\n",
    "    if monthly_rate == 0:\n",
    "        monthly_payment = loan_amount / loan_term_months\n",
    "    else:\n",
    "        monthly_payment = loan_amount * (\n",
    "            monthly_rate * (1 + monthly_rate) ** loan_term_months\n",
    "        ) / ((1 + monthly_rate) ** loan_term_months - 1)\n",
    "    \n",
    "    total_payment = monthly_payment * loan_term_months\n",
    "    total_interest = total_payment - loan_amount\n",
    "    \n",
    "    result = {\n",
    "        \"monthly_payment\": round(monthly_payment, 2),\n",
    "        \"total_payment\": round(total_payment, 2),\n",
    "        \"total_interest\": round(total_interest, 2),\n",
    "        \"loan_amount\": loan_amount,\n",
    "        \"annual_interest_rate\": annual_interest_rate,\n",
    "        \"loan_term_months\": loan_term_months\n",
    "    }\n",
    "    return json.dumps(result)\n",
    "\n",
    "\n",
    "@tool\n",
    "def check_loan_eligibility(\n",
    "    age: int,\n",
    "    monthly_income: float,\n",
    "    credit_score: int,\n",
    "    requested_loan_amount: float,\n",
    "    employment_status: str\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Check if applicant is eligible for a loan based on criteria.\n",
    "    \n",
    "    Args:\n",
    "        age: Applicant's age in years\n",
    "        monthly_income: Monthly income in dollars\n",
    "        credit_score: Credit score (300-850)\n",
    "        requested_loan_amount: Requested loan amount in dollars\n",
    "        employment_status: Employment status (full_time, part_time, self_employed, unemployed)\n",
    "    \n",
    "    Returns:\n",
    "        JSON string with eligibility result\n",
    "    \"\"\"\n",
    "    reasons = []\n",
    "    is_eligible = True\n",
    "    \n",
    "    # Age check (18-65)\n",
    "    if age < 18 or age > 65:\n",
    "        is_eligible = False\n",
    "        reasons.append(f\"Age {age} outside eligible range (18-65)\")\n",
    "    else:\n",
    "        reasons.append(\"Age requirement met\")\n",
    "    \n",
    "    # Income check (minimum $3000)\n",
    "    if monthly_income < 3000:\n",
    "        is_eligible = False\n",
    "        reasons.append(f\"Income ${monthly_income} below minimum ($3000)\")\n",
    "    else:\n",
    "        reasons.append(\"Income requirement met\")\n",
    "    \n",
    "    # Credit score check\n",
    "    if credit_score < 600:\n",
    "        is_eligible = False\n",
    "        credit_rating = \"Poor\"\n",
    "        reasons.append(f\"Credit score {credit_score} below minimum (600)\")\n",
    "    elif credit_score < 670:\n",
    "        credit_rating = \"Fair\"\n",
    "        reasons.append(\"Credit score acceptable\")\n",
    "    elif credit_score < 740:\n",
    "        credit_rating = \"Good\"\n",
    "        reasons.append(\"Credit score good\")\n",
    "    else:\n",
    "        credit_rating = \"Excellent\"\n",
    "        reasons.append(\"Credit score excellent\")\n",
    "    \n",
    "    # Employment check\n",
    "    if employment_status == \"unemployed\":\n",
    "        is_eligible = False\n",
    "        reasons.append(\"Unemployed applicants not eligible\")\n",
    "    else:\n",
    "        reasons.append(f\"Employment status ({employment_status}) acceptable\")\n",
    "    \n",
    "    # DTI ratio estimate\n",
    "    estimated_monthly_payment = requested_loan_amount / 36  # Rough estimate\n",
    "    dti_ratio = estimated_monthly_payment / monthly_income\n",
    "    \n",
    "    if dti_ratio > 0.5:\n",
    "        is_eligible = False\n",
    "        reasons.append(f\"DTI ratio {dti_ratio:.1%} exceeds 50% limit\")\n",
    "    \n",
    "    result = {\n",
    "        \"is_eligible\": is_eligible,\n",
    "        \"credit_rating\": credit_rating,\n",
    "        \"dti_ratio\": round(dti_ratio, 3),\n",
    "        \"reasons\": reasons,\n",
    "        \"max_recommended_loan\": round(monthly_income * 0.5 * 36, 2)\n",
    "    }\n",
    "    return json.dumps(result)\n",
    "\n",
    "\n",
    "@tool\n",
    "def check_loan_affordability(\n",
    "    loan_amount: float,\n",
    "    annual_interest_rate: float,\n",
    "    loan_term_months: int,\n",
    "    monthly_income: float,\n",
    "    existing_monthly_debt: float\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Check if a loan is affordable based on debt-to-income ratio.\n",
    "    \n",
    "    Args:\n",
    "        loan_amount: Loan amount in dollars\n",
    "        annual_interest_rate: Annual interest rate as decimal\n",
    "        loan_term_months: Loan term in months\n",
    "        monthly_income: Monthly income in dollars\n",
    "        existing_monthly_debt: Existing monthly debt payments\n",
    "    \n",
    "    Returns:\n",
    "        JSON string with affordability analysis\n",
    "    \"\"\"\n",
    "    # Calculate monthly payment\n",
    "    monthly_rate = annual_interest_rate / 12\n",
    "    if monthly_rate == 0:\n",
    "        monthly_payment = loan_amount / loan_term_months\n",
    "    else:\n",
    "        monthly_payment = loan_amount * (\n",
    "            monthly_rate * (1 + monthly_rate) ** loan_term_months\n",
    "        ) / ((1 + monthly_rate) ** loan_term_months - 1)\n",
    "    \n",
    "    # Calculate DTI ratios\n",
    "    current_dti = existing_monthly_debt / monthly_income\n",
    "    new_dti = (existing_monthly_debt + monthly_payment) / monthly_income\n",
    "    \n",
    "    is_affordable = new_dti <= 0.5\n",
    "    \n",
    "    result = {\n",
    "        \"is_affordable\": is_affordable,\n",
    "        \"monthly_payment\": round(monthly_payment, 2),\n",
    "        \"current_dti\": round(current_dti, 3),\n",
    "        \"new_dti\": round(new_dti, 3),\n",
    "        \"max_dti\": 0.5,\n",
    "        \"total_monthly_debt\": round(existing_monthly_debt + monthly_payment, 2),\n",
    "        \"remaining_income\": round(monthly_income - existing_monthly_debt - monthly_payment, 2)\n",
    "    }\n",
    "    return json.dumps(result)\n",
    "\n",
    "\n",
    "# Collect all tools\n",
    "tools = [calculate_loan_payment, check_loan_eligibility, check_loan_affordability]\n",
    "\n",
    "print(f\"Defined {len(tools)} tools:\")\n",
    "for t in tools:\n",
    "    print(f\"  - {t.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create LangChain ReAct Agent\n",
    "\n",
    "Using LangGraph's `create_react_agent` for a production-ready agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from langchain_openai import ChatOpenAI\nfrom langgraph.prebuilt import create_react_agent\n\n# Initialize LLM\nllm = ChatOpenAI(\n    model=\"gpt-4o-mini\",\n    temperature=0\n)\n\n# System prompt for loan advisor\nsystem_prompt = \"\"\"You are a helpful Loan Advisor assistant. \nYou help users with loan calculations, eligibility checks, and affordability analysis.\nAlways use the provided tools to give accurate information.\nBe clear and concise in your responses.\"\"\"\n\n# Create ReAct agent using LangGraph\n# Note: Use 'prompt' parameter (state_modifier is deprecated)\nagent = create_react_agent(\n    llm,\n    tools,\n    prompt=system_prompt\n)\n\nprint(\"LangChain ReAct Agent created!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Agent Runner with Context Extraction\n",
    "\n",
    "This class runs queries through the agent and extracts:\n",
    "- `actual_output`: Agent's final response\n",
    "- `tools_called`: Which tools were invoked\n",
    "- `retrieval_context`: Tool outputs (ground truth for evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, AIMessage, ToolMessage\n",
    "\n",
    "class LangChainAgentRunner:\n",
    "    \"\"\"Runs queries through LangChain agent and extracts evaluation data.\"\"\"\n",
    "    \n",
    "    def __init__(self, agent):\n",
    "        self.agent = agent\n",
    "    \n",
    "    def run(self, query: str) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Run query through agent and extract results.\n",
    "        \n",
    "        Returns:\n",
    "            Dict with actual_output, tools_called, retrieval_context\n",
    "        \"\"\"\n",
    "        # Invoke agent\n",
    "        result = self.agent.invoke({\n",
    "            \"messages\": [HumanMessage(content=query)]\n",
    "        })\n",
    "        \n",
    "        messages = result[\"messages\"]\n",
    "        \n",
    "        # Extract final output (last AI message)\n",
    "        actual_output = \"\"\n",
    "        for msg in reversed(messages):\n",
    "            if isinstance(msg, AIMessage) and msg.content:\n",
    "                actual_output = msg.content\n",
    "                break\n",
    "        \n",
    "        # Extract tool calls and results\n",
    "        tools_called = []\n",
    "        retrieval_context = []\n",
    "        \n",
    "        for msg in messages:\n",
    "            # Tool calls from AI messages\n",
    "            if isinstance(msg, AIMessage) and hasattr(msg, 'tool_calls'):\n",
    "                for tc in msg.tool_calls:\n",
    "                    if isinstance(tc, dict):\n",
    "                        tools_called.append(tc.get('name', ''))\n",
    "            \n",
    "            # Tool results\n",
    "            if isinstance(msg, ToolMessage):\n",
    "                retrieval_context.append(msg.content)\n",
    "        \n",
    "        return {\n",
    "            \"actual_output\": actual_output,\n",
    "            \"tools_called\": tools_called,\n",
    "            \"retrieval_context\": retrieval_context\n",
    "        }\n",
    "\n",
    "\n",
    "# Create runner\n",
    "runner = LangChainAgentRunner(agent)\n",
    "print(\"Agent runner ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Test the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick test\n",
    "test_query = \"Calculate my monthly payment for a $50,000 loan at 5% interest for 36 months.\"\n",
    "\n",
    "print(f\"Query: {test_query}\")\n",
    "print(\"\\nRunning agent...\")\n",
    "\n",
    "result = runner.run(test_query)\n",
    "\n",
    "print(f\"\\nTools called: {result['tools_called']}\")\n",
    "print(f\"\\nAgent response:\\n{result['actual_output']}\")\n",
    "print(f\"\\nRetrieval context: {result['retrieval_context'][:1]}...\")  # First tool result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Define Goldens (Test Templates)\n",
    "\n",
    "Following [DeepEval best practices](https://www.confident-ai.com/docs/llm-evaluation/core-concepts/test-cases-goldens-datasets)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GOLDENS = [\n",
    "    {\n",
    "        \"id\": \"loan_calculation\",\n",
    "        \"input\": \"Calculate my monthly payment for a $50,000 loan at 5% annual interest for 36 months.\",\n",
    "        \"expected_tools\": [\"calculate_loan_payment\"],\n",
    "        \"expected_keywords\": [\"1,498\", \"1498\", \"monthly\"],\n",
    "        \"complexity\": \"simple\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"eligibility_check\",\n",
    "        \"input\": \"Check my loan eligibility: I'm 25 years old, monthly income $6000, credit score 720, requesting $30,000 loan. I work full-time.\",\n",
    "        \"expected_tools\": [\"check_loan_eligibility\"],\n",
    "        \"expected_keywords\": [\"eligible\", \"720\", \"good\"],\n",
    "        \"complexity\": \"simple\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"affordability_check\",\n",
    "        \"input\": \"Can I afford a $40,000 loan at 6% interest for 48 months? My monthly income is $5,500 and I have $500 in existing debt.\",\n",
    "        \"expected_tools\": [\"check_loan_affordability\"],\n",
    "        \"expected_keywords\": [\"afford\", \"DTI\", \"debt\"],\n",
    "        \"complexity\": \"moderate\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"low_income_edge_case\",\n",
    "        \"input\": \"I earn $2000/month with $300 existing debt. Can I afford a $50,000 loan at 7% for 60 months?\",\n",
    "        \"expected_tools\": [\"check_loan_affordability\"],\n",
    "        \"expected_keywords\": [\"not\", \"afford\", \"exceed\"],\n",
    "        \"complexity\": \"edge_case\"\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f\"Defined {len(GOLDENS)} test cases:\")\n",
    "for g in GOLDENS:\n",
    "    print(f\"  [{g['complexity']}] {g['id']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Run Agent on All Test Cases\n",
    "\n",
    "Execute each golden through the agent to get actual outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval.dataset import EvaluationDataset, Golden\n",
    "\n",
    "print(\"Running agent on all test cases...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "goldens_with_output = []\n",
    "\n",
    "for g in GOLDENS:\n",
    "    print(f\"\\nRunning: {g['id']}\")\n",
    "    \n",
    "    # Run through agent\n",
    "    result = runner.run(g[\"input\"])\n",
    "    \n",
    "    print(f\"  Tools: {result['tools_called']}\")\n",
    "    print(f\"  Output: {result['actual_output'][:100]}...\")\n",
    "    \n",
    "    # Create Golden with actual output\n",
    "    golden = Golden(\n",
    "        input=g[\"input\"],\n",
    "        actual_output=result[\"actual_output\"],\n",
    "        retrieval_context=result[\"retrieval_context\"],\n",
    "        additional_metadata={\n",
    "            \"test_id\": g[\"id\"],\n",
    "            \"complexity\": g[\"complexity\"],\n",
    "            \"expected_tools\": g[\"expected_tools\"],\n",
    "            \"actual_tools\": result[\"tools_called\"],\n",
    "            \"expected_keywords\": g[\"expected_keywords\"]\n",
    "        }\n",
    "    )\n",
    "    goldens_with_output.append(golden)\n",
    "\n",
    "# Create dataset\n",
    "dataset = EvaluationDataset(goldens=goldens_with_output)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(f\"Created dataset with {len(dataset.goldens)} test cases\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Define DeepEval Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval import evaluate\n",
    "from deepeval.metrics import (\n",
    "    AnswerRelevancyMetric,\n",
    "    FaithfulnessMetric,\n",
    "    HallucinationMetric,\n",
    "    GEval,\n",
    ")\n",
    "from deepeval.test_case import LLMTestCase, LLMTestCaseParams\n",
    "\n",
    "EVAL_MODEL = \"gpt-4o-mini\"\n",
    "\n",
    "# Standard metrics\n",
    "metrics = [\n",
    "    AnswerRelevancyMetric(threshold=0.7, model=EVAL_MODEL),\n",
    "    FaithfulnessMetric(threshold=0.7, model=EVAL_MODEL),\n",
    "    HallucinationMetric(threshold=0.5, model=EVAL_MODEL),\n",
    "]\n",
    "\n",
    "# Custom metric for financial advice\n",
    "financial_metric = GEval(\n",
    "    name=\"Financial Accuracy\",\n",
    "    criteria=\"\"\"Evaluate the financial accuracy of the response:\n",
    "    1. Are calculations mathematically correct?\n",
    "    2. Are financial terms used appropriately?\n",
    "    3. Is the advice financially sound?\"\"\",\n",
    "    evaluation_params=[LLMTestCaseParams.INPUT, LLMTestCaseParams.ACTUAL_OUTPUT],\n",
    "    threshold=0.7,\n",
    "    model=EVAL_MODEL\n",
    ")\n",
    "\n",
    "all_metrics = metrics + [financial_metric]\n",
    "\n",
    "print(f\"Configured {len(all_metrics)} metrics:\")\n",
    "for m in all_metrics:\n",
    "    name = m.name if hasattr(m, 'name') else m.__class__.__name__\n",
    "    print(f\"  - {name} (threshold: {m.threshold})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Convert Goldens to Test Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to LLMTestCase for evaluation\n",
    "test_cases = [\n",
    "    LLMTestCase(\n",
    "        input=g.input,\n",
    "        actual_output=g.actual_output,\n",
    "        context=g.retrieval_context,           # For Hallucination\n",
    "        retrieval_context=g.retrieval_context,  # For Faithfulness\n",
    "    )\n",
    "    for g in dataset.goldens\n",
    "]\n",
    "\n",
    "print(f\"Created {len(test_cases)} LLMTestCase objects\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Run Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Running DeepEval evaluation...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "results = evaluate(\n",
    "    test_cases=test_cases,\n",
    "    metrics=all_metrics,\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Evaluation Complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Analyze Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "results_data = []\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"DETAILED RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for i, (tc, golden) in enumerate(zip(test_cases, dataset.goldens)):\n",
    "    meta = golden.additional_metadata\n",
    "    test_id = meta[\"test_id\"]\n",
    "    complexity = meta[\"complexity\"]\n",
    "    \n",
    "    print(f\"\\n[{complexity.upper()}] {test_id}\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    row = {\"test_id\": test_id, \"complexity\": complexity}\n",
    "    \n",
    "    for metric in all_metrics:\n",
    "        metric.measure(tc)\n",
    "        name = metric.name if hasattr(metric, 'name') else metric.__class__.__name__.replace(\"Metric\", \"\")\n",
    "        status = \"PASS\" if metric.is_successful() else \"FAIL\"\n",
    "        print(f\"  {name}: {metric.score:.2f} ({status})\")\n",
    "        \n",
    "        row[name] = metric.score\n",
    "        row[f\"{name}_pass\"] = metric.is_successful()\n",
    "    \n",
    "    results_data.append(row)\n",
    "\n",
    "df = pd.DataFrame(results_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "score_cols = [c for c in df.columns if not c.endswith('_pass') and c not in ['test_id', 'complexity']]\n",
    "\n",
    "print(\"\\nAverage Scores:\")\n",
    "for col in score_cols:\n",
    "    print(f\"  {col}: {df[col].mean():.2f}\")\n",
    "\n",
    "print(\"\\nPass Rates:\")\n",
    "pass_cols = [c for c in df.columns if c.endswith('_pass')]\n",
    "for col in pass_cols:\n",
    "    print(f\"  {col.replace('_pass', '')}: {df[col].mean():.1%}\")\n",
    "\n",
    "overall = df[pass_cols].all(axis=1).mean()\n",
    "print(f\"\\nOverall Pass Rate: {overall:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Tool Call Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TOOL CALL VALIDATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "tool_accuracy = []\n",
    "\n",
    "for golden in dataset.goldens:\n",
    "    meta = golden.additional_metadata\n",
    "    expected = set(meta[\"expected_tools\"])\n",
    "    actual = set(meta[\"actual_tools\"])\n",
    "    \n",
    "    match = expected.issubset(actual)\n",
    "    status = \"PASS\" if match else \"FAIL\"\n",
    "    \n",
    "    print(f\"\\n{meta['test_id']}: {status}\")\n",
    "    print(f\"  Expected: {list(expected)}\")\n",
    "    print(f\"  Actual:   {list(actual)}\")\n",
    "    \n",
    "    tool_accuracy.append(match)\n",
    "\n",
    "print(f\"\\nTool Accuracy: {sum(tool_accuracy)/len(tool_accuracy):.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final results table\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"FINAL RESULTS TABLE\")\n",
    "print(\"=\" * 60)\n",
    "print(df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Key Takeaways\n",
    "\n",
    "### LangChain + DeepEval Integration\n",
    "\n",
    "1. **Build Agent**: Use LangGraph's `create_react_agent` with custom tools\n",
    "2. **Extract Context**: Tool outputs become `retrieval_context` for evaluation\n",
    "3. **Evaluate**: Use DeepEval metrics (Relevancy, Faithfulness, Hallucination)\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "| Practice | Description |\n",
    "|----------|-------------|\n",
    "| **Golden â†’ TestCase** | Define templates, generate outputs at eval time |\n",
    "| **Context = Tool Outputs** | Use tool results as ground truth |\n",
    "| **Multiple Metrics** | Combine standard + custom (G-Eval) metrics |\n",
    "| **Validate Tools** | Check correct tools were called |\n",
    "\n",
    "---\n",
    "\n",
    "### Resources\n",
    "\n",
    "- [LangChain Agents](https://docs.langchain.com/oss/python/langchain/agents)\n",
    "- [LangGraph ReAct Agent](https://langchain-ai.github.io/langgraph/how-tos/react-agent-from-scratch/)\n",
    "- [DeepEval Documentation](https://docs.confident-ai.com/)\n",
    "- [DeepEval Datasets](https://deepeval.com/docs/evaluation-datasets)\n",
    "- [G-Eval Custom Metrics](https://deepeval.com/docs/metrics-llm-evals)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}