{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# ğŸ¯ DeepEval: Evaluating LangChain Agents with LLM-as-Judge\n\n**Tech Stack:** DeepEval Â· LangChain Â· Python 3.11+\n\n---\n\n## ğŸ“š What You'll Learn\n\nThis notebook demonstrates how to evaluate AI agents using **DeepEval's LLM-as-Judge** approach:\n\n| Section | Topic |\n|---------|-------|\n| ğŸ”§ Setup | Environment and API configuration |\n| ğŸ› ï¸ Tools | LangChain tools for loan calculations |\n| ğŸ¤– Agent | Building a ReAct agent |\n| ğŸ“ Goldens | Test templates covering multiple scenarios |\n| ğŸ“Š Metrics | **Reference-based**, **Referenceless**, and **Agent Tool** metrics |\n| ğŸ”¬ Evaluation | Running DeepEval on agent outputs |\n| ğŸ“ˆ Analysis | Understanding and interpreting results |\n\n---\n\n## ğŸ“ Key Concept: DeepEval Metric Categories\n\n### ğŸ“‹ Reference-Based Metrics\n> **Require ground truth** (`expected_output` or `retrieval_context`)\n\n| Metric | Required Parameter | What It Measures |\n|--------|-------------------|------------------|\n| `Faithfulness` | `retrieval_context` | Is output grounded in provided context? |\n| `Hallucination` | `context` | Does output contain fabricated info? |\n\n### ğŸ†“ Referenceless Metrics  \n> **No ground truth needed** - evaluate output quality directly\n\n| Metric | Required Parameter | What It Measures |\n|--------|-------------------|------------------|\n| `AnswerRelevancy` | `input` only | Is response relevant to the question? |\n\n### ğŸ”§ Agent Tool Metrics\n> **Evaluate tool calling behavior** - specific to agentic systems\n\n| Metric | Required Parameter | What It Measures |\n|--------|-------------------|------------------|\n| `ToolCorrectness` | `tools_called`, `expected_tools` | Did agent call the right tools? |\n| `ArgumentCorrectness` | `tools_called` (with input) | Did agent parse correct parameters? |\n\n---\n\n## ğŸ¦ Demo: Loan Advisor Agent\n\nWe'll evaluate a **multi-type loan advisor** with 6 tools:\n\n| Tool | Loan Type | Description |\n|------|-----------|-------------|\n| `calculate_personal_loan` | ğŸ’³ Personal | Unsecured loan payment calculation |\n| `calculate_mortgage` | ğŸ  Mortgage | Home loan with LTV & PMI analysis |\n| `calculate_auto_loan` | ğŸš— Auto | Car loan with trade-in support |\n| `check_loan_eligibility` | âœ… All | Credit & income eligibility check |\n| `check_affordability` | ğŸ’° All | DTI-based affordability analysis |\n| `compare_loan_options` | ğŸ“Š All | Compare different loan terms |\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ”§ Part 1: Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“¦ Install dependencies (uncomment for Kaggle/Colab)\n",
    "# !pip install -q deepeval langchain langchain-openai langgraph pandas python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os\nimport json\nfrom typing import Dict, Any\n\n# =============================================================================\n# ğŸ”‘ API Key Setup\n# =============================================================================\n# For Kaggle: Add your key to \"Add-ons\" â†’ \"Secrets\" â†’ \"OPENAI_API_KEY\"\n# For local/Colab: You'll be prompted to enter your key\n# =============================================================================\n\nif not os.getenv(\"OPENAI_API_KEY\"):\n    # Try Kaggle Secrets first (for auto-run when saving version)\n    try:\n        from kaggle_secrets import UserSecretsClient\n        os.environ[\"OPENAI_API_KEY\"] = UserSecretsClient().get_secret(\"OPENAI_API_KEY\")\n        print(\"âœ… API key loaded from Kaggle Secrets\")\n    except:\n        # Fall back to manual input\n        from getpass import getpass\n        os.environ[\"OPENAI_API_KEY\"] = getpass(\"ğŸ”‘ Enter your OpenAI API Key: \")\n        print(\"âœ… API key configured\")\nelse:\n    print(\"âœ… API key already set\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ› ï¸ Part 2: LangChain Tools Definition\n",
    "\n",
    "This notebook uses **6 LangChain tools** for loan calculations:\n",
    "\n",
    "| Tool | Type | Description |\n",
    "|------|------|-------------|\n",
    "| `calculate_personal_loan` | ğŸ’³ Personal | Unsecured loan payment calculation |\n",
    "| `calculate_mortgage` | ğŸ  Mortgage | Home loan with down payment & LTV ratio |\n",
    "| `calculate_auto_loan` | ğŸš— Auto | Car loan with trade-in support |\n",
    "| `check_loan_eligibility` | âœ… Check | Credit & income eligibility verification |\n",
    "| `check_affordability` | ğŸ’° Check | DTI-based affordability analysis |\n",
    "| `compare_loan_options` | ğŸ“Š Compare | Compare different loan terms side-by-side |\n",
    "\n",
    "> â¬‡ï¸ **Next cell:** Run once to create `langchain_tools.py`. You can **collapse it** after running (click the blue bar on the left in Kaggle/Jupyter)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“¦ Create langchain_tools.py (run once, then collapse this cell)\n",
    "# ğŸ‘† Click the blue bar on the left to collapse/expand in Kaggle/Jupyter\n",
    "\n",
    "TOOLS_CODE = '''\n",
    "import json\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "def _calculate_monthly_payment(principal: float, annual_rate: float, months: int) -> float:\n",
    "    if annual_rate == 0: return principal / months\n",
    "    monthly_rate = annual_rate / 12\n",
    "    return round(principal * (monthly_rate * (1 + monthly_rate) ** months) / ((1 + monthly_rate) ** months - 1), 2)\n",
    "\n",
    "def _calculate_totals(principal: float, monthly_payment: float, months: int) -> dict:\n",
    "    total_payment = monthly_payment * months\n",
    "    total_interest = total_payment - principal\n",
    "    return {\"total_payment\": round(total_payment, 2), \"total_interest\": round(total_interest, 2), \"interest_percentage\": round(total_interest / principal * 100, 2)}\n",
    "\n",
    "@tool\n",
    "def calculate_personal_loan(loan_amount: float, annual_interest_rate: float, loan_term_months: int) -> str:\n",
    "    \"\"\"Calculate monthly payment for a personal loan. Args: loan_amount (dollars), annual_interest_rate (decimal, e.g. 0.10), loan_term_months.\"\"\"\n",
    "    monthly = _calculate_monthly_payment(loan_amount, annual_interest_rate, loan_term_months)\n",
    "    return json.dumps({\"loan_type\": \"personal\", \"loan_amount\": loan_amount, \"annual_interest_rate\": annual_interest_rate, \"loan_term_months\": loan_term_months, \"monthly_payment\": monthly, **_calculate_totals(loan_amount, monthly, loan_term_months)})\n",
    "\n",
    "@tool\n",
    "def calculate_mortgage(home_price: float, down_payment_percent: float, annual_interest_rate: float, loan_term_years: int) -> str:\n",
    "    \"\"\"Calculate monthly payment for a mortgage. Args: home_price, down_payment_percent (decimal), annual_interest_rate (decimal), loan_term_years.\"\"\"\n",
    "    down_payment = home_price * down_payment_percent\n",
    "    loan_amount = home_price - down_payment\n",
    "    loan_term_months = loan_term_years * 12\n",
    "    ltv_ratio = loan_amount / home_price\n",
    "    monthly = _calculate_monthly_payment(loan_amount, annual_interest_rate, loan_term_months)\n",
    "    return json.dumps({\"loan_type\": \"mortgage\", \"home_price\": home_price, \"down_payment\": round(down_payment, 2), \"loan_amount\": round(loan_amount, 2), \"ltv_ratio\": round(ltv_ratio, 3), \"annual_interest_rate\": annual_interest_rate, \"loan_term_years\": loan_term_years, \"monthly_payment\": monthly, **_calculate_totals(loan_amount, monthly, loan_term_months), \"ltv_warning\": \"LTV exceeds 80%, PMI may be required\" if ltv_ratio > 0.80 else None})\n",
    "\n",
    "@tool\n",
    "def calculate_auto_loan(vehicle_price: float, down_payment: float, annual_interest_rate: float, loan_term_months: int, trade_in_value: float = 0) -> str:\n",
    "    \"\"\"Calculate monthly payment for an auto loan. Args: vehicle_price, down_payment (dollars), annual_interest_rate (decimal), loan_term_months, trade_in_value (optional).\"\"\"\n",
    "    effective_price = vehicle_price - trade_in_value\n",
    "    loan_amount = effective_price - down_payment\n",
    "    monthly = _calculate_monthly_payment(loan_amount, annual_interest_rate, loan_term_months)\n",
    "    return json.dumps({\"loan_type\": \"auto\", \"vehicle_price\": vehicle_price, \"trade_in_value\": trade_in_value, \"effective_price\": round(effective_price, 2), \"down_payment\": down_payment, \"loan_amount\": round(loan_amount, 2), \"monthly_payment\": monthly, **_calculate_totals(loan_amount, monthly, loan_term_months)})\n",
    "\n",
    "@tool\n",
    "def check_loan_eligibility(age: int, monthly_income: float, credit_score: int, employment_status: str, requested_loan_amount: float, loan_type: str = \"personal\") -> str:\n",
    "    \"\"\"Check loan eligibility. Args: age, monthly_income, credit_score (300-850), employment_status (full_time/part_time/self_employed/unemployed), requested_loan_amount, loan_type.\"\"\"\n",
    "    reasons, is_eligible = [], True\n",
    "    if age < 18 or age > 65: is_eligible, reasons = False, reasons + [f\"Age {age} outside 18-65 range\"]\n",
    "    else: reasons.append(\"Age requirement met\")\n",
    "    min_income = 3000 if loan_type == \"personal\" else 4000\n",
    "    if monthly_income < min_income: is_eligible, reasons = False, reasons + [f\"Income below ${min_income}\"]\n",
    "    else: reasons.append(f\"Income OK (${monthly_income:,.0f}/month)\")\n",
    "    min_scores = {\"personal\": 600, \"mortgage\": 620, \"auto\": 580}\n",
    "    min_score = min_scores.get(loan_type, 600)\n",
    "    credit_rating = \"Poor\" if credit_score < min_score else \"Fair\" if credit_score < 670 else \"Good\" if credit_score < 740 else \"Excellent\"\n",
    "    if credit_score < min_score: is_eligible, reasons = False, reasons + [f\"Credit {credit_score} below {min_score}\"]\n",
    "    else: reasons.append(f\"Credit {credit_rating} ({credit_score})\")\n",
    "    if employment_status == \"unemployed\": is_eligible, reasons = False, reasons + [\"Employment required\"]\n",
    "    else: reasons.append(f\"Employment OK ({employment_status})\")\n",
    "    max_dti = 0.43 if loan_type == \"mortgage\" else 0.50\n",
    "    dti = (requested_loan_amount / 36) / monthly_income\n",
    "    if dti > max_dti: is_eligible, reasons = False, reasons + [f\"DTI {dti:.1%} exceeds {max_dti:.0%}\"]\n",
    "    return json.dumps({\"loan_type\": loan_type, \"is_eligible\": is_eligible, \"credit_rating\": credit_rating, \"estimated_dti\": round(dti, 3), \"reasons\": reasons, \"max_recommended_loan\": round(monthly_income * max_dti * 36, 2)})\n",
    "\n",
    "@tool\n",
    "def check_affordability(monthly_income: float, existing_monthly_debt: float, proposed_loan_amount: float, annual_interest_rate: float, loan_term_months: int) -> str:\n",
    "    \"\"\"Check loan affordability based on DTI. Args: monthly_income, existing_monthly_debt, proposed_loan_amount, annual_interest_rate, loan_term_months.\"\"\"\n",
    "    new_payment = _calculate_monthly_payment(proposed_loan_amount, annual_interest_rate, loan_term_months)\n",
    "    new_dti = (existing_monthly_debt + new_payment) / monthly_income\n",
    "    assessment = \"Excellent\" if new_dti <= 0.30 else \"Good\" if new_dti <= 0.40 else \"Acceptable\" if new_dti <= 0.50 else \"Not recommended\"\n",
    "    return json.dumps({\"is_affordable\": new_dti <= 0.50, \"monthly_income\": monthly_income, \"existing_monthly_debt\": existing_monthly_debt, \"new_monthly_payment\": new_payment, \"total_monthly_debt\": round(existing_monthly_debt + new_payment, 2), \"current_dti\": round(existing_monthly_debt / monthly_income, 3), \"new_dti\": round(new_dti, 3), \"assessment\": assessment})\n",
    "\n",
    "@tool\n",
    "def compare_loan_options(loan_amount: float, annual_interest_rate: float, term_options: str = \"36,48,60\") -> str:\n",
    "    \"\"\"Compare loan options across different terms. Args: loan_amount, annual_interest_rate, term_options (comma-separated months).\"\"\"\n",
    "    terms = [int(t.strip()) for t in term_options.split(\",\")]\n",
    "    comparisons = []\n",
    "    for months in terms:\n",
    "        monthly = _calculate_monthly_payment(loan_amount, annual_interest_rate, months)\n",
    "        totals = _calculate_totals(loan_amount, monthly, months)\n",
    "        comparisons.append({\"term_months\": months, \"monthly_payment\": monthly, \"total_interest\": totals[\"total_interest\"]})\n",
    "    comparisons.sort(key=lambda x: x[\"term_months\"])\n",
    "    if len(comparisons) > 1:\n",
    "        for c in comparisons: c[\"interest_savings_vs_longest\"] = round(comparisons[-1][\"total_interest\"] - c[\"total_interest\"], 2)\n",
    "    return json.dumps({\"loan_amount\": loan_amount, \"annual_interest_rate\": annual_interest_rate, \"comparisons\": comparisons})\n",
    "\n",
    "def get_all_tools():\n",
    "    return [calculate_personal_loan, calculate_mortgage, calculate_auto_loan, check_loan_eligibility, check_affordability, compare_loan_options]\n",
    "\n",
    "def get_tool_descriptions() -> str:\n",
    "    return \"\\\\n\".join([f\"- **{t.name}**: {t.description.split(chr(46))[0]}\" for t in get_all_tools()])\n",
    "'''\n",
    "\n",
    "# Write to file\n",
    "with open(\"langchain_tools.py\", \"w\") as f:\n",
    "    f.write(TOOLS_CODE.strip())\n",
    "print(\"âœ… Created langchain_tools.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ğŸ“¥ Import the tools we just created\nfrom langchain_tools import get_all_tools, get_tool_descriptions\n\ntools = get_all_tools()\nprint(f\"âœ… Loaded {len(tools)} tools:\\n\")\nprint(get_tool_descriptions())\n\n# ğŸ”§ Create tool descriptions mapping for ArgumentCorrectnessMetric\nTOOL_DESCRIPTIONS = {tool.name: tool.description for tool in tools}\nprint(f\"\\nğŸ“‹ Tool descriptions captured for ArgumentCorrectnessMetric\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## ğŸ¤– Part 3: Create LangChain ReAct Agent\n\nWe use a **ReAct (Reason + Act) agent** pattern that:\n1. Receives user input\n2. Decides which tool(s) to call\n3. Executes tools and observes results\n4. Generates final response"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langchain_core.messages import HumanMessage, AIMessage, ToolMessage\n",
    "\n",
    "# ğŸ§  Create LLM\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "# ğŸ“‹ System prompt defines agent behavior\n",
    "SYSTEM_PROMPT = \"\"\"You are a Loan Advisor assistant helping users with:\n",
    "- Personal loans, mortgages (home loans), and auto (car) loans\n",
    "- Payment calculations, eligibility checks, and affordability analysis\n",
    "\n",
    "Use the provided tools for accurate calculations. Be clear and helpful.\n",
    "Always show the key numbers in your response.\"\"\"\n",
    "\n",
    "# ğŸ¤– Create ReAct agent\n",
    "agent = create_react_agent(llm, tools, prompt=SYSTEM_PROMPT)\n",
    "print(\"ğŸ¤– Agent created with 6 loan advisor tools\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ğŸƒ Agent Runner - extracts outputs and tool context\nclass AgentRunner:\n    \"\"\"Wrapper that runs agent and extracts outputs for evaluation.\"\"\"\n    \n    def __init__(self, agent, tool_descriptions: dict):\n        self.agent = agent\n        self.tool_descriptions = tool_descriptions  # ğŸ”§ For ArgumentCorrectnessMetric\n    \n    def run(self, query: str) -> Dict[str, Any]:\n        \"\"\"Run agent and extract actual_output + retrieval_context + tools_called with args.\"\"\"\n        result = self.agent.invoke({\"messages\": [HumanMessage(content=query)]})\n        messages = result[\"messages\"]\n        \n        # ğŸ“¤ Extract final AI response\n        actual_output = next(\n            (m.content for m in reversed(messages) if isinstance(m, AIMessage) and m.content),\n            \"\"\n        )\n        \n        # ğŸ”§ Extract tool calls with full details for both metrics\n        tools_called = []      # List of tool names\n        tools_with_args = []   # List of dicts with name, description, input (args)\n        retrieval_context = []\n        \n        for msg in messages:\n            if isinstance(msg, AIMessage) and msg.tool_calls:\n                for tc in msg.tool_calls:\n                    if isinstance(tc, dict):\n                        tool_name = tc.get(\"name\", \"\")\n                        tool_args = tc.get(\"args\", {})  # ğŸ”§ Capture arguments!\n                        \n                        tools_called.append(tool_name)\n                        tools_with_args.append({\n                            \"name\": tool_name,\n                            \"description\": self.tool_descriptions.get(tool_name, \"\"),\n                            \"input\": tool_args,  # ğŸ”§ For ArgumentCorrectnessMetric\n                        })\n            if isinstance(msg, ToolMessage):\n                retrieval_context.append(msg.content)\n        \n        return {\n            \"actual_output\": actual_output,\n            \"tools_called\": tools_called,        # ğŸ”§ For ToolCorrectnessMetric\n            \"tools_with_args\": tools_with_args,  # ğŸ”§ For ArgumentCorrectnessMetric\n            \"retrieval_context\": retrieval_context,\n        }\n\nrunner = AgentRunner(agent, TOOL_DESCRIPTIONS)\nprint(\"âœ… AgentRunner ready (captures tool arguments for ArgumentCorrectnessMetric)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“ Part 4: Define Test Goldens\n",
    "\n",
    "**Goldens** = test templates that define:\n",
    "- `input`: User query to test\n",
    "- `expected_tools`: Which tools should be called\n",
    "- `expected_keywords`: Key terms that should appear in response\n",
    "\n",
    "We cover **5 categories** across all loan types:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GOLDENS = [\n",
    "    # ==========================================================================\n",
    "    # ğŸ’³ PERSONAL LOAN\n",
    "    # ==========================================================================\n",
    "    {\n",
    "        \"id\": \"personal_loan_basic\",\n",
    "        \"category\": \"ğŸ’³ Personal\",\n",
    "        \"input\": \"Calculate monthly payment for a $25,000 personal loan at 10% interest for 48 months.\",\n",
    "        \"expected_tools\": [\"calculate_personal_loan\"],\n",
    "        \"expected_keywords\": [\"634\", \"monthly\", \"payment\"],\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"personal_loan_comparison\",\n",
    "        \"category\": \"ğŸ’³ Personal\",\n",
    "        \"input\": \"Compare a $20,000 personal loan at 9% interest for 36, 48, and 60 months.\",\n",
    "        \"expected_tools\": [\"compare_loan_options\"],\n",
    "        \"expected_keywords\": [\"36\", \"48\", \"60\", \"interest\"],\n",
    "    },\n",
    "    \n",
    "    # ==========================================================================\n",
    "    # ğŸ  MORTGAGE (HOME LOAN)\n",
    "    # ==========================================================================\n",
    "    {\n",
    "        \"id\": \"mortgage_basic\",\n",
    "        \"category\": \"ğŸ  Mortgage\",\n",
    "        \"input\": \"Calculate mortgage payment for a $500,000 home with 20% down payment at 6.5% for 30 years.\",\n",
    "        \"expected_tools\": [\"calculate_mortgage\"],\n",
    "        \"expected_keywords\": [\"2,528\", \"monthly\", \"down payment\", \"400,000\"],\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"mortgage_low_down\",\n",
    "        \"category\": \"ğŸ  Mortgage\",\n",
    "        \"input\": \"What's the monthly payment for a $400,000 house with only 10% down at 7% for 30 years? Will I need PMI?\",\n",
    "        \"expected_tools\": [\"calculate_mortgage\"],\n",
    "        \"expected_keywords\": [\"LTV\", \"PMI\", \"monthly\"],\n",
    "    },\n",
    "    \n",
    "    # ==========================================================================\n",
    "    # ğŸš— AUTO LOAN (CAR LOAN)\n",
    "    # ==========================================================================\n",
    "    {\n",
    "        \"id\": \"auto_loan_basic\",\n",
    "        \"category\": \"ğŸš— Auto\",\n",
    "        \"input\": \"Calculate car loan payment for a $35,000 vehicle with $5,000 down at 5.9% for 60 months.\",\n",
    "        \"expected_tools\": [\"calculate_auto_loan\"],\n",
    "        \"expected_keywords\": [\"581\", \"monthly\", \"30,000\"],\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"auto_loan_trade_in\",\n",
    "        \"category\": \"ğŸš— Auto\",\n",
    "        \"input\": \"I want to buy a $40,000 car. I have a trade-in worth $8,000 and can put $2,000 down. What's my payment at 6% for 72 months?\",\n",
    "        \"expected_tools\": [\"calculate_auto_loan\"],\n",
    "        \"expected_keywords\": [\"trade\", \"monthly\", \"30,000\"],\n",
    "    },\n",
    "    \n",
    "    # ==========================================================================\n",
    "    # âœ… ELIGIBILITY CHECK\n",
    "    # ==========================================================================\n",
    "    {\n",
    "        \"id\": \"eligibility_good_credit\",\n",
    "        \"category\": \"âœ… Eligibility\",\n",
    "        \"input\": \"Check my loan eligibility: age 35, income $8,000/month, credit score 750, full-time employed, requesting $50,000 personal loan.\",\n",
    "        \"expected_tools\": [\"check_loan_eligibility\"],\n",
    "        \"expected_keywords\": [\"eligible\", \"Excellent\", \"750\"],\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"eligibility_low_credit\",\n",
    "        \"category\": \"âœ… Eligibility\",\n",
    "        \"input\": \"Am I eligible for a mortgage? Age 28, income $5,000/month, credit score 580, self-employed, want $300,000.\",\n",
    "        \"expected_tools\": [\"check_loan_eligibility\"],\n",
    "        \"expected_keywords\": [\"not\", \"credit\", \"580\"],\n",
    "    },\n",
    "    \n",
    "    # ==========================================================================\n",
    "    # ğŸ’° AFFORDABILITY ANALYSIS\n",
    "    # ==========================================================================\n",
    "    {\n",
    "        \"id\": \"affordability_ok\",\n",
    "        \"category\": \"ğŸ’° Affordability\",\n",
    "        \"input\": \"Can I afford a $30,000 car loan at 6% for 60 months? I earn $6,000/month with $500 existing debt.\",\n",
    "        \"expected_tools\": [\"check_affordability\"],\n",
    "        \"expected_keywords\": [\"affordable\", \"DTI\"],\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"affordability_stretched\",\n",
    "        \"category\": \"ğŸ’° Affordability\",\n",
    "        \"input\": \"Monthly income $4,000, existing debt $1,500. Can I afford a $25,000 loan at 8% for 48 months?\",\n",
    "        \"expected_tools\": [\"check_affordability\"],\n",
    "        \"expected_keywords\": [\"DTI\", \"exceed\", \"not\"],\n",
    "    },\n",
    "]\n",
    "\n",
    "# ğŸ“Š Summary\n",
    "print(f\"ğŸ“ Defined {len(GOLDENS)} test cases:\\n\")\n",
    "from collections import Counter\n",
    "categories = Counter(g[\"category\"] for g in GOLDENS)\n",
    "for cat, count in categories.items():\n",
    "    print(f\"  {cat}: {count} tests\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸƒ Part 5: Run Agent on All Test Cases\n",
    "\n",
    "Execute the agent for each Golden and collect:\n",
    "- `actual_output`: Agent's final response\n",
    "- `retrieval_context`: Tool outputs (used by Faithfulness/Hallucination metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from deepeval.dataset import EvaluationDataset, Golden\n\nprint(\"ğŸƒ Running agent on all test cases...\\n\")\nprint(\"=\" * 60)\n\ngoldens_with_output = []\n\nfor g in GOLDENS:\n    print(f\"\\n{g['category']} | {g['id']}\")\n    print(f\"  ğŸ“¥ Input: {g['input'][:50]}...\")\n    \n    result = runner.run(g[\"input\"])\n    print(f\"  ğŸ”§ Tools called: {result['tools_called']}\")\n    \n    # Create Golden with agent output\n    golden = Golden(\n        input=g[\"input\"],\n        actual_output=result[\"actual_output\"],\n        retrieval_context=result[\"retrieval_context\"],  # ğŸ’¡ Tool outputs!\n        additional_metadata={\n            \"test_id\": g[\"id\"],\n            \"category\": g[\"category\"],\n            \"expected_tools\": g[\"expected_tools\"],\n            \"actual_tools\": result[\"tools_called\"],\n            \"tools_with_args\": result[\"tools_with_args\"],  # ğŸ”§ For ArgumentCorrectness\n            \"expected_keywords\": g[\"expected_keywords\"],\n        }\n    )\n    goldens_with_output.append(golden)\n\nprint(\"\\n\" + \"=\" * 60)\ndataset = EvaluationDataset(goldens=goldens_with_output)\nprint(f\"\\nâœ… Created dataset with {len(dataset.goldens)} test cases\")\nprint(\"   Captured: tool names, arguments, and descriptions for evaluation\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## ğŸ“Š Part 6: Configure DeepEval Metrics\n\nWe use **5 metrics** across three categories:\n\n| Metric | Category | Uses Context? | What It Measures |\n|--------|----------|---------------|------------------|\n| `AnswerRelevancy` | ğŸ†“ Referenceless | âŒ No | Is response relevant to the question? |\n| `Faithfulness` | ğŸ“‹ Reference-based | âœ… Yes | Is response grounded in tool outputs? |\n| `Hallucination` | ğŸ“‹ Reference-based | âœ… Yes | Does response make things up? |\n| `ToolCorrectness` | ğŸ”§ Agent Tool | Uses `expected_tools` | Did agent call the right tools? |\n| `ArgumentCorrectness` | ğŸ”§ Agent Tool | Uses `tools_called.input` | Did agent parse correct parameters? |\n\n> ğŸ’¡ **Why these metrics?**  \n> - `AnswerRelevancy`: Ensures agent addresses the user's question  \n> - `Faithfulness`: Ensures agent uses tool outputs correctly  \n> - `Hallucination`: Catches fabricated numbers or facts\n> - `ToolCorrectness`: Validates agent's tool selection behavior\n> - `ArgumentCorrectness`: Validates agent extracts correct parameters from user input"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from deepeval import evaluate\nfrom deepeval.metrics import (\n    AnswerRelevancyMetric, \n    FaithfulnessMetric, \n    HallucinationMetric,\n    ToolCorrectnessMetric,      # ğŸ”§ Agent tool metric (deterministic)\n    ArgumentCorrectnessMetric,  # ğŸ”§ Agent tool metric (LLM-based)\n)\nfrom deepeval.test_case import LLMTestCase, ToolCall  # ğŸ”§ ToolCall for agent metrics\nfrom deepeval.tracing.api import MetricData  # ğŸ”‘ Native Pydantic model for serialization\n\n# ğŸ§  Judge model for evaluation\nEVAL_MODEL = \"gpt-4o-mini\"\n\n# ğŸ“Š Configure metrics\nmetrics = [\n    # ğŸ†“ REFERENCELESS: Only needs input + actual_output\n    AnswerRelevancyMetric(threshold=0.7, model=EVAL_MODEL),\n    \n    # ğŸ“‹ REFERENCE-BASED: Need retrieval_context (tool outputs)\n    FaithfulnessMetric(threshold=0.7, model=EVAL_MODEL),\n    HallucinationMetric(threshold=0.5, model=EVAL_MODEL),\n    \n    # ğŸ”§ AGENT TOOL: Validates tool calling behavior\n    ToolCorrectnessMetric(threshold=0.5),  # Deterministic, no LLM needed\n    ArgumentCorrectnessMetric(threshold=0.7, model=EVAL_MODEL),  # LLM evaluates args\n]\n\nprint(\"ğŸ“Š Metrics configured:\\n\")\nprint(\"  ğŸ†“ Referenceless (no ground truth needed):\")\nprint(f\"     â€¢ AnswerRelevancy (threshold: 0.7)\")\nprint(\"\\n  ğŸ“‹ Reference-based (uses retrieval_context):\")\nprint(f\"     â€¢ Faithfulness (threshold: 0.7)\")\nprint(f\"     â€¢ Hallucination (threshold: 0.5)\")\nprint(\"\\n  ğŸ”§ Agent Tool Metrics:\")\nprint(f\"     â€¢ ToolCorrectness (threshold: 0.5, deterministic)\")\nprint(f\"     â€¢ ArgumentCorrectness (threshold: 0.7, LLM-based)\")\nprint(\"\\nğŸ’¾ Using native MetricData for serialization/deserialization\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ğŸ”„ Convert Goldens to LLMTestCases\ntest_cases = []\nfor g in dataset.goldens:\n    meta = g.additional_metadata\n    \n    # ğŸ”§ For ToolCorrectnessMetric: simple ToolCall with name only\n    expected_tools = [ToolCall(name=name) for name in meta[\"expected_tools\"]]\n    \n    # ğŸ”§ For ArgumentCorrectnessMetric: full ToolCall with name, description, input\n    tools_called = [\n        ToolCall(\n            name=tc[\"name\"],\n            description=tc[\"description\"],\n            input=tc[\"input\"],  # ğŸ”§ The extracted parameters!\n        )\n        for tc in meta[\"tools_with_args\"]\n    ]\n    \n    test_case = LLMTestCase(\n        input=g.input,\n        actual_output=g.actual_output,\n        context=g.retrieval_context,           # ğŸ“‹ For Hallucination metric\n        retrieval_context=g.retrieval_context,  # ğŸ“‹ For Faithfulness metric\n        tools_called=tools_called,              # ğŸ”§ Full ToolCall for ArgumentCorrectness\n        expected_tools=expected_tools,          # ğŸ”§ For ToolCorrectness metric\n    )\n    test_cases.append(test_case)\n\nprint(f\"âœ… Created {len(test_cases)} LLMTestCases\")\nprint(\"\\nğŸ’¡ LLMTestCase fields used:\")\nprint(\"   â€¢ retrieval_context = tool outputs (for Faithfulness/Hallucination)\")\nprint(\"   â€¢ tools_called = ToolCall(name, description, input) (for ArgumentCorrectness)\")\nprint(\"   â€¢ expected_tools = ToolCall(name) (for ToolCorrectness)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## ğŸ”¬ Part 7: Run DeepEval Evaluation\n\nNow we run the LLM-as-Judge evaluation on all test cases.\n\n> ğŸ’¾ **Native JSON Export**: Using DeepEval's Pydantic `MetricData.model_dump()` for serialization.\n> \n> DeepEval's `MetricData` is a Pydantic BaseModel with fields:\n> - `name`, `score`, `success`, `threshold`, `reason`, `evaluation_model`, etc."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"ğŸ”¬ Running DeepEval evaluation...\")\nprint(\"   This may take a few minutes as the judge LLM evaluates each test case.\\n\")\n\n# ğŸš€ Run evaluation\nresults = evaluate(\n    test_cases=test_cases,\n    metrics=metrics,\n)\n\nprint(f\"\\nâœ… Evaluation complete!\")\n\n# ğŸ’¾ Save results using DeepEval's native Pydantic serialization\n# MetricData is a Pydantic BaseModel with model_dump() method\nRESULTS_FILE = \"eval_results.json\"\n\nnative_results = []\nfor i, golden in enumerate(dataset.goldens):\n    meta = golden.additional_metadata\n    test_result = results.test_results[i]\n    \n    # Build result dict with native MetricData serialization\n    result_entry = {\n        \"test_id\": meta[\"test_id\"],\n        \"category\": meta[\"category\"],\n        \"input\": golden.input,\n        \"success\": test_result.success,\n        # ğŸ”‘ Native Pydantic serialization: MetricData.model_dump()\n        \"metrics\": [\n            metric_data.model_dump() for metric_data in test_result.metrics_data\n        ] if test_result.metrics_data else []\n    }\n    native_results.append(result_entry)\n\n# Save to JSON\nwith open(RESULTS_FILE, \"w\") as f:\n    json.dump(native_results, f, indent=2, default=str)\n\nprint(f\"ğŸ’¾ Results saved to {RESULTS_FILE} (using MetricData.model_dump())\")\nprint(f\"   Contains {len(native_results)} test results with full metric details\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## ğŸ“ˆ Part 8: Analyze Results\n\nLoad results using DeepEval's native Pydantic deserialization.\n\n> ğŸ’¾ **Fully Native Approach**:\n> - **Save**: `MetricData.model_dump()` â†’ JSON\n> - **Load**: `MetricData.model_validate()` â†’ reconstruct objects\n> \n> This ensures type safety and schema validation through Pydantic."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import pandas as pd\n\n# ğŸ“Š Load native JSON results\nwith open(RESULTS_FILE, \"r\") as f:\n    native_results = json.load(f)\n\n# ğŸ”„ Transform using native MetricData.model_validate()\nresults_data = []\nfor entry in native_results:\n    row = {\n        \"test_id\": entry[\"test_id\"],\n        \"category\": entry[\"category\"],\n    }\n    \n    # ğŸ”‘ Reconstruct native MetricData objects using Pydantic\n    for metric_dict in entry[\"metrics\"]:\n        metric = MetricData.model_validate(metric_dict)  # Native deserialization!\n        \n        # Access attributes from native MetricData object\n        # Handle different metric name formats for cleaner column names\n        name = metric.name.replace(\"Metric\", \"\").replace(\" \", \"\")\n        if name == \"ToolCorrectness\":\n            name = \"ToolCorrect\"\n        elif name == \"ArgumentCorrectness\":\n            name = \"ArgCorrect\"\n        row[name] = round(metric.score, 2) if metric.score is not None else 0.0\n        row[f\"{name}_pass\"] = \"âœ…\" if metric.success else \"âŒ\"\n    \n    results_data.append(row)\n\ndf = pd.DataFrame(results_data)\n\nprint(\"ğŸ“Š RESULTS BY TEST CASE (loaded via MetricData.model_validate())\\n\")\nprint(df.to_string(index=False))\n\n# ğŸ“‹ Demonstrate native object reconstruction\nprint(f\"\\n\" + \"=\" * 50)\nprint(\"ğŸ“‹ Native MetricData Object Example:\")\nsample_metric = MetricData.model_validate(native_results[0][\"metrics\"][0])\nprint(f\"   Type: {type(sample_metric).__name__}\")\nprint(f\"   Name: {sample_metric.name}\")\nprint(f\"   Score: {sample_metric.score}\")\nprint(f\"   Success: {sample_metric.success}\")\nprint(f\"   Threshold: {sample_metric.threshold}\")\nprint(f\"   Reason: {sample_metric.reason[:80]}...\" if sample_metric.reason else \"   Reason: None\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ğŸ“Š Summary by category\nprint(\"ğŸ“Š AVERAGE SCORES BY CATEGORY\\n\")\nscore_cols = [\"AnswerRelevancy\", \"Faithfulness\", \"Hallucination\", \"ToolCorrect\", \"ArgCorrect\"]\nsummary = df.groupby(\"category\")[score_cols].mean().round(2)\nprint(summary.to_string())\n\n# ğŸ¨ Visual representation\nprint(\"\\n\" + \"=\" * 50)\nprint(\"ğŸ“ˆ CATEGORY PERFORMANCE\")\nprint(\"=\" * 50)\nfor cat in summary.index:\n    scores = summary.loc[cat]\n    avg = scores.mean()\n    bar = \"â–ˆ\" * int(avg * 20) + \"â–‘\" * (20 - int(avg * 20))\n    print(f\"\\n{cat}\")\n    print(f\"  {bar} {avg:.0%}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ğŸ“Š Overall pass rates\nprint(\"ğŸ“Š OVERALL PASS RATES\\n\")\n\npass_data = {\n    \"ğŸ†“ AnswerRelevancy\": (df[\"AnswerRelevancy_pass\"] == \"âœ…\").mean(),\n    \"ğŸ“‹ Faithfulness\": (df[\"Faithfulness_pass\"] == \"âœ…\").mean(),\n    \"ğŸ“‹ Hallucination\": (df[\"Hallucination_pass\"] == \"âœ…\").mean(),\n    \"ğŸ”§ ToolCorrectness\": (df[\"ToolCorrect_pass\"] == \"âœ…\").mean(),\n    \"ğŸ”§ ArgCorrectness\": (df[\"ArgCorrect_pass\"] == \"âœ…\").mean(),\n}\n\nfor metric, rate in pass_data.items():\n    bar = \"â–ˆ\" * int(rate * 20) + \"â–‘\" * (20 - int(rate * 20))\n    status = \"âœ…\" if rate >= 0.8 else \"âš ï¸\" if rate >= 0.6 else \"âŒ\"\n    print(f\"  {metric}: {bar} {rate:.0%} {status}\")\n\n# Overall\nall_pass = (\n    (df[\"AnswerRelevancy_pass\"] == \"âœ…\") & \n    (df[\"Faithfulness_pass\"] == \"âœ…\") & \n    (df[\"Hallucination_pass\"] == \"âœ…\") &\n    (df[\"ToolCorrect_pass\"] == \"âœ…\") &\n    (df[\"ArgCorrect_pass\"] == \"âœ…\")\n).mean()\nprint(f\"\\n  ğŸ¯ All metrics pass: {all_pass:.0%}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## ğŸ”§ Part 9: Understanding Agent Tool Metrics\n\nDeepEval provides two specialized metrics for evaluating agent tool-calling behavior:\n\n### ToolCorrectnessMetric (Deterministic)\n- Compares `tools_called` vs `expected_tools`\n- **No LLM needed** - purely deterministic comparison\n- Score = `Correctly Used Tools / Total Tools Called`\n\n### ArgumentCorrectnessMetric (LLM-based)\n- Evaluates if agent extracted **correct parameters** from user input\n- Uses LLM to judge if `tools_called.input` matches the task\n- Score = `Correctly Generated Input Parameters / Total Tool Calls`\n\n> ğŸ’¡ Both metrics use DeepEval's native `ToolCall` object with `name`, `description`, and `input` fields."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ğŸ”§ Detailed Agent Tool Metrics Analysis\nprint(\"ğŸ”§ AGENT TOOL METRICS BY TEST CASE\\n\")\n\nfor golden in dataset.goldens:\n    meta = golden.additional_metadata\n    expected = meta[\"expected_tools\"]\n    actual = meta[\"actual_tools\"]\n    tools_with_args = meta[\"tools_with_args\"]\n    \n    tool_match = set(expected).issubset(set(actual))\n    status = \"âœ…\" if tool_match else \"âŒ\"\n    \n    print(f\"{status} {meta['test_id']}\")\n    print(f\"   ğŸ“‹ Expected tools: {expected}\")\n    print(f\"   ğŸ”§ Called tools:   {actual}\")\n    \n    # Show extracted parameters for ArgumentCorrectness insight\n    for tc in tools_with_args:\n        print(f\"   ğŸ“¥ {tc['name']} args: {tc['input']}\")\n    print()\n\n# Summary\nprint(\"=\" * 50)\ntool_accuracy = df[\"ToolCorrect_pass\"].value_counts(normalize=True).get(\"âœ…\", 0)\narg_accuracy = df[\"ArgCorrect_pass\"].value_counts(normalize=True).get(\"âœ…\", 0)\nprint(f\"ğŸ¯ ToolCorrectness Pass Rate: {tool_accuracy:.0%}\")\nprint(f\"ğŸ¯ ArgumentCorrectness Pass Rate: {arg_accuracy:.0%}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## ğŸ“ Part 10: Key Takeaways\n\n### ğŸ”„ DeepEval Evaluation Flow\n\n```\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚   Goldens   â”‚ â”€â”€â–º â”‚   Agent     â”‚ â”€â”€â–º â”‚ LLMTestCase â”‚ â”€â”€â–º â”‚  Metrics    â”‚\nâ”‚ (templates) â”‚     â”‚ Execution   â”‚     â”‚             â”‚     â”‚             â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                          â”‚\n                          â–¼\n                   Tool Outputs = retrieval_context\n                   Tool Args = tools_called.input\n```\n\n---\n\n### ğŸ’¾ Native Serialization with Pydantic\n\nDeepEval's `MetricData` is a Pydantic BaseModel, enabling native serialization:\n\n```python\n# Save: MetricData â†’ dict â†’ JSON\nmetric_data.model_dump()\n\n# Load: JSON â†’ dict â†’ MetricData\nMetricData.model_validate(metric_dict)\n```\n\n---\n\n### ğŸ“Š Metric Categories Summary\n\n| Category | Metrics | Required Parameters | Use Case |\n|----------|---------|---------------------|----------|\n| ğŸ†“ **Referenceless** | AnswerRelevancy | `input`, `actual_output` only | No labeled data needed |\n| ğŸ“‹ **Reference-based** | Faithfulness, Hallucination | + `retrieval_context` | Ground truth from tools |\n| ğŸ”§ **Agent Tool** | ToolCorrectness | `expected_tools` | Validate tool selection |\n| ğŸ”§ **Agent Tool** | ArgumentCorrectness | `tools_called.input` | Validate parameter extraction |\n\n---\n\n### âœ… Best Practices\n\n1. **ğŸ”§ Tool outputs â†’ retrieval_context**  \n   Use tool results as ground truth for Faithfulness/Hallucination\n\n2. **ğŸ”§ ToolCorrectness for agents**  \n   Validate that your agent selects the right tools (deterministic)\n\n3. **ğŸ”§ ArgumentCorrectness for agents**  \n   Validate that your agent extracts correct parameters (LLM-based)\n\n4. **ğŸ’¾ Native Pydantic serialization**  \n   Use `model_dump()` / `model_validate()` for results persistence\n\n5. **ğŸ·ï¸ Category-based Goldens**  \n   Organize tests by loan type/scenario for better analysis\n\n---\n\n### ğŸ“š Resources\n\n- [DeepEval Documentation](https://docs.confident-ai.com/)\n- [DeepEval Metrics Guide](https://deepeval.com/docs/metrics-introduction)\n- [Tool Correctness Metric](https://deepeval.com/docs/metrics-tool-correctness)\n- [Argument Correctness Metric](https://deepeval.com/docs/metrics-argument-correctness)\n- [LangChain](https://python.langchain.com/)\n\n---\n\n**ğŸ‘ Found this helpful? Please upvote!**"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}