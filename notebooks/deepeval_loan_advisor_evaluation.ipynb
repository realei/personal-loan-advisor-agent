{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# ğŸ¯ DeepEval: Evaluating LangChain Agents with LLM-as-Judge\n\n[![DeepEval](https://img.shields.io/badge/DeepEval-Latest-purple.svg)](https://docs.confident-ai.com/)\n[![LangChain](https://img.shields.io/badge/LangChain-0.2+-green.svg)](https://python.langchain.com/)\n[![Python](https://img.shields.io/badge/Python-3.11+-blue.svg)](https://python.org)\n\n---\n\n## ğŸ“š What You'll Learn\n\nThis notebook demonstrates how to evaluate AI agents using **DeepEval's LLM-as-Judge** approach:\n\n| Section | Topic |\n|---------|-------|\n| ğŸ”§ Setup | Environment and API configuration |\n| ğŸ› ï¸ Tools | LangChain tools for loan calculations |\n| ğŸ¤– Agent | Building a ReAct agent with LangGraph |\n| ğŸ“ Goldens | Test templates covering multiple scenarios |\n| ğŸ“Š Metrics | **Reference-based** vs **Referenceless** evaluation |\n| ğŸ”¬ Evaluation | Running DeepEval on agent outputs |\n| ğŸ“ˆ Analysis | Understanding and interpreting results |\n\n---\n\n## ğŸ“ Key Concept: Reference vs Referenceless Metrics\n\nDeepEval metrics fall into two categories:\n\n### ğŸ“‹ Reference-Based Metrics\n> **Require ground truth** (`expected_output` or `retrieval_context`)\n\n| Metric | Required Parameter | What It Measures |\n|--------|-------------------|------------------|\n| `Faithfulness` | `retrieval_context` | Is output grounded in provided context? |\n| `Hallucination` | `context` | Does output contain fabricated info? |\n| `ContextualRecall` | `expected_output` + `retrieval_context` | Does context contain all needed info? |\n| `ContextualPrecision` | `expected_output` + `retrieval_context` | Is context focused and relevant? |\n\n### ğŸ†“ Referenceless Metrics  \n> **No ground truth needed** - evaluate output quality directly\n\n| Metric | Required Parameter | What It Measures |\n|--------|-------------------|------------------|\n| `AnswerRelevancy` | `input` only | Is response relevant to the question? |\n| `Toxicity` | `actual_output` only | Contains harmful content? |\n| `Bias` | `actual_output` only | Contains biased language? |\n| `TaskCompletion` | Agent trace | Did agent complete the task? |\n\n---\n\n## ğŸ¦ Demo: Loan Advisor Agent\n\nWe'll evaluate a **multi-type loan advisor** with 6 tools:\n\n| Tool | Loan Type | Description |\n|------|-----------|-------------|\n| `calculate_personal_loan` | ğŸ’³ Personal | Unsecured loan payment calculation |\n| `calculate_mortgage` | ğŸ  Mortgage | Home loan with LTV & PMI analysis |\n| `calculate_auto_loan` | ğŸš— Auto | Car loan with trade-in support |\n| `check_loan_eligibility` | âœ… All | Credit & income eligibility check |\n| `check_affordability` | ğŸ’° All | DTI-based affordability analysis |\n| `compare_loan_options` | ğŸ“Š All | Compare different loan terms |\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## ğŸ”§ Part 1: Environment Setup"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ğŸ“¦ Install dependencies (uncomment for Kaggle/Colab)\n# !pip install -q deepeval langchain langchain-openai langgraph pandas python-dotenv"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os\nimport json\nfrom typing import Dict, Any\nfrom getpass import getpass\n\n# =============================================================================\n# ğŸ”‘ API Key Setup\n# =============================================================================\n\nif not os.getenv(\"OPENAI_API_KEY\"):\n    os.environ[\"OPENAI_API_KEY\"] = getpass(\"ğŸ”‘ Enter your OpenAI API Key: \")\n\nprint(\"âœ… API key configured\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## ğŸ› ï¸ Part 2: LangChain Tools Definition\n\nThis notebook uses **6 LangChain tools** for loan calculations:\n\n| Tool | Type | Description |\n|------|------|-------------|\n| `calculate_personal_loan` | ğŸ’³ Personal | Unsecured loan payment calculation |\n| `calculate_mortgage` | ğŸ  Mortgage | Home loan with down payment & LTV ratio |\n| `calculate_auto_loan` | ğŸš— Auto | Car loan with trade-in support |\n| `check_loan_eligibility` | âœ… Check | Credit & income eligibility verification |\n| `check_affordability` | ğŸ’° Check | DTI-based affordability analysis |\n| `compare_loan_options` | ğŸ“Š Compare | Compare different loan terms side-by-side |\n\n> â¬‡ï¸ **Next cell:** Run once to create `langchain_tools.py`. You can **collapse it** after running (click the blue bar on the left in Kaggle/Jupyter)."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ğŸ“¦ Create langchain_tools.py (run once, then collapse this cell)\n# ğŸ‘† Click the blue bar on the left to collapse/expand in Kaggle/Jupyter\n\nTOOLS_CODE = '''\nimport json\nfrom langchain_core.tools import tool\n\ndef _calculate_monthly_payment(principal: float, annual_rate: float, months: int) -> float:\n    if annual_rate == 0: return principal / months\n    monthly_rate = annual_rate / 12\n    return round(principal * (monthly_rate * (1 + monthly_rate) ** months) / ((1 + monthly_rate) ** months - 1), 2)\n\ndef _calculate_totals(principal: float, monthly_payment: float, months: int) -> dict:\n    total_payment = monthly_payment * months\n    total_interest = total_payment - principal\n    return {\"total_payment\": round(total_payment, 2), \"total_interest\": round(total_interest, 2), \"interest_percentage\": round(total_interest / principal * 100, 2)}\n\n@tool\ndef calculate_personal_loan(loan_amount: float, annual_interest_rate: float, loan_term_months: int) -> str:\n    \"\"\"Calculate monthly payment for a personal loan. Args: loan_amount (dollars), annual_interest_rate (decimal, e.g. 0.10), loan_term_months.\"\"\"\n    monthly = _calculate_monthly_payment(loan_amount, annual_interest_rate, loan_term_months)\n    return json.dumps({\"loan_type\": \"personal\", \"loan_amount\": loan_amount, \"annual_interest_rate\": annual_interest_rate, \"loan_term_months\": loan_term_months, \"monthly_payment\": monthly, **_calculate_totals(loan_amount, monthly, loan_term_months)})\n\n@tool\ndef calculate_mortgage(home_price: float, down_payment_percent: float, annual_interest_rate: float, loan_term_years: int) -> str:\n    \"\"\"Calculate monthly payment for a mortgage. Args: home_price, down_payment_percent (decimal), annual_interest_rate (decimal), loan_term_years.\"\"\"\n    down_payment = home_price * down_payment_percent\n    loan_amount = home_price - down_payment\n    loan_term_months = loan_term_years * 12\n    ltv_ratio = loan_amount / home_price\n    monthly = _calculate_monthly_payment(loan_amount, annual_interest_rate, loan_term_months)\n    return json.dumps({\"loan_type\": \"mortgage\", \"home_price\": home_price, \"down_payment\": round(down_payment, 2), \"loan_amount\": round(loan_amount, 2), \"ltv_ratio\": round(ltv_ratio, 3), \"annual_interest_rate\": annual_interest_rate, \"loan_term_years\": loan_term_years, \"monthly_payment\": monthly, **_calculate_totals(loan_amount, monthly, loan_term_months), \"ltv_warning\": \"LTV exceeds 80%, PMI may be required\" if ltv_ratio > 0.80 else None})\n\n@tool\ndef calculate_auto_loan(vehicle_price: float, down_payment: float, annual_interest_rate: float, loan_term_months: int, trade_in_value: float = 0) -> str:\n    \"\"\"Calculate monthly payment for an auto loan. Args: vehicle_price, down_payment (dollars), annual_interest_rate (decimal), loan_term_months, trade_in_value (optional).\"\"\"\n    effective_price = vehicle_price - trade_in_value\n    loan_amount = effective_price - down_payment\n    monthly = _calculate_monthly_payment(loan_amount, annual_interest_rate, loan_term_months)\n    return json.dumps({\"loan_type\": \"auto\", \"vehicle_price\": vehicle_price, \"trade_in_value\": trade_in_value, \"effective_price\": round(effective_price, 2), \"down_payment\": down_payment, \"loan_amount\": round(loan_amount, 2), \"monthly_payment\": monthly, **_calculate_totals(loan_amount, monthly, loan_term_months)})\n\n@tool\ndef check_loan_eligibility(age: int, monthly_income: float, credit_score: int, employment_status: str, requested_loan_amount: float, loan_type: str = \"personal\") -> str:\n    \"\"\"Check loan eligibility. Args: age, monthly_income, credit_score (300-850), employment_status (full_time/part_time/self_employed/unemployed), requested_loan_amount, loan_type.\"\"\"\n    reasons, is_eligible = [], True\n    if age < 18 or age > 65: is_eligible, reasons = False, reasons + [f\"Age {age} outside 18-65 range\"]\n    else: reasons.append(\"Age requirement met\")\n    min_income = 3000 if loan_type == \"personal\" else 4000\n    if monthly_income < min_income: is_eligible, reasons = False, reasons + [f\"Income below ${min_income}\"]\n    else: reasons.append(f\"Income OK (${monthly_income:,.0f}/month)\")\n    min_scores = {\"personal\": 600, \"mortgage\": 620, \"auto\": 580}\n    min_score = min_scores.get(loan_type, 600)\n    credit_rating = \"Poor\" if credit_score < min_score else \"Fair\" if credit_score < 670 else \"Good\" if credit_score < 740 else \"Excellent\"\n    if credit_score < min_score: is_eligible, reasons = False, reasons + [f\"Credit {credit_score} below {min_score}\"]\n    else: reasons.append(f\"Credit {credit_rating} ({credit_score})\")\n    if employment_status == \"unemployed\": is_eligible, reasons = False, reasons + [\"Employment required\"]\n    else: reasons.append(f\"Employment OK ({employment_status})\")\n    max_dti = 0.43 if loan_type == \"mortgage\" else 0.50\n    dti = (requested_loan_amount / 36) / monthly_income\n    if dti > max_dti: is_eligible, reasons = False, reasons + [f\"DTI {dti:.1%} exceeds {max_dti:.0%}\"]\n    return json.dumps({\"loan_type\": loan_type, \"is_eligible\": is_eligible, \"credit_rating\": credit_rating, \"estimated_dti\": round(dti, 3), \"reasons\": reasons, \"max_recommended_loan\": round(monthly_income * max_dti * 36, 2)})\n\n@tool\ndef check_affordability(monthly_income: float, existing_monthly_debt: float, proposed_loan_amount: float, annual_interest_rate: float, loan_term_months: int) -> str:\n    \"\"\"Check loan affordability based on DTI. Args: monthly_income, existing_monthly_debt, proposed_loan_amount, annual_interest_rate, loan_term_months.\"\"\"\n    new_payment = _calculate_monthly_payment(proposed_loan_amount, annual_interest_rate, loan_term_months)\n    new_dti = (existing_monthly_debt + new_payment) / monthly_income\n    assessment = \"Excellent\" if new_dti <= 0.30 else \"Good\" if new_dti <= 0.40 else \"Acceptable\" if new_dti <= 0.50 else \"Not recommended\"\n    return json.dumps({\"is_affordable\": new_dti <= 0.50, \"monthly_income\": monthly_income, \"existing_monthly_debt\": existing_monthly_debt, \"new_monthly_payment\": new_payment, \"total_monthly_debt\": round(existing_monthly_debt + new_payment, 2), \"current_dti\": round(existing_monthly_debt / monthly_income, 3), \"new_dti\": round(new_dti, 3), \"assessment\": assessment})\n\n@tool\ndef compare_loan_options(loan_amount: float, annual_interest_rate: float, term_options: str = \"36,48,60\") -> str:\n    \"\"\"Compare loan options across different terms. Args: loan_amount, annual_interest_rate, term_options (comma-separated months).\"\"\"\n    terms = [int(t.strip()) for t in term_options.split(\",\")]\n    comparisons = []\n    for months in terms:\n        monthly = _calculate_monthly_payment(loan_amount, annual_interest_rate, months)\n        totals = _calculate_totals(loan_amount, monthly, months)\n        comparisons.append({\"term_months\": months, \"monthly_payment\": monthly, \"total_interest\": totals[\"total_interest\"]})\n    comparisons.sort(key=lambda x: x[\"term_months\"])\n    if len(comparisons) > 1:\n        for c in comparisons: c[\"interest_savings_vs_longest\"] = round(comparisons[-1][\"total_interest\"] - c[\"total_interest\"], 2)\n    return json.dumps({\"loan_amount\": loan_amount, \"annual_interest_rate\": annual_interest_rate, \"comparisons\": comparisons})\n\ndef get_all_tools():\n    return [calculate_personal_loan, calculate_mortgage, calculate_auto_loan, check_loan_eligibility, check_affordability, compare_loan_options]\n\ndef get_tool_descriptions() -> str:\n    return \"\\\\n\".join([f\"- **{t.name}**: {t.description.split(chr(46))[0]}\" for t in get_all_tools()])\n'''\n\n# Write to file\nwith open(\"langchain_tools.py\", \"w\") as f:\n    f.write(TOOLS_CODE.strip())\nprint(\"âœ… Created langchain_tools.py\")"
  },
  {
   "cell_type": "code",
   "source": "# ğŸ“¥ Import the tools we just created\nfrom langchain_tools import get_all_tools, get_tool_descriptions\n\ntools = get_all_tools()\nprint(f\"âœ… Loaded {len(tools)} tools:\\n\")\nprint(get_tool_descriptions())",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## ğŸ¤– Part 3: Create LangChain ReAct Agent\n\nWe use **LangGraph's `create_react_agent`** â€” a prebuilt ReAct (Reason + Act) agent that:\n1. Receives user input\n2. Decides which tool(s) to call\n3. Executes tools and observes results\n4. Generates final response"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from langchain_openai import ChatOpenAI\nfrom langgraph.prebuilt import create_react_agent\nfrom langchain_core.messages import HumanMessage, AIMessage, ToolMessage\n\n# ğŸ§  Create LLM\nllm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n\n# ğŸ“‹ System prompt defines agent behavior\nSYSTEM_PROMPT = \"\"\"You are a Loan Advisor assistant helping users with:\n- Personal loans, mortgages (home loans), and auto (car) loans\n- Payment calculations, eligibility checks, and affordability analysis\n\nUse the provided tools for accurate calculations. Be clear and helpful.\nAlways show the key numbers in your response.\"\"\"\n\n# ğŸ¤– Create ReAct agent\nagent = create_react_agent(llm, tools, prompt=SYSTEM_PROMPT)\nprint(\"ğŸ¤– Agent created with 6 loan advisor tools\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ğŸƒ Agent Runner - extracts outputs and tool context\nclass AgentRunner:\n    \"\"\"Wrapper that runs agent and extracts outputs for evaluation.\"\"\"\n    \n    def __init__(self, agent):\n        self.agent = agent\n    \n    def run(self, query: str) -> Dict[str, Any]:\n        \"\"\"Run agent and extract actual_output + retrieval_context.\"\"\"\n        result = self.agent.invoke({\"messages\": [HumanMessage(content=query)]})\n        messages = result[\"messages\"]\n        \n        # ğŸ“¤ Extract final AI response\n        actual_output = next(\n            (m.content for m in reversed(messages) if isinstance(m, AIMessage) and m.content),\n            \"\"\n        )\n        \n        # ğŸ”§ Extract tool calls and results\n        tools_called = []\n        retrieval_context = []  # ğŸ’¡ Tool outputs become retrieval_context!\n        \n        for msg in messages:\n            if isinstance(msg, AIMessage) and msg.tool_calls:\n                tools_called.extend([tc.get(\"name\", \"\") for tc in msg.tool_calls if isinstance(tc, dict)])\n            if isinstance(msg, ToolMessage):\n                retrieval_context.append(msg.content)\n        \n        return {\n            \"actual_output\": actual_output,\n            \"tools_called\": tools_called,\n            \"retrieval_context\": retrieval_context,  # ğŸ¯ Key for Faithfulness/Hallucination metrics!\n        }\n\nrunner = AgentRunner(agent)\nprint(\"âœ… AgentRunner ready\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## ğŸ“ Part 4: Define Test Goldens\n\n**Goldens** = test templates that define:\n- `input`: User query to test\n- `expected_tools`: Which tools should be called\n- `expected_keywords`: Key terms that should appear in response\n\nWe cover **5 categories** across all loan types:"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "GOLDENS = [\n    # ==========================================================================\n    # ğŸ’³ PERSONAL LOAN\n    # ==========================================================================\n    {\n        \"id\": \"personal_loan_basic\",\n        \"category\": \"ğŸ’³ Personal\",\n        \"input\": \"Calculate monthly payment for a $25,000 personal loan at 10% interest for 48 months.\",\n        \"expected_tools\": [\"calculate_personal_loan\"],\n        \"expected_keywords\": [\"634\", \"monthly\", \"payment\"],\n    },\n    {\n        \"id\": \"personal_loan_comparison\",\n        \"category\": \"ğŸ’³ Personal\",\n        \"input\": \"Compare a $20,000 personal loan at 9% interest for 36, 48, and 60 months.\",\n        \"expected_tools\": [\"compare_loan_options\"],\n        \"expected_keywords\": [\"36\", \"48\", \"60\", \"interest\"],\n    },\n    \n    # ==========================================================================\n    # ğŸ  MORTGAGE (HOME LOAN)\n    # ==========================================================================\n    {\n        \"id\": \"mortgage_basic\",\n        \"category\": \"ğŸ  Mortgage\",\n        \"input\": \"Calculate mortgage payment for a $500,000 home with 20% down payment at 6.5% for 30 years.\",\n        \"expected_tools\": [\"calculate_mortgage\"],\n        \"expected_keywords\": [\"2,528\", \"monthly\", \"down payment\", \"400,000\"],\n    },\n    {\n        \"id\": \"mortgage_low_down\",\n        \"category\": \"ğŸ  Mortgage\",\n        \"input\": \"What's the monthly payment for a $400,000 house with only 10% down at 7% for 30 years? Will I need PMI?\",\n        \"expected_tools\": [\"calculate_mortgage\"],\n        \"expected_keywords\": [\"LTV\", \"PMI\", \"monthly\"],\n    },\n    \n    # ==========================================================================\n    # ğŸš— AUTO LOAN (CAR LOAN)\n    # ==========================================================================\n    {\n        \"id\": \"auto_loan_basic\",\n        \"category\": \"ğŸš— Auto\",\n        \"input\": \"Calculate car loan payment for a $35,000 vehicle with $5,000 down at 5.9% for 60 months.\",\n        \"expected_tools\": [\"calculate_auto_loan\"],\n        \"expected_keywords\": [\"581\", \"monthly\", \"30,000\"],\n    },\n    {\n        \"id\": \"auto_loan_trade_in\",\n        \"category\": \"ğŸš— Auto\",\n        \"input\": \"I want to buy a $40,000 car. I have a trade-in worth $8,000 and can put $2,000 down. What's my payment at 6% for 72 months?\",\n        \"expected_tools\": [\"calculate_auto_loan\"],\n        \"expected_keywords\": [\"trade\", \"monthly\", \"30,000\"],\n    },\n    \n    # ==========================================================================\n    # âœ… ELIGIBILITY CHECK\n    # ==========================================================================\n    {\n        \"id\": \"eligibility_good_credit\",\n        \"category\": \"âœ… Eligibility\",\n        \"input\": \"Check my loan eligibility: age 35, income $8,000/month, credit score 750, full-time employed, requesting $50,000 personal loan.\",\n        \"expected_tools\": [\"check_loan_eligibility\"],\n        \"expected_keywords\": [\"eligible\", \"Excellent\", \"750\"],\n    },\n    {\n        \"id\": \"eligibility_low_credit\",\n        \"category\": \"âœ… Eligibility\",\n        \"input\": \"Am I eligible for a mortgage? Age 28, income $5,000/month, credit score 580, self-employed, want $300,000.\",\n        \"expected_tools\": [\"check_loan_eligibility\"],\n        \"expected_keywords\": [\"not\", \"credit\", \"580\"],\n    },\n    \n    # ==========================================================================\n    # ğŸ’° AFFORDABILITY ANALYSIS\n    # ==========================================================================\n    {\n        \"id\": \"affordability_ok\",\n        \"category\": \"ğŸ’° Affordability\",\n        \"input\": \"Can I afford a $30,000 car loan at 6% for 60 months? I earn $6,000/month with $500 existing debt.\",\n        \"expected_tools\": [\"check_affordability\"],\n        \"expected_keywords\": [\"affordable\", \"DTI\"],\n    },\n    {\n        \"id\": \"affordability_stretched\",\n        \"category\": \"ğŸ’° Affordability\",\n        \"input\": \"Monthly income $4,000, existing debt $1,500. Can I afford a $25,000 loan at 8% for 48 months?\",\n        \"expected_tools\": [\"check_affordability\"],\n        \"expected_keywords\": [\"DTI\", \"exceed\", \"not\"],\n    },\n]\n\n# ğŸ“Š Summary\nprint(f\"ğŸ“ Defined {len(GOLDENS)} test cases:\\n\")\nfrom collections import Counter\ncategories = Counter(g[\"category\"] for g in GOLDENS)\nfor cat, count in categories.items():\n    print(f\"  {cat}: {count} tests\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## ğŸƒ Part 5: Run Agent on All Test Cases\n\nExecute the agent for each Golden and collect:\n- `actual_output`: Agent's final response\n- `retrieval_context`: Tool outputs (used by Faithfulness/Hallucination metrics)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from deepeval.dataset import EvaluationDataset, Golden\n\nprint(\"ğŸƒ Running agent on all test cases...\\n\")\nprint(\"=\" * 60)\n\ngoldens_with_output = []\n\nfor g in GOLDENS:\n    print(f\"\\n{g['category']} | {g['id']}\")\n    print(f\"  ğŸ“¥ Input: {g['input'][:50]}...\")\n    \n    result = runner.run(g[\"input\"])\n    print(f\"  ğŸ”§ Tools called: {result['tools_called']}\")\n    \n    # Create Golden with agent output\n    golden = Golden(\n        input=g[\"input\"],\n        actual_output=result[\"actual_output\"],\n        retrieval_context=result[\"retrieval_context\"],  # ğŸ’¡ Tool outputs!\n        additional_metadata={\n            \"test_id\": g[\"id\"],\n            \"category\": g[\"category\"],\n            \"expected_tools\": g[\"expected_tools\"],\n            \"actual_tools\": result[\"tools_called\"],\n            \"expected_keywords\": g[\"expected_keywords\"],\n        }\n    )\n    goldens_with_output.append(golden)\n\nprint(\"\\n\" + \"=\" * 60)\ndataset = EvaluationDataset(goldens=goldens_with_output)\nprint(f\"\\nâœ… Created dataset with {len(dataset.goldens)} test cases\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## ğŸ“Š Part 6: Configure DeepEval Metrics\n\nWe use **3 metrics** that represent both categories:\n\n| Metric | Type | Uses Context? | What It Measures |\n|--------|------|---------------|------------------|\n| `AnswerRelevancy` | ğŸ†“ Referenceless | âŒ No | Is response relevant to the question? |\n| `Faithfulness` | ğŸ“‹ Reference-based | âœ… Yes | Is response grounded in tool outputs? |\n| `Hallucination` | ğŸ“‹ Reference-based | âœ… Yes | Does response make things up? |\n\n> ğŸ’¡ **Why these metrics?**  \n> - `AnswerRelevancy`: Ensures agent addresses the user's question  \n> - `Faithfulness`: Ensures agent uses tool outputs correctly  \n> - `Hallucination`: Catches fabricated numbers or facts"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from deepeval import evaluate\nfrom deepeval.metrics import AnswerRelevancyMetric, FaithfulnessMetric, HallucinationMetric\nfrom deepeval.test_case import LLMTestCase\n\n# ğŸ§  Judge model for evaluation\nEVAL_MODEL = \"gpt-4o-mini\"\n\n# ğŸ“Š Configure metrics\nmetrics = [\n    # ğŸ†“ REFERENCELESS: Only needs input + actual_output\n    AnswerRelevancyMetric(threshold=0.7, model=EVAL_MODEL),\n    \n    # ğŸ“‹ REFERENCE-BASED: Need retrieval_context (tool outputs)\n    FaithfulnessMetric(threshold=0.7, model=EVAL_MODEL),\n    HallucinationMetric(threshold=0.5, model=EVAL_MODEL),\n]\n\nprint(\"ğŸ“Š Metrics configured:\\n\")\nprint(\"  ğŸ†“ Referenceless (no ground truth needed):\")\nprint(f\"     â€¢ AnswerRelevancy (threshold: 0.7)\")\nprint(\"\\n  ğŸ“‹ Reference-based (uses retrieval_context):\")\nprint(f\"     â€¢ Faithfulness (threshold: 0.7)\")\nprint(f\"     â€¢ Hallucination (threshold: 0.5)\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ğŸ”„ Convert Goldens to LLMTestCases\ntest_cases = [\n    LLMTestCase(\n        input=g.input,\n        actual_output=g.actual_output,\n        context=g.retrieval_context,           # ğŸ“‹ For Hallucination metric\n        retrieval_context=g.retrieval_context,  # ğŸ“‹ For Faithfulness metric\n    )\n    for g in dataset.goldens\n]\n\nprint(f\"âœ… Created {len(test_cases)} LLMTestCases\")\nprint(\"\\nğŸ’¡ Note: retrieval_context = tool outputs = ground truth for reference-based metrics\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## ğŸ”¬ Part 7: Run DeepEval Evaluation\n\nNow we run the LLM-as-Judge evaluation on all test cases."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"ğŸ”¬ Running DeepEval evaluation...\")\nprint(\"   This may take a few minutes as the judge LLM evaluates each test case.\\n\")\n\nresults = evaluate(\n    test_cases=test_cases,\n    metrics=metrics,\n)\n\nprint(\"\\nâœ… Evaluation complete!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## ğŸ“ˆ Part 8: Analyze Results\n\nLet's examine the evaluation results in detail."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import pandas as pd\n\n# ğŸ“Š Build results dataframe\nresults_data = []\n\nfor tc, golden in zip(test_cases, dataset.goldens):\n    meta = golden.additional_metadata\n    row = {\n        \"test_id\": meta[\"test_id\"], \n        \"category\": meta[\"category\"]\n    }\n    \n    # Evaluate each metric\n    for metric in metrics:\n        metric.measure(tc)\n        name = metric.__class__.__name__.replace(\"Metric\", \"\")\n        row[name] = round(metric.score, 2)\n        row[f\"{name}_pass\"] = \"âœ…\" if metric.is_successful() else \"âŒ\"\n    \n    results_data.append(row)\n\ndf = pd.DataFrame(results_data)\n\nprint(\"ğŸ“Š RESULTS BY TEST CASE\\n\")\nprint(df.to_string(index=False))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ğŸ“Š Summary by category\nprint(\"ğŸ“Š AVERAGE SCORES BY CATEGORY\\n\")\nscore_cols = [\"AnswerRelevancy\", \"Faithfulness\", \"Hallucination\"]\nsummary = df.groupby(\"category\")[score_cols].mean().round(2)\nprint(summary.to_string())\n\n# ğŸ¨ Visual representation\nprint(\"\\n\" + \"=\" * 50)\nprint(\"ğŸ“ˆ CATEGORY PERFORMANCE\")\nprint(\"=\" * 50)\nfor cat in summary.index:\n    scores = summary.loc[cat]\n    avg = scores.mean()\n    bar = \"â–ˆ\" * int(avg * 20) + \"â–‘\" * (20 - int(avg * 20))\n    print(f\"\\n{cat}\")\n    print(f\"  {bar} {avg:.0%}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ğŸ“Š Overall pass rates\nprint(\"ğŸ“Š OVERALL PASS RATES\\n\")\n\npass_data = {\n    \"ğŸ†“ AnswerRelevancy\": (df[\"AnswerRelevancy_pass\"] == \"âœ…\").mean(),\n    \"ğŸ“‹ Faithfulness\": (df[\"Faithfulness_pass\"] == \"âœ…\").mean(),\n    \"ğŸ“‹ Hallucination\": (df[\"Hallucination_pass\"] == \"âœ…\").mean(),\n}\n\nfor metric, rate in pass_data.items():\n    bar = \"â–ˆ\" * int(rate * 20) + \"â–‘\" * (20 - int(rate * 20))\n    status = \"âœ…\" if rate >= 0.8 else \"âš ï¸\" if rate >= 0.6 else \"âŒ\"\n    print(f\"  {metric}: {bar} {rate:.0%} {status}\")\n\n# Overall\nall_pass = (\n    (df[\"AnswerRelevancy_pass\"] == \"âœ…\") & \n    (df[\"Faithfulness_pass\"] == \"âœ…\") & \n    (df[\"Hallucination_pass\"] == \"âœ…\")\n).mean()\nprint(f\"\\n  ğŸ¯ All metrics pass: {all_pass:.0%}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## ğŸ”§ Part 9: Tool Call Validation\n\nBeyond LLM-as-Judge metrics, we also validate that the agent used the correct tools."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"ğŸ”§ TOOL CALL VALIDATION\\n\")\n\ntool_results = []\nfor golden in dataset.goldens:\n    meta = golden.additional_metadata\n    expected = set(meta[\"expected_tools\"])\n    actual = set(meta[\"actual_tools\"])\n    match = expected.issubset(actual)\n    \n    status = \"âœ…\" if match else \"âŒ\"\n    print(f\"  {status} {meta['test_id']}\")\n    if not match:\n        print(f\"      Expected: {list(expected)}\")\n        print(f\"      Got: {list(actual)}\")\n    \n    tool_results.append(match)\n\naccuracy = sum(tool_results) / len(tool_results)\nprint(f\"\\nğŸ¯ Tool Call Accuracy: {accuracy:.0%}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## ğŸ“ Part 10: Key Takeaways\n\n### ğŸ”„ DeepEval Evaluation Flow\n\n```\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚   Goldens   â”‚ â”€â”€â–º â”‚   Agent     â”‚ â”€â”€â–º â”‚ LLMTestCase â”‚ â”€â”€â–º â”‚  Metrics    â”‚\nâ”‚ (templates) â”‚     â”‚ Execution   â”‚     â”‚             â”‚     â”‚             â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                          â”‚\n                          â–¼\n                   Tool Outputs = retrieval_context\n                   (ground truth for reference-based metrics)\n```\n\n---\n\n### ğŸ“Š Metric Types Summary\n\n| Type | Metrics | Required Parameters | Use Case |\n|------|---------|---------------------|----------|\n| ğŸ†“ **Referenceless** | AnswerRelevancy, Toxicity, Bias | `input`, `actual_output` only | No labeled data needed |\n| ğŸ“‹ **Reference-based** | Faithfulness, Hallucination | + `retrieval_context` | Ground truth from tools |\n| ğŸ“‹ **Reference-based** | ContextualRecall, Precision | + `expected_output` | Need expected answers |\n\n---\n\n### âœ… Best Practices\n\n1. **ğŸ”§ Tool outputs â†’ retrieval_context**  \n   Use tool results as ground truth for Faithfulness/Hallucination\n\n2. **ğŸ“ Separate tools module**  \n   Keep notebook focused on evaluation logic\n\n3. **ğŸ·ï¸ Category-based Goldens**  \n   Organize tests by loan type/scenario for better analysis\n\n4. **ğŸ” Multi-level validation**  \n   Combine LLM-as-Judge metrics with deterministic tool validation\n\n5. **ğŸ“ˆ Track by category**  \n   Identify which scenarios need improvement\n\n---\n\n### ğŸ“š Resources\n\n- [DeepEval Documentation](https://docs.confident-ai.com/)\n- [LangChain](https://python.langchain.com/)\n- [LangGraph](https://langchain-ai.github.io/langgraph/)\n- [DeepEval Metrics Guide](https://deepeval.com/docs/metrics-introduction)\n\n---\n\n**ğŸ‘ Found this helpful? Please upvote!**"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}