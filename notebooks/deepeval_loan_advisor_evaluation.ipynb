{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# ğŸ¯ DeepEval: Evaluating LangChain Agents with LLM-as-Judge\n\n[![DeepEval](https://img.shields.io/badge/DeepEval-Latest-purple.svg)](https://docs.confident-ai.com/)\n[![LangChain](https://img.shields.io/badge/LangChain-0.2+-green.svg)](https://python.langchain.com/)\n[![Python](https://img.shields.io/badge/Python-3.11+-blue.svg)](https://python.org)\n\n---\n\n## ğŸ“š What You'll Learn\n\nThis notebook demonstrates how to evaluate AI agents using **DeepEval's LLM-as-Judge** approach:\n\n| Section | Topic |\n|---------|-------|\n| ğŸ”§ Setup | Environment and API configuration |\n| ğŸ› ï¸ Tools | LangChain tools for loan calculations |\n| ğŸ¤– Agent | Building a ReAct agent with LangGraph |\n| ğŸ“ Goldens | Test templates covering multiple scenarios |\n| ğŸ“Š Metrics | **Reference-based** vs **Referenceless** evaluation |\n| ğŸ”¬ Evaluation | Running DeepEval on agent outputs |\n| ğŸ“ˆ Analysis | Understanding and interpreting results |\n\n---\n\n## ğŸ“ Key Concept: Reference vs Referenceless Metrics\n\nDeepEval metrics fall into two categories:\n\n### ğŸ“‹ Reference-Based Metrics\n> **Require ground truth** (`expected_output` or `retrieval_context`)\n\n| Metric | Required Parameter | What It Measures |\n|--------|-------------------|------------------|\n| `Faithfulness` | `retrieval_context` | Is output grounded in provided context? |\n| `Hallucination` | `context` | Does output contain fabricated info? |\n| `ContextualRecall` | `expected_output` + `retrieval_context` | Does context contain all needed info? |\n| `ContextualPrecision` | `expected_output` + `retrieval_context` | Is context focused and relevant? |\n\n### ğŸ†“ Referenceless Metrics  \n> **No ground truth needed** - evaluate output quality directly\n\n| Metric | Required Parameter | What It Measures |\n|--------|-------------------|------------------|\n| `AnswerRelevancy` | `input` only | Is response relevant to the question? |\n| `Toxicity` | `actual_output` only | Contains harmful content? |\n| `Bias` | `actual_output` only | Contains biased language? |\n| `TaskCompletion` | Agent trace | Did agent complete the task? |\n\n---\n\n## ğŸ¦ Demo: Loan Advisor Agent\n\nWe'll evaluate a **multi-type loan advisor** with 6 tools:\n\n| Tool | Loan Type | Description |\n|------|-----------|-------------|\n| `calculate_personal_loan` | ğŸ’³ Personal | Unsecured loan payment calculation |\n| `calculate_mortgage` | ğŸ  Mortgage | Home loan with LTV & PMI analysis |\n| `calculate_auto_loan` | ğŸš— Auto | Car loan with trade-in support |\n| `check_loan_eligibility` | âœ… All | Credit & income eligibility check |\n| `check_affordability` | ğŸ’° All | DTI-based affordability analysis |\n| `compare_loan_options` | ğŸ“Š All | Compare different loan terms |\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## ğŸ”§ Part 1: Environment Setup"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ğŸ“¦ Install dependencies (uncomment for Kaggle/Colab)\n# !pip install -q deepeval langchain langchain-openai langgraph pandas python-dotenv"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os\nimport json\nfrom typing import Dict, Any\n\n# =============================================================================\n# ğŸ”‘ API Key Setup (Kaggle / Colab / Local)\n# =============================================================================\n\ndef setup_api_key():\n    \"\"\"Load API key from various sources with priority order.\"\"\"\n    \n    # 1ï¸âƒ£ Environment variable (highest priority)\n    if os.getenv(\"OPENAI_API_KEY\"):\n        return \"environment\"\n    \n    # 2ï¸âƒ£ Kaggle Secrets\n    try:\n        from kaggle_secrets import UserSecretsClient\n        os.environ[\"OPENAI_API_KEY\"] = UserSecretsClient().get_secret(\"OPENAI_API_KEY\")\n        return \"Kaggle Secrets\"\n    except: pass\n    \n    # 3ï¸âƒ£ Google Colab Secrets\n    try:\n        from google.colab import userdata\n        os.environ[\"OPENAI_API_KEY\"] = userdata.get(\"OPENAI_API_KEY\")\n        return \"Colab Secrets\"\n    except: pass\n    \n    # 4ï¸âƒ£ Local .env file\n    try:\n        from dotenv import load_dotenv\n        load_dotenv()\n        if os.getenv(\"OPENAI_API_KEY\"):\n            return \".env file\"\n    except: pass\n    \n    return None\n\nsource = setup_api_key()\nif source:\n    print(f\"âœ… API key loaded from {source}\")\nelse:\n    raise ValueError(\"âŒ OPENAI_API_KEY not found. Set via Kaggle/Colab Secrets or .env file.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## ğŸ› ï¸ Part 2: Load Loan Advisor Tools\n\nThe tools are defined in `langchain_tools.py` â€” a separate module for cleaner organization.\n\nEach tool uses LangChain's `@tool` decorator and returns JSON for easy parsing."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ğŸ› ï¸ Import tools from external module\nfrom langchain_tools import (\n    get_all_tools,\n    get_tool_descriptions,\n    calculate_personal_loan,\n    calculate_mortgage,\n    calculate_auto_loan,\n    check_loan_eligibility,\n    check_affordability,\n    compare_loan_options,\n)\n\ntools = get_all_tools()\nprint(f\"ğŸ› ï¸ Loaded {len(tools)} tools:\\n\")\nprint(get_tool_descriptions())"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## ğŸ¤– Part 3: Create LangChain ReAct Agent\n\nWe use **LangGraph's `create_react_agent`** â€” a prebuilt ReAct (Reason + Act) agent that:\n1. Receives user input\n2. Decides which tool(s) to call\n3. Executes tools and observes results\n4. Generates final response"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from langchain_openai import ChatOpenAI\nfrom langgraph.prebuilt import create_react_agent\nfrom langchain_core.messages import HumanMessage, AIMessage, ToolMessage\n\n# ğŸ§  Create LLM\nllm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n\n# ğŸ“‹ System prompt defines agent behavior\nSYSTEM_PROMPT = \"\"\"You are a Loan Advisor assistant helping users with:\n- Personal loans, mortgages (home loans), and auto (car) loans\n- Payment calculations, eligibility checks, and affordability analysis\n\nUse the provided tools for accurate calculations. Be clear and helpful.\nAlways show the key numbers in your response.\"\"\"\n\n# ğŸ¤– Create ReAct agent\nagent = create_react_agent(llm, tools, prompt=SYSTEM_PROMPT)\nprint(\"ğŸ¤– Agent created with 6 loan advisor tools\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ğŸƒ Agent Runner - extracts outputs and tool context\nclass AgentRunner:\n    \"\"\"Wrapper that runs agent and extracts outputs for evaluation.\"\"\"\n    \n    def __init__(self, agent):\n        self.agent = agent\n    \n    def run(self, query: str) -> Dict[str, Any]:\n        \"\"\"Run agent and extract actual_output + retrieval_context.\"\"\"\n        result = self.agent.invoke({\"messages\": [HumanMessage(content=query)]})\n        messages = result[\"messages\"]\n        \n        # ğŸ“¤ Extract final AI response\n        actual_output = next(\n            (m.content for m in reversed(messages) if isinstance(m, AIMessage) and m.content),\n            \"\"\n        )\n        \n        # ğŸ”§ Extract tool calls and results\n        tools_called = []\n        retrieval_context = []  # ğŸ’¡ Tool outputs become retrieval_context!\n        \n        for msg in messages:\n            if isinstance(msg, AIMessage) and msg.tool_calls:\n                tools_called.extend([tc.get(\"name\", \"\") for tc in msg.tool_calls if isinstance(tc, dict)])\n            if isinstance(msg, ToolMessage):\n                retrieval_context.append(msg.content)\n        \n        return {\n            \"actual_output\": actual_output,\n            \"tools_called\": tools_called,\n            \"retrieval_context\": retrieval_context,  # ğŸ¯ Key for Faithfulness/Hallucination metrics!\n        }\n\nrunner = AgentRunner(agent)\nprint(\"âœ… AgentRunner ready\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## ğŸ“ Part 4: Define Test Goldens\n\n**Goldens** = test templates that define:\n- `input`: User query to test\n- `expected_tools`: Which tools should be called\n- `expected_keywords`: Key terms that should appear in response\n\nWe cover **5 categories** across all loan types:"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "GOLDENS = [\n    # ==========================================================================\n    # ğŸ’³ PERSONAL LOAN\n    # ==========================================================================\n    {\n        \"id\": \"personal_loan_basic\",\n        \"category\": \"ğŸ’³ Personal\",\n        \"input\": \"Calculate monthly payment for a $25,000 personal loan at 10% interest for 48 months.\",\n        \"expected_tools\": [\"calculate_personal_loan\"],\n        \"expected_keywords\": [\"634\", \"monthly\", \"payment\"],\n    },\n    {\n        \"id\": \"personal_loan_comparison\",\n        \"category\": \"ğŸ’³ Personal\",\n        \"input\": \"Compare a $20,000 personal loan at 9% interest for 36, 48, and 60 months.\",\n        \"expected_tools\": [\"compare_loan_options\"],\n        \"expected_keywords\": [\"36\", \"48\", \"60\", \"interest\"],\n    },\n    \n    # ==========================================================================\n    # ğŸ  MORTGAGE (HOME LOAN)\n    # ==========================================================================\n    {\n        \"id\": \"mortgage_basic\",\n        \"category\": \"ğŸ  Mortgage\",\n        \"input\": \"Calculate mortgage payment for a $500,000 home with 20% down payment at 6.5% for 30 years.\",\n        \"expected_tools\": [\"calculate_mortgage\"],\n        \"expected_keywords\": [\"2,528\", \"monthly\", \"down payment\", \"400,000\"],\n    },\n    {\n        \"id\": \"mortgage_low_down\",\n        \"category\": \"ğŸ  Mortgage\",\n        \"input\": \"What's the monthly payment for a $400,000 house with only 10% down at 7% for 30 years? Will I need PMI?\",\n        \"expected_tools\": [\"calculate_mortgage\"],\n        \"expected_keywords\": [\"LTV\", \"PMI\", \"monthly\"],\n    },\n    \n    # ==========================================================================\n    # ğŸš— AUTO LOAN (CAR LOAN)\n    # ==========================================================================\n    {\n        \"id\": \"auto_loan_basic\",\n        \"category\": \"ğŸš— Auto\",\n        \"input\": \"Calculate car loan payment for a $35,000 vehicle with $5,000 down at 5.9% for 60 months.\",\n        \"expected_tools\": [\"calculate_auto_loan\"],\n        \"expected_keywords\": [\"581\", \"monthly\", \"30,000\"],\n    },\n    {\n        \"id\": \"auto_loan_trade_in\",\n        \"category\": \"ğŸš— Auto\",\n        \"input\": \"I want to buy a $40,000 car. I have a trade-in worth $8,000 and can put $2,000 down. What's my payment at 6% for 72 months?\",\n        \"expected_tools\": [\"calculate_auto_loan\"],\n        \"expected_keywords\": [\"trade\", \"monthly\", \"30,000\"],\n    },\n    \n    # ==========================================================================\n    # âœ… ELIGIBILITY CHECK\n    # ==========================================================================\n    {\n        \"id\": \"eligibility_good_credit\",\n        \"category\": \"âœ… Eligibility\",\n        \"input\": \"Check my loan eligibility: age 35, income $8,000/month, credit score 750, full-time employed, requesting $50,000 personal loan.\",\n        \"expected_tools\": [\"check_loan_eligibility\"],\n        \"expected_keywords\": [\"eligible\", \"Excellent\", \"750\"],\n    },\n    {\n        \"id\": \"eligibility_low_credit\",\n        \"category\": \"âœ… Eligibility\",\n        \"input\": \"Am I eligible for a mortgage? Age 28, income $5,000/month, credit score 580, self-employed, want $300,000.\",\n        \"expected_tools\": [\"check_loan_eligibility\"],\n        \"expected_keywords\": [\"not\", \"credit\", \"580\"],\n    },\n    \n    # ==========================================================================\n    # ğŸ’° AFFORDABILITY ANALYSIS\n    # ==========================================================================\n    {\n        \"id\": \"affordability_ok\",\n        \"category\": \"ğŸ’° Affordability\",\n        \"input\": \"Can I afford a $30,000 car loan at 6% for 60 months? I earn $6,000/month with $500 existing debt.\",\n        \"expected_tools\": [\"check_affordability\"],\n        \"expected_keywords\": [\"affordable\", \"DTI\"],\n    },\n    {\n        \"id\": \"affordability_stretched\",\n        \"category\": \"ğŸ’° Affordability\",\n        \"input\": \"Monthly income $4,000, existing debt $1,500. Can I afford a $25,000 loan at 8% for 48 months?\",\n        \"expected_tools\": [\"check_affordability\"],\n        \"expected_keywords\": [\"DTI\", \"exceed\", \"not\"],\n    },\n]\n\n# ğŸ“Š Summary\nprint(f\"ğŸ“ Defined {len(GOLDENS)} test cases:\\n\")\nfrom collections import Counter\ncategories = Counter(g[\"category\"] for g in GOLDENS)\nfor cat, count in categories.items():\n    print(f\"  {cat}: {count} tests\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## ğŸƒ Part 5: Run Agent on All Test Cases\n\nExecute the agent for each Golden and collect:\n- `actual_output`: Agent's final response\n- `retrieval_context`: Tool outputs (used by Faithfulness/Hallucination metrics)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from deepeval.dataset import EvaluationDataset, Golden\n\nprint(\"ğŸƒ Running agent on all test cases...\\n\")\nprint(\"=\" * 60)\n\ngoldens_with_output = []\n\nfor g in GOLDENS:\n    print(f\"\\n{g['category']} | {g['id']}\")\n    print(f\"  ğŸ“¥ Input: {g['input'][:50]}...\")\n    \n    result = runner.run(g[\"input\"])\n    print(f\"  ğŸ”§ Tools called: {result['tools_called']}\")\n    \n    # Create Golden with agent output\n    golden = Golden(\n        input=g[\"input\"],\n        actual_output=result[\"actual_output\"],\n        retrieval_context=result[\"retrieval_context\"],  # ğŸ’¡ Tool outputs!\n        additional_metadata={\n            \"test_id\": g[\"id\"],\n            \"category\": g[\"category\"],\n            \"expected_tools\": g[\"expected_tools\"],\n            \"actual_tools\": result[\"tools_called\"],\n            \"expected_keywords\": g[\"expected_keywords\"],\n        }\n    )\n    goldens_with_output.append(golden)\n\nprint(\"\\n\" + \"=\" * 60)\ndataset = EvaluationDataset(goldens=goldens_with_output)\nprint(f\"\\nâœ… Created dataset with {len(dataset.goldens)} test cases\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## ğŸ“Š Part 6: Configure DeepEval Metrics\n\nWe use **3 metrics** that represent both categories:\n\n| Metric | Type | Uses Context? | What It Measures |\n|--------|------|---------------|------------------|\n| `AnswerRelevancy` | ğŸ†“ Referenceless | âŒ No | Is response relevant to the question? |\n| `Faithfulness` | ğŸ“‹ Reference-based | âœ… Yes | Is response grounded in tool outputs? |\n| `Hallucination` | ğŸ“‹ Reference-based | âœ… Yes | Does response make things up? |\n\n> ğŸ’¡ **Why these metrics?**  \n> - `AnswerRelevancy`: Ensures agent addresses the user's question  \n> - `Faithfulness`: Ensures agent uses tool outputs correctly  \n> - `Hallucination`: Catches fabricated numbers or facts"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from deepeval import evaluate\nfrom deepeval.metrics import AnswerRelevancyMetric, FaithfulnessMetric, HallucinationMetric\nfrom deepeval.test_case import LLMTestCase\n\n# ğŸ§  Judge model for evaluation\nEVAL_MODEL = \"gpt-4o-mini\"\n\n# ğŸ“Š Configure metrics\nmetrics = [\n    # ğŸ†“ REFERENCELESS: Only needs input + actual_output\n    AnswerRelevancyMetric(threshold=0.7, model=EVAL_MODEL),\n    \n    # ğŸ“‹ REFERENCE-BASED: Need retrieval_context (tool outputs)\n    FaithfulnessMetric(threshold=0.7, model=EVAL_MODEL),\n    HallucinationMetric(threshold=0.5, model=EVAL_MODEL),\n]\n\nprint(\"ğŸ“Š Metrics configured:\\n\")\nprint(\"  ğŸ†“ Referenceless (no ground truth needed):\")\nprint(f\"     â€¢ AnswerRelevancy (threshold: 0.7)\")\nprint(\"\\n  ğŸ“‹ Reference-based (uses retrieval_context):\")\nprint(f\"     â€¢ Faithfulness (threshold: 0.7)\")\nprint(f\"     â€¢ Hallucination (threshold: 0.5)\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ğŸ”„ Convert Goldens to LLMTestCases\ntest_cases = [\n    LLMTestCase(\n        input=g.input,\n        actual_output=g.actual_output,\n        context=g.retrieval_context,           # ğŸ“‹ For Hallucination metric\n        retrieval_context=g.retrieval_context,  # ğŸ“‹ For Faithfulness metric\n    )\n    for g in dataset.goldens\n]\n\nprint(f\"âœ… Created {len(test_cases)} LLMTestCases\")\nprint(\"\\nğŸ’¡ Note: retrieval_context = tool outputs = ground truth for reference-based metrics\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## ğŸ”¬ Part 7: Run DeepEval Evaluation\n\nNow we run the LLM-as-Judge evaluation on all test cases."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"ğŸ”¬ Running DeepEval evaluation...\")\nprint(\"   This may take a few minutes as the judge LLM evaluates each test case.\\n\")\n\nresults = evaluate(\n    test_cases=test_cases,\n    metrics=metrics,\n)\n\nprint(\"\\nâœ… Evaluation complete!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## ğŸ“ˆ Part 8: Analyze Results\n\nLet's examine the evaluation results in detail."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import pandas as pd\n\n# ğŸ“Š Build results dataframe\nresults_data = []\n\nfor tc, golden in zip(test_cases, dataset.goldens):\n    meta = golden.additional_metadata\n    row = {\n        \"test_id\": meta[\"test_id\"], \n        \"category\": meta[\"category\"]\n    }\n    \n    # Evaluate each metric\n    for metric in metrics:\n        metric.measure(tc)\n        name = metric.__class__.__name__.replace(\"Metric\", \"\")\n        row[name] = round(metric.score, 2)\n        row[f\"{name}_pass\"] = \"âœ…\" if metric.is_successful() else \"âŒ\"\n    \n    results_data.append(row)\n\ndf = pd.DataFrame(results_data)\n\nprint(\"ğŸ“Š RESULTS BY TEST CASE\\n\")\nprint(df.to_string(index=False))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ğŸ“Š Summary by category\nprint(\"ğŸ“Š AVERAGE SCORES BY CATEGORY\\n\")\nscore_cols = [\"AnswerRelevancy\", \"Faithfulness\", \"Hallucination\"]\nsummary = df.groupby(\"category\")[score_cols].mean().round(2)\nprint(summary.to_string())\n\n# ğŸ¨ Visual representation\nprint(\"\\n\" + \"=\" * 50)\nprint(\"ğŸ“ˆ CATEGORY PERFORMANCE\")\nprint(\"=\" * 50)\nfor cat in summary.index:\n    scores = summary.loc[cat]\n    avg = scores.mean()\n    bar = \"â–ˆ\" * int(avg * 20) + \"â–‘\" * (20 - int(avg * 20))\n    print(f\"\\n{cat}\")\n    print(f\"  {bar} {avg:.0%}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ğŸ“Š Overall pass rates\nprint(\"ğŸ“Š OVERALL PASS RATES\\n\")\n\npass_data = {\n    \"ğŸ†“ AnswerRelevancy\": (df[\"AnswerRelevancy_pass\"] == \"âœ…\").mean(),\n    \"ğŸ“‹ Faithfulness\": (df[\"Faithfulness_pass\"] == \"âœ…\").mean(),\n    \"ğŸ“‹ Hallucination\": (df[\"Hallucination_pass\"] == \"âœ…\").mean(),\n}\n\nfor metric, rate in pass_data.items():\n    bar = \"â–ˆ\" * int(rate * 20) + \"â–‘\" * (20 - int(rate * 20))\n    status = \"âœ…\" if rate >= 0.8 else \"âš ï¸\" if rate >= 0.6 else \"âŒ\"\n    print(f\"  {metric}: {bar} {rate:.0%} {status}\")\n\n# Overall\nall_pass = (\n    (df[\"AnswerRelevancy_pass\"] == \"âœ…\") & \n    (df[\"Faithfulness_pass\"] == \"âœ…\") & \n    (df[\"Hallucination_pass\"] == \"âœ…\")\n).mean()\nprint(f\"\\n  ğŸ¯ All metrics pass: {all_pass:.0%}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## ğŸ”§ Part 9: Tool Call Validation\n\nBeyond LLM-as-Judge metrics, we also validate that the agent used the correct tools."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"ğŸ”§ TOOL CALL VALIDATION\\n\")\n\ntool_results = []\nfor golden in dataset.goldens:\n    meta = golden.additional_metadata\n    expected = set(meta[\"expected_tools\"])\n    actual = set(meta[\"actual_tools\"])\n    match = expected.issubset(actual)\n    \n    status = \"âœ…\" if match else \"âŒ\"\n    print(f\"  {status} {meta['test_id']}\")\n    if not match:\n        print(f\"      Expected: {list(expected)}\")\n        print(f\"      Got: {list(actual)}\")\n    \n    tool_results.append(match)\n\naccuracy = sum(tool_results) / len(tool_results)\nprint(f\"\\nğŸ¯ Tool Call Accuracy: {accuracy:.0%}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## ğŸ“ Part 10: Key Takeaways\n\n### ğŸ”„ DeepEval Evaluation Flow\n\n```\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚   Goldens   â”‚ â”€â”€â–º â”‚   Agent     â”‚ â”€â”€â–º â”‚ LLMTestCase â”‚ â”€â”€â–º â”‚  Metrics    â”‚\nâ”‚ (templates) â”‚     â”‚ Execution   â”‚     â”‚             â”‚     â”‚             â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                          â”‚\n                          â–¼\n                   Tool Outputs = retrieval_context\n                   (ground truth for reference-based metrics)\n```\n\n---\n\n### ğŸ“Š Metric Types Summary\n\n| Type | Metrics | Required Parameters | Use Case |\n|------|---------|---------------------|----------|\n| ğŸ†“ **Referenceless** | AnswerRelevancy, Toxicity, Bias | `input`, `actual_output` only | No labeled data needed |\n| ğŸ“‹ **Reference-based** | Faithfulness, Hallucination | + `retrieval_context` | Ground truth from tools |\n| ğŸ“‹ **Reference-based** | ContextualRecall, Precision | + `expected_output` | Need expected answers |\n\n---\n\n### âœ… Best Practices\n\n1. **ğŸ”§ Tool outputs â†’ retrieval_context**  \n   Use tool results as ground truth for Faithfulness/Hallucination\n\n2. **ğŸ“ Separate tools module**  \n   Keep notebook focused on evaluation logic\n\n3. **ğŸ·ï¸ Category-based Goldens**  \n   Organize tests by loan type/scenario for better analysis\n\n4. **ğŸ” Multi-level validation**  \n   Combine LLM-as-Judge metrics with deterministic tool validation\n\n5. **ğŸ“ˆ Track by category**  \n   Identify which scenarios need improvement\n\n---\n\n### ğŸ“š Resources\n\n- [DeepEval Documentation](https://docs.confident-ai.com/)\n- [LangChain](https://python.langchain.com/)\n- [LangGraph](https://langchain-ai.github.io/langgraph/)\n- [DeepEval Metrics Guide](https://deepeval.com/docs/metrics-introduction)\n\n---\n\n**ğŸ‘ Found this helpful? Please upvote!**"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}