{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# ğŸ¯ DeepEval: Evaluating LangChain Agents with LLM-as-Judge\n\n[![DeepEval](https://img.shields.io/badge/DeepEval-Latest-purple.svg)](https://docs.confident-ai.com/)\n[![LangChain](https://img.shields.io/badge/LangChain-0.2+-green.svg)](https://python.langchain.com/)\n[![Python](https://img.shields.io/badge/Python-3.11+-blue.svg)](https://python.org)\n\n---\n\n## ğŸ“š What You'll Learn\n\nThis notebook demonstrates how to evaluate AI agents using **DeepEval's LLM-as-Judge** approach:\n\n| Section | Topic |\n|---------|-------|\n| ğŸ”§ Setup | Environment and API configuration |\n| ğŸ› ï¸ Tools | LangChain tools for loan calculations |\n| ğŸ¤– Agent | Building a ReAct agent with LangGraph |\n| ğŸ“ Goldens | Test templates covering multiple scenarios |\n| ğŸ“Š Metrics | **Reference-based** vs **Referenceless** evaluation |\n| ğŸ”¬ Evaluation | Running DeepEval on agent outputs |\n| ğŸ“ˆ Analysis | Understanding and interpreting results |\n\n---\n\n## ğŸ“ Key Concept: Reference vs Referenceless Metrics\n\nDeepEval metrics fall into two categories:\n\n### ğŸ“‹ Reference-Based Metrics\n> **Require ground truth** (`expected_output` or `retrieval_context`)\n\n| Metric | Required Parameter | What It Measures |\n|--------|-------------------|------------------|\n| `Faithfulness` | `retrieval_context` | Is output grounded in provided context? |\n| `Hallucination` | `context` | Does output contain fabricated info? |\n| `ContextualRecall` | `expected_output` + `retrieval_context` | Does context contain all needed info? |\n| `ContextualPrecision` | `expected_output` + `retrieval_context` | Is context focused and relevant? |\n\n### ğŸ†“ Referenceless Metrics  \n> **No ground truth needed** - evaluate output quality directly\n\n| Metric | Required Parameter | What It Measures |\n|--------|-------------------|------------------|\n| `AnswerRelevancy` | `input` only | Is response relevant to the question? |\n| `Toxicity` | `actual_output` only | Contains harmful content? |\n| `Bias` | `actual_output` only | Contains biased language? |\n| `TaskCompletion` | Agent trace | Did agent complete the task? |\n\n---\n\n## ğŸ¦ Demo: Loan Advisor Agent\n\nWe'll evaluate a **multi-type loan advisor** with 6 tools:\n\n| Tool | Loan Type | Description |\n|------|-----------|-------------|\n| `calculate_personal_loan` | ğŸ’³ Personal | Unsecured loan payment calculation |\n| `calculate_mortgage` | ğŸ  Mortgage | Home loan with LTV & PMI analysis |\n| `calculate_auto_loan` | ğŸš— Auto | Car loan with trade-in support |\n| `check_loan_eligibility` | âœ… All | Credit & income eligibility check |\n| `check_affordability` | ğŸ’° All | DTI-based affordability analysis |\n| `compare_loan_options` | ğŸ“Š All | Compare different loan terms |\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## ğŸ”§ Part 1: Environment Setup"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ğŸ“¦ Install dependencies (uncomment for Kaggle/Colab)\n# !pip install -q deepeval langchain langchain-openai langgraph pandas python-dotenv"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os\nimport json\nfrom typing import Dict, Any\n\n# =============================================================================\n# ğŸ”‘ API Key Setup (Kaggle / Colab / Local)\n# =============================================================================\n\ndef setup_api_key():\n    \"\"\"Load API key from various sources with priority order.\"\"\"\n    \n    # 1ï¸âƒ£ Environment variable (highest priority)\n    if os.getenv(\"OPENAI_API_KEY\"):\n        return \"environment\"\n    \n    # 2ï¸âƒ£ Kaggle Secrets\n    try:\n        from kaggle_secrets import UserSecretsClient\n        os.environ[\"OPENAI_API_KEY\"] = UserSecretsClient().get_secret(\"OPENAI_API_KEY\")\n        return \"Kaggle Secrets\"\n    except: pass\n    \n    # 3ï¸âƒ£ Google Colab Secrets\n    try:\n        from google.colab import userdata\n        os.environ[\"OPENAI_API_KEY\"] = userdata.get(\"OPENAI_API_KEY\")\n        return \"Colab Secrets\"\n    except: pass\n    \n    # 4ï¸âƒ£ Local .env file\n    try:\n        from dotenv import load_dotenv\n        load_dotenv()\n        if os.getenv(\"OPENAI_API_KEY\"):\n            return \".env file\"\n    except: pass\n    \n    return None\n\nsource = setup_api_key()\nif source:\n    print(f\"âœ… API key loaded from {source}\")\nelse:\n    raise ValueError(\"âŒ OPENAI_API_KEY not found. Set via Kaggle/Colab Secrets or .env file.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## ğŸ› ï¸ Part 2: LangChain Tools Definition\n\n<details>\n<summary>â„¹ï¸ <b>About This Section</b> (click to expand)</summary>\n\nThe next cell contains **6 LangChain tools** for loan calculations:\n\n| Tool | Description |\n|------|-------------|\n| `calculate_personal_loan` | Personal loan payment calculation |\n| `calculate_mortgage` | Home loan with down payment & LTV ratio |\n| `calculate_auto_loan` | Car loan with trade-in support |\n| `check_loan_eligibility` | Credit & income eligibility check |\n| `check_affordability` | DTI-based affordability analysis |\n| `compare_loan_options` | Compare different loan terms |\n\n**ğŸ’¡ Tip:** You can **collapse the next code cell** in Kaggle/Jupyter by clicking the blue bar on the left. The tools are standard LangChain `@tool` decorated functions.\n\n</details>\n\n---"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "%%writefile langchain_tools.py\n# =============================================================================\n# ğŸ› ï¸ LangChain Tools for Loan Advisor Agent\n# =============================================================================\n# ğŸ’¡ TIP: Collapse this cell in Kaggle/Jupyter - just run it once to create the file!\n# =============================================================================\n\n\"\"\"LangChain Tools for Loan Advisor Agent.\n\nTools:\n    - calculate_personal_loan: Personal loan payment calculation\n    - calculate_mortgage: Mortgage/home loan with down payment and LTV\n    - calculate_auto_loan: Auto/car loan calculation\n    - check_loan_eligibility: Check if applicant qualifies for a loan\n    - check_affordability: Analyze debt-to-income ratio\n    - compare_loan_options: Compare different loan terms\n\"\"\"\n\nimport json\nfrom langchain_core.tools import tool\n\n# =============================================================================\n# Core Calculation Functions\n# =============================================================================\n\ndef _calculate_monthly_payment(principal: float, annual_rate: float, months: int) -> float:\n    \"\"\"Calculate monthly payment using amortization formula.\"\"\"\n    if annual_rate == 0:\n        return principal / months\n    monthly_rate = annual_rate / 12\n    payment = principal * (\n        monthly_rate * (1 + monthly_rate) ** months\n    ) / ((1 + monthly_rate) ** months - 1)\n    return round(payment, 2)\n\ndef _calculate_totals(principal: float, monthly_payment: float, months: int) -> dict:\n    \"\"\"Calculate total payment and interest.\"\"\"\n    total_payment = monthly_payment * months\n    total_interest = total_payment - principal\n    return {\n        \"total_payment\": round(total_payment, 2),\n        \"total_interest\": round(total_interest, 2),\n        \"interest_percentage\": round(total_interest / principal * 100, 2),\n    }\n\n# =============================================================================\n# ğŸ’³ Personal Loan Tool\n# =============================================================================\n\n@tool\ndef calculate_personal_loan(\n    loan_amount: float,\n    annual_interest_rate: float,\n    loan_term_months: int,\n) -> str:\n    \"\"\"Calculate monthly payment for a personal loan.\n\n    Args:\n        loan_amount: Principal loan amount in dollars (e.g., 25000)\n        annual_interest_rate: Annual interest rate as decimal (e.g., 0.10 for 10%)\n        loan_term_months: Loan term in months (typically 12-84)\n\n    Returns:\n        JSON with monthly_payment, total_payment, total_interest, and loan details\n    \"\"\"\n    monthly = _calculate_monthly_payment(loan_amount, annual_interest_rate, loan_term_months)\n    totals = _calculate_totals(loan_amount, monthly, loan_term_months)\n    result = {\n        \"loan_type\": \"personal\",\n        \"loan_amount\": loan_amount,\n        \"annual_interest_rate\": annual_interest_rate,\n        \"loan_term_months\": loan_term_months,\n        \"monthly_payment\": monthly,\n        **totals,\n    }\n    return json.dumps(result)\n\n# =============================================================================\n# ğŸ  Mortgage (Home Loan) Tool\n# =============================================================================\n\n@tool\ndef calculate_mortgage(\n    home_price: float,\n    down_payment_percent: float,\n    annual_interest_rate: float,\n    loan_term_years: int,\n) -> str:\n    \"\"\"Calculate monthly payment for a mortgage (home loan).\n\n    Args:\n        home_price: Total home price in dollars (e.g., 500000)\n        down_payment_percent: Down payment as percentage (e.g., 0.20 for 20%)\n        annual_interest_rate: Annual interest rate as decimal (e.g., 0.065 for 6.5%)\n        loan_term_years: Loan term in years (typically 15 or 30)\n\n    Returns:\n        JSON with monthly_payment, loan_amount, down_payment, LTV ratio, and totals\n    \"\"\"\n    down_payment = home_price * down_payment_percent\n    loan_amount = home_price - down_payment\n    loan_term_months = loan_term_years * 12\n    ltv_ratio = loan_amount / home_price\n    monthly = _calculate_monthly_payment(loan_amount, annual_interest_rate, loan_term_months)\n    totals = _calculate_totals(loan_amount, monthly, loan_term_months)\n    result = {\n        \"loan_type\": \"mortgage\",\n        \"home_price\": home_price,\n        \"down_payment\": round(down_payment, 2),\n        \"down_payment_percent\": down_payment_percent,\n        \"loan_amount\": round(loan_amount, 2),\n        \"ltv_ratio\": round(ltv_ratio, 3),\n        \"annual_interest_rate\": annual_interest_rate,\n        \"loan_term_years\": loan_term_years,\n        \"loan_term_months\": loan_term_months,\n        \"monthly_payment\": monthly,\n        **totals,\n        \"ltv_warning\": \"LTV exceeds 80%, PMI may be required\" if ltv_ratio > 0.80 else None,\n    }\n    return json.dumps(result)\n\n# =============================================================================\n# ğŸš— Auto Loan (Car Loan) Tool\n# =============================================================================\n\n@tool\ndef calculate_auto_loan(\n    vehicle_price: float,\n    down_payment: float,\n    annual_interest_rate: float,\n    loan_term_months: int,\n    trade_in_value: float = 0,\n) -> str:\n    \"\"\"Calculate monthly payment for an auto (car) loan.\n\n    Args:\n        vehicle_price: Total vehicle price in dollars (e.g., 35000)\n        down_payment: Down payment amount in dollars (e.g., 5000)\n        annual_interest_rate: Annual interest rate as decimal (e.g., 0.059 for 5.9%)\n        loan_term_months: Loan term in months (typically 36, 48, 60, or 72)\n        trade_in_value: Value of trade-in vehicle, if any (default 0)\n\n    Returns:\n        JSON with monthly_payment, loan_amount, effective_price, and totals\n    \"\"\"\n    effective_price = vehicle_price - trade_in_value\n    loan_amount = effective_price - down_payment\n    ltv_ratio = loan_amount / vehicle_price\n    monthly = _calculate_monthly_payment(loan_amount, annual_interest_rate, loan_term_months)\n    totals = _calculate_totals(loan_amount, monthly, loan_term_months)\n    result = {\n        \"loan_type\": \"auto\",\n        \"vehicle_price\": vehicle_price,\n        \"trade_in_value\": trade_in_value,\n        \"effective_price\": round(effective_price, 2),\n        \"down_payment\": down_payment,\n        \"loan_amount\": round(loan_amount, 2),\n        \"ltv_ratio\": round(ltv_ratio, 3),\n        \"annual_interest_rate\": annual_interest_rate,\n        \"loan_term_months\": loan_term_months,\n        \"monthly_payment\": monthly,\n        **totals,\n    }\n    return json.dumps(result)\n\n# =============================================================================\n# âœ… Eligibility Check Tool\n# =============================================================================\n\n@tool\ndef check_loan_eligibility(\n    age: int,\n    monthly_income: float,\n    credit_score: int,\n    employment_status: str,\n    requested_loan_amount: float,\n    loan_type: str = \"personal\",\n) -> str:\n    \"\"\"Check if an applicant is eligible for a loan.\n\n    Args:\n        age: Applicant's age in years\n        monthly_income: Monthly gross income in dollars\n        credit_score: Credit score (300-850 range)\n        employment_status: One of: full_time, part_time, self_employed, unemployed\n        requested_loan_amount: Desired loan amount in dollars\n        loan_type: Type of loan: personal, mortgage, or auto\n\n    Returns:\n        JSON with eligibility decision, credit rating, and detailed reasons\n    \"\"\"\n    reasons = []\n    is_eligible = True\n\n    # Age check\n    min_age, max_age = 18, 65\n    if age < min_age:\n        is_eligible = False\n        reasons.append(f\"Age {age} below minimum ({min_age})\")\n    elif age > max_age:\n        is_eligible = False\n        reasons.append(f\"Age {age} above maximum ({max_age})\")\n    else:\n        reasons.append(\"Age requirement met\")\n\n    # Income check\n    min_income = 3000 if loan_type == \"personal\" else 4000\n    if monthly_income < min_income:\n        is_eligible = False\n        reasons.append(f\"Income ${monthly_income:,.0f} below minimum (${min_income:,.0f})\")\n    else:\n        reasons.append(f\"Income requirement met (${monthly_income:,.0f}/month)\")\n\n    # Credit score check\n    min_scores = {\"personal\": 600, \"mortgage\": 620, \"auto\": 580}\n    min_score = min_scores.get(loan_type, 600)\n    if credit_score < min_score:\n        is_eligible = False\n        credit_rating = \"Poor\"\n        reasons.append(f\"Credit score {credit_score} below minimum ({min_score})\")\n    elif credit_score < 670:\n        credit_rating = \"Fair\"\n        reasons.append(f\"Credit score acceptable ({credit_score})\")\n    elif credit_score < 740:\n        credit_rating = \"Good\"\n        reasons.append(f\"Credit score good ({credit_score})\")\n    else:\n        credit_rating = \"Excellent\"\n        reasons.append(f\"Credit score excellent ({credit_score})\")\n\n    # Employment check\n    if employment_status == \"unemployed\":\n        is_eligible = False\n        reasons.append(\"Employment required for loan approval\")\n    else:\n        reasons.append(f\"Employment status acceptable ({employment_status})\")\n\n    # DTI estimate\n    estimated_payment = requested_loan_amount / 36\n    dti_estimate = estimated_payment / monthly_income\n    max_dti = 0.43 if loan_type == \"mortgage\" else 0.50\n    if dti_estimate > max_dti:\n        is_eligible = False\n        reasons.append(f\"Estimated DTI {dti_estimate:.1%} exceeds maximum ({max_dti:.0%})\")\n\n    result = {\n        \"loan_type\": loan_type,\n        \"is_eligible\": is_eligible,\n        \"credit_rating\": credit_rating,\n        \"estimated_dti\": round(dti_estimate, 3),\n        \"max_dti\": max_dti,\n        \"reasons\": reasons,\n        \"max_recommended_loan\": round(monthly_income * max_dti * 36, 2),\n    }\n    return json.dumps(result)\n\n# =============================================================================\n# ğŸ’° Affordability Check Tool\n# =============================================================================\n\n@tool\ndef check_affordability(\n    monthly_income: float,\n    existing_monthly_debt: float,\n    proposed_loan_amount: float,\n    annual_interest_rate: float,\n    loan_term_months: int,\n) -> str:\n    \"\"\"Analyze if a loan is affordable based on debt-to-income ratio.\n\n    Args:\n        monthly_income: Monthly gross income in dollars\n        existing_monthly_debt: Current monthly debt payments\n        proposed_loan_amount: New loan amount being considered\n        annual_interest_rate: Interest rate for the new loan\n        loan_term_months: Term of the new loan in months\n\n    Returns:\n        JSON with affordability analysis including current/new DTI\n    \"\"\"\n    new_payment = _calculate_monthly_payment(\n        proposed_loan_amount, annual_interest_rate, loan_term_months\n    )\n    current_dti = existing_monthly_debt / monthly_income\n    new_total_debt = existing_monthly_debt + new_payment\n    new_dti = new_total_debt / monthly_income\n    max_dti = 0.50\n    is_affordable = new_dti <= max_dti\n    remaining_capacity = (monthly_income * max_dti) - new_total_debt\n\n    if new_dti <= 0.30:\n        assessment = \"Excellent - very comfortable debt level\"\n    elif new_dti <= 0.40:\n        assessment = \"Good - manageable debt level\"\n    elif new_dti <= 0.50:\n        assessment = \"Acceptable - at the higher end of recommended range\"\n    else:\n        assessment = \"Not recommended - debt exceeds safe limits\"\n\n    result = {\n        \"is_affordable\": is_affordable,\n        \"monthly_income\": monthly_income,\n        \"existing_monthly_debt\": existing_monthly_debt,\n        \"new_monthly_payment\": new_payment,\n        \"total_monthly_debt\": round(new_total_debt, 2),\n        \"current_dti\": round(current_dti, 3),\n        \"new_dti\": round(new_dti, 3),\n        \"max_dti\": max_dti,\n        \"remaining_capacity\": round(max(0, remaining_capacity), 2),\n        \"assessment\": assessment,\n    }\n    return json.dumps(result)\n\n# =============================================================================\n# ğŸ“Š Loan Comparison Tool\n# =============================================================================\n\n@tool\ndef compare_loan_options(\n    loan_amount: float,\n    annual_interest_rate: float,\n    term_options: str = \"36,48,60\",\n) -> str:\n    \"\"\"Compare monthly payments across different loan terms.\n\n    Args:\n        loan_amount: Principal loan amount in dollars\n        annual_interest_rate: Annual interest rate as decimal\n        term_options: Comma-separated loan terms in months (default: \"36,48,60\")\n\n    Returns:\n        JSON with comparison of monthly payment and total interest for each term\n    \"\"\"\n    terms = [int(t.strip()) for t in term_options.split(\",\")]\n    comparisons = []\n    for months in terms:\n        monthly = _calculate_monthly_payment(loan_amount, annual_interest_rate, months)\n        totals = _calculate_totals(loan_amount, monthly, months)\n        comparisons.append({\n            \"term_months\": months,\n            \"term_years\": round(months / 12, 1),\n            \"monthly_payment\": monthly,\n            \"total_interest\": totals[\"total_interest\"],\n            \"total_payment\": totals[\"total_payment\"],\n        })\n    comparisons.sort(key=lambda x: x[\"term_months\"])\n    if len(comparisons) > 1:\n        longest = comparisons[-1]\n        for comp in comparisons:\n            comp[\"interest_savings_vs_longest\"] = round(\n                longest[\"total_interest\"] - comp[\"total_interest\"], 2\n            )\n    result = {\n        \"loan_amount\": loan_amount,\n        \"annual_interest_rate\": annual_interest_rate,\n        \"comparisons\": comparisons,\n        \"recommendation\": (\n            f\"Shorter terms save money on interest but have higher monthly payments. \"\n            f\"The {comparisons[0]['term_months']}-month option saves \"\n            f\"${comparisons[0].get('interest_savings_vs_longest', 0):,.2f} in interest.\"\n        ) if len(comparisons) > 1 else None,\n    }\n    return json.dumps(result)\n\n# =============================================================================\n# Tool Registry\n# =============================================================================\n\ndef get_all_tools():\n    \"\"\"Get all available LangChain tools for the loan advisor agent.\"\"\"\n    return [\n        calculate_personal_loan,\n        calculate_mortgage,\n        calculate_auto_loan,\n        check_loan_eligibility,\n        check_affordability,\n        compare_loan_options,\n    ]\n\ndef get_tool_descriptions() -> str:\n    \"\"\"Get formatted descriptions of all available tools.\"\"\"\n    tools = get_all_tools()\n    descriptions = []\n    for t in tools:\n        descriptions.append(f\"- **{t.name}**: {t.description.split('.')[0]}\")\n    return \"\\n\".join(descriptions)"
  },
  {
   "cell_type": "code",
   "source": "# ğŸ“¥ Import the tools we just created\nfrom langchain_tools import get_all_tools, get_tool_descriptions\n\ntools = get_all_tools()\nprint(f\"âœ… Loaded {len(tools)} tools:\\n\")\nprint(get_tool_descriptions())",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## ğŸ¤– Part 3: Create LangChain ReAct Agent\n\nWe use **LangGraph's `create_react_agent`** â€” a prebuilt ReAct (Reason + Act) agent that:\n1. Receives user input\n2. Decides which tool(s) to call\n3. Executes tools and observes results\n4. Generates final response"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from langchain_openai import ChatOpenAI\nfrom langgraph.prebuilt import create_react_agent\nfrom langchain_core.messages import HumanMessage, AIMessage, ToolMessage\n\n# ğŸ§  Create LLM\nllm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n\n# ğŸ“‹ System prompt defines agent behavior\nSYSTEM_PROMPT = \"\"\"You are a Loan Advisor assistant helping users with:\n- Personal loans, mortgages (home loans), and auto (car) loans\n- Payment calculations, eligibility checks, and affordability analysis\n\nUse the provided tools for accurate calculations. Be clear and helpful.\nAlways show the key numbers in your response.\"\"\"\n\n# ğŸ¤– Create ReAct agent\nagent = create_react_agent(llm, tools, prompt=SYSTEM_PROMPT)\nprint(\"ğŸ¤– Agent created with 6 loan advisor tools\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ğŸƒ Agent Runner - extracts outputs and tool context\nclass AgentRunner:\n    \"\"\"Wrapper that runs agent and extracts outputs for evaluation.\"\"\"\n    \n    def __init__(self, agent):\n        self.agent = agent\n    \n    def run(self, query: str) -> Dict[str, Any]:\n        \"\"\"Run agent and extract actual_output + retrieval_context.\"\"\"\n        result = self.agent.invoke({\"messages\": [HumanMessage(content=query)]})\n        messages = result[\"messages\"]\n        \n        # ğŸ“¤ Extract final AI response\n        actual_output = next(\n            (m.content for m in reversed(messages) if isinstance(m, AIMessage) and m.content),\n            \"\"\n        )\n        \n        # ğŸ”§ Extract tool calls and results\n        tools_called = []\n        retrieval_context = []  # ğŸ’¡ Tool outputs become retrieval_context!\n        \n        for msg in messages:\n            if isinstance(msg, AIMessage) and msg.tool_calls:\n                tools_called.extend([tc.get(\"name\", \"\") for tc in msg.tool_calls if isinstance(tc, dict)])\n            if isinstance(msg, ToolMessage):\n                retrieval_context.append(msg.content)\n        \n        return {\n            \"actual_output\": actual_output,\n            \"tools_called\": tools_called,\n            \"retrieval_context\": retrieval_context,  # ğŸ¯ Key for Faithfulness/Hallucination metrics!\n        }\n\nrunner = AgentRunner(agent)\nprint(\"âœ… AgentRunner ready\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## ğŸ“ Part 4: Define Test Goldens\n\n**Goldens** = test templates that define:\n- `input`: User query to test\n- `expected_tools`: Which tools should be called\n- `expected_keywords`: Key terms that should appear in response\n\nWe cover **5 categories** across all loan types:"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "GOLDENS = [\n    # ==========================================================================\n    # ğŸ’³ PERSONAL LOAN\n    # ==========================================================================\n    {\n        \"id\": \"personal_loan_basic\",\n        \"category\": \"ğŸ’³ Personal\",\n        \"input\": \"Calculate monthly payment for a $25,000 personal loan at 10% interest for 48 months.\",\n        \"expected_tools\": [\"calculate_personal_loan\"],\n        \"expected_keywords\": [\"634\", \"monthly\", \"payment\"],\n    },\n    {\n        \"id\": \"personal_loan_comparison\",\n        \"category\": \"ğŸ’³ Personal\",\n        \"input\": \"Compare a $20,000 personal loan at 9% interest for 36, 48, and 60 months.\",\n        \"expected_tools\": [\"compare_loan_options\"],\n        \"expected_keywords\": [\"36\", \"48\", \"60\", \"interest\"],\n    },\n    \n    # ==========================================================================\n    # ğŸ  MORTGAGE (HOME LOAN)\n    # ==========================================================================\n    {\n        \"id\": \"mortgage_basic\",\n        \"category\": \"ğŸ  Mortgage\",\n        \"input\": \"Calculate mortgage payment for a $500,000 home with 20% down payment at 6.5% for 30 years.\",\n        \"expected_tools\": [\"calculate_mortgage\"],\n        \"expected_keywords\": [\"2,528\", \"monthly\", \"down payment\", \"400,000\"],\n    },\n    {\n        \"id\": \"mortgage_low_down\",\n        \"category\": \"ğŸ  Mortgage\",\n        \"input\": \"What's the monthly payment for a $400,000 house with only 10% down at 7% for 30 years? Will I need PMI?\",\n        \"expected_tools\": [\"calculate_mortgage\"],\n        \"expected_keywords\": [\"LTV\", \"PMI\", \"monthly\"],\n    },\n    \n    # ==========================================================================\n    # ğŸš— AUTO LOAN (CAR LOAN)\n    # ==========================================================================\n    {\n        \"id\": \"auto_loan_basic\",\n        \"category\": \"ğŸš— Auto\",\n        \"input\": \"Calculate car loan payment for a $35,000 vehicle with $5,000 down at 5.9% for 60 months.\",\n        \"expected_tools\": [\"calculate_auto_loan\"],\n        \"expected_keywords\": [\"581\", \"monthly\", \"30,000\"],\n    },\n    {\n        \"id\": \"auto_loan_trade_in\",\n        \"category\": \"ğŸš— Auto\",\n        \"input\": \"I want to buy a $40,000 car. I have a trade-in worth $8,000 and can put $2,000 down. What's my payment at 6% for 72 months?\",\n        \"expected_tools\": [\"calculate_auto_loan\"],\n        \"expected_keywords\": [\"trade\", \"monthly\", \"30,000\"],\n    },\n    \n    # ==========================================================================\n    # âœ… ELIGIBILITY CHECK\n    # ==========================================================================\n    {\n        \"id\": \"eligibility_good_credit\",\n        \"category\": \"âœ… Eligibility\",\n        \"input\": \"Check my loan eligibility: age 35, income $8,000/month, credit score 750, full-time employed, requesting $50,000 personal loan.\",\n        \"expected_tools\": [\"check_loan_eligibility\"],\n        \"expected_keywords\": [\"eligible\", \"Excellent\", \"750\"],\n    },\n    {\n        \"id\": \"eligibility_low_credit\",\n        \"category\": \"âœ… Eligibility\",\n        \"input\": \"Am I eligible for a mortgage? Age 28, income $5,000/month, credit score 580, self-employed, want $300,000.\",\n        \"expected_tools\": [\"check_loan_eligibility\"],\n        \"expected_keywords\": [\"not\", \"credit\", \"580\"],\n    },\n    \n    # ==========================================================================\n    # ğŸ’° AFFORDABILITY ANALYSIS\n    # ==========================================================================\n    {\n        \"id\": \"affordability_ok\",\n        \"category\": \"ğŸ’° Affordability\",\n        \"input\": \"Can I afford a $30,000 car loan at 6% for 60 months? I earn $6,000/month with $500 existing debt.\",\n        \"expected_tools\": [\"check_affordability\"],\n        \"expected_keywords\": [\"affordable\", \"DTI\"],\n    },\n    {\n        \"id\": \"affordability_stretched\",\n        \"category\": \"ğŸ’° Affordability\",\n        \"input\": \"Monthly income $4,000, existing debt $1,500. Can I afford a $25,000 loan at 8% for 48 months?\",\n        \"expected_tools\": [\"check_affordability\"],\n        \"expected_keywords\": [\"DTI\", \"exceed\", \"not\"],\n    },\n]\n\n# ğŸ“Š Summary\nprint(f\"ğŸ“ Defined {len(GOLDENS)} test cases:\\n\")\nfrom collections import Counter\ncategories = Counter(g[\"category\"] for g in GOLDENS)\nfor cat, count in categories.items():\n    print(f\"  {cat}: {count} tests\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## ğŸƒ Part 5: Run Agent on All Test Cases\n\nExecute the agent for each Golden and collect:\n- `actual_output`: Agent's final response\n- `retrieval_context`: Tool outputs (used by Faithfulness/Hallucination metrics)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from deepeval.dataset import EvaluationDataset, Golden\n\nprint(\"ğŸƒ Running agent on all test cases...\\n\")\nprint(\"=\" * 60)\n\ngoldens_with_output = []\n\nfor g in GOLDENS:\n    print(f\"\\n{g['category']} | {g['id']}\")\n    print(f\"  ğŸ“¥ Input: {g['input'][:50]}...\")\n    \n    result = runner.run(g[\"input\"])\n    print(f\"  ğŸ”§ Tools called: {result['tools_called']}\")\n    \n    # Create Golden with agent output\n    golden = Golden(\n        input=g[\"input\"],\n        actual_output=result[\"actual_output\"],\n        retrieval_context=result[\"retrieval_context\"],  # ğŸ’¡ Tool outputs!\n        additional_metadata={\n            \"test_id\": g[\"id\"],\n            \"category\": g[\"category\"],\n            \"expected_tools\": g[\"expected_tools\"],\n            \"actual_tools\": result[\"tools_called\"],\n            \"expected_keywords\": g[\"expected_keywords\"],\n        }\n    )\n    goldens_with_output.append(golden)\n\nprint(\"\\n\" + \"=\" * 60)\ndataset = EvaluationDataset(goldens=goldens_with_output)\nprint(f\"\\nâœ… Created dataset with {len(dataset.goldens)} test cases\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## ğŸ“Š Part 6: Configure DeepEval Metrics\n\nWe use **3 metrics** that represent both categories:\n\n| Metric | Type | Uses Context? | What It Measures |\n|--------|------|---------------|------------------|\n| `AnswerRelevancy` | ğŸ†“ Referenceless | âŒ No | Is response relevant to the question? |\n| `Faithfulness` | ğŸ“‹ Reference-based | âœ… Yes | Is response grounded in tool outputs? |\n| `Hallucination` | ğŸ“‹ Reference-based | âœ… Yes | Does response make things up? |\n\n> ğŸ’¡ **Why these metrics?**  \n> - `AnswerRelevancy`: Ensures agent addresses the user's question  \n> - `Faithfulness`: Ensures agent uses tool outputs correctly  \n> - `Hallucination`: Catches fabricated numbers or facts"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from deepeval import evaluate\nfrom deepeval.metrics import AnswerRelevancyMetric, FaithfulnessMetric, HallucinationMetric\nfrom deepeval.test_case import LLMTestCase\n\n# ğŸ§  Judge model for evaluation\nEVAL_MODEL = \"gpt-4o-mini\"\n\n# ğŸ“Š Configure metrics\nmetrics = [\n    # ğŸ†“ REFERENCELESS: Only needs input + actual_output\n    AnswerRelevancyMetric(threshold=0.7, model=EVAL_MODEL),\n    \n    # ğŸ“‹ REFERENCE-BASED: Need retrieval_context (tool outputs)\n    FaithfulnessMetric(threshold=0.7, model=EVAL_MODEL),\n    HallucinationMetric(threshold=0.5, model=EVAL_MODEL),\n]\n\nprint(\"ğŸ“Š Metrics configured:\\n\")\nprint(\"  ğŸ†“ Referenceless (no ground truth needed):\")\nprint(f\"     â€¢ AnswerRelevancy (threshold: 0.7)\")\nprint(\"\\n  ğŸ“‹ Reference-based (uses retrieval_context):\")\nprint(f\"     â€¢ Faithfulness (threshold: 0.7)\")\nprint(f\"     â€¢ Hallucination (threshold: 0.5)\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ğŸ”„ Convert Goldens to LLMTestCases\ntest_cases = [\n    LLMTestCase(\n        input=g.input,\n        actual_output=g.actual_output,\n        context=g.retrieval_context,           # ğŸ“‹ For Hallucination metric\n        retrieval_context=g.retrieval_context,  # ğŸ“‹ For Faithfulness metric\n    )\n    for g in dataset.goldens\n]\n\nprint(f\"âœ… Created {len(test_cases)} LLMTestCases\")\nprint(\"\\nğŸ’¡ Note: retrieval_context = tool outputs = ground truth for reference-based metrics\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## ğŸ”¬ Part 7: Run DeepEval Evaluation\n\nNow we run the LLM-as-Judge evaluation on all test cases."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"ğŸ”¬ Running DeepEval evaluation...\")\nprint(\"   This may take a few minutes as the judge LLM evaluates each test case.\\n\")\n\nresults = evaluate(\n    test_cases=test_cases,\n    metrics=metrics,\n)\n\nprint(\"\\nâœ… Evaluation complete!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## ğŸ“ˆ Part 8: Analyze Results\n\nLet's examine the evaluation results in detail."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import pandas as pd\n\n# ğŸ“Š Build results dataframe\nresults_data = []\n\nfor tc, golden in zip(test_cases, dataset.goldens):\n    meta = golden.additional_metadata\n    row = {\n        \"test_id\": meta[\"test_id\"], \n        \"category\": meta[\"category\"]\n    }\n    \n    # Evaluate each metric\n    for metric in metrics:\n        metric.measure(tc)\n        name = metric.__class__.__name__.replace(\"Metric\", \"\")\n        row[name] = round(metric.score, 2)\n        row[f\"{name}_pass\"] = \"âœ…\" if metric.is_successful() else \"âŒ\"\n    \n    results_data.append(row)\n\ndf = pd.DataFrame(results_data)\n\nprint(\"ğŸ“Š RESULTS BY TEST CASE\\n\")\nprint(df.to_string(index=False))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ğŸ“Š Summary by category\nprint(\"ğŸ“Š AVERAGE SCORES BY CATEGORY\\n\")\nscore_cols = [\"AnswerRelevancy\", \"Faithfulness\", \"Hallucination\"]\nsummary = df.groupby(\"category\")[score_cols].mean().round(2)\nprint(summary.to_string())\n\n# ğŸ¨ Visual representation\nprint(\"\\n\" + \"=\" * 50)\nprint(\"ğŸ“ˆ CATEGORY PERFORMANCE\")\nprint(\"=\" * 50)\nfor cat in summary.index:\n    scores = summary.loc[cat]\n    avg = scores.mean()\n    bar = \"â–ˆ\" * int(avg * 20) + \"â–‘\" * (20 - int(avg * 20))\n    print(f\"\\n{cat}\")\n    print(f\"  {bar} {avg:.0%}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ğŸ“Š Overall pass rates\nprint(\"ğŸ“Š OVERALL PASS RATES\\n\")\n\npass_data = {\n    \"ğŸ†“ AnswerRelevancy\": (df[\"AnswerRelevancy_pass\"] == \"âœ…\").mean(),\n    \"ğŸ“‹ Faithfulness\": (df[\"Faithfulness_pass\"] == \"âœ…\").mean(),\n    \"ğŸ“‹ Hallucination\": (df[\"Hallucination_pass\"] == \"âœ…\").mean(),\n}\n\nfor metric, rate in pass_data.items():\n    bar = \"â–ˆ\" * int(rate * 20) + \"â–‘\" * (20 - int(rate * 20))\n    status = \"âœ…\" if rate >= 0.8 else \"âš ï¸\" if rate >= 0.6 else \"âŒ\"\n    print(f\"  {metric}: {bar} {rate:.0%} {status}\")\n\n# Overall\nall_pass = (\n    (df[\"AnswerRelevancy_pass\"] == \"âœ…\") & \n    (df[\"Faithfulness_pass\"] == \"âœ…\") & \n    (df[\"Hallucination_pass\"] == \"âœ…\")\n).mean()\nprint(f\"\\n  ğŸ¯ All metrics pass: {all_pass:.0%}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## ğŸ”§ Part 9: Tool Call Validation\n\nBeyond LLM-as-Judge metrics, we also validate that the agent used the correct tools."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"ğŸ”§ TOOL CALL VALIDATION\\n\")\n\ntool_results = []\nfor golden in dataset.goldens:\n    meta = golden.additional_metadata\n    expected = set(meta[\"expected_tools\"])\n    actual = set(meta[\"actual_tools\"])\n    match = expected.issubset(actual)\n    \n    status = \"âœ…\" if match else \"âŒ\"\n    print(f\"  {status} {meta['test_id']}\")\n    if not match:\n        print(f\"      Expected: {list(expected)}\")\n        print(f\"      Got: {list(actual)}\")\n    \n    tool_results.append(match)\n\naccuracy = sum(tool_results) / len(tool_results)\nprint(f\"\\nğŸ¯ Tool Call Accuracy: {accuracy:.0%}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## ğŸ“ Part 10: Key Takeaways\n\n### ğŸ”„ DeepEval Evaluation Flow\n\n```\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚   Goldens   â”‚ â”€â”€â–º â”‚   Agent     â”‚ â”€â”€â–º â”‚ LLMTestCase â”‚ â”€â”€â–º â”‚  Metrics    â”‚\nâ”‚ (templates) â”‚     â”‚ Execution   â”‚     â”‚             â”‚     â”‚             â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                          â”‚\n                          â–¼\n                   Tool Outputs = retrieval_context\n                   (ground truth for reference-based metrics)\n```\n\n---\n\n### ğŸ“Š Metric Types Summary\n\n| Type | Metrics | Required Parameters | Use Case |\n|------|---------|---------------------|----------|\n| ğŸ†“ **Referenceless** | AnswerRelevancy, Toxicity, Bias | `input`, `actual_output` only | No labeled data needed |\n| ğŸ“‹ **Reference-based** | Faithfulness, Hallucination | + `retrieval_context` | Ground truth from tools |\n| ğŸ“‹ **Reference-based** | ContextualRecall, Precision | + `expected_output` | Need expected answers |\n\n---\n\n### âœ… Best Practices\n\n1. **ğŸ”§ Tool outputs â†’ retrieval_context**  \n   Use tool results as ground truth for Faithfulness/Hallucination\n\n2. **ğŸ“ Separate tools module**  \n   Keep notebook focused on evaluation logic\n\n3. **ğŸ·ï¸ Category-based Goldens**  \n   Organize tests by loan type/scenario for better analysis\n\n4. **ğŸ” Multi-level validation**  \n   Combine LLM-as-Judge metrics with deterministic tool validation\n\n5. **ğŸ“ˆ Track by category**  \n   Identify which scenarios need improvement\n\n---\n\n### ğŸ“š Resources\n\n- [DeepEval Documentation](https://docs.confident-ai.com/)\n- [LangChain](https://python.langchain.com/)\n- [LangGraph](https://langchain-ai.github.io/langgraph/)\n- [DeepEval Metrics Guide](https://deepeval.com/docs/metrics-introduction)\n\n---\n\n**ğŸ‘ Found this helpful? Please upvote!**"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}