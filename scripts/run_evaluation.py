#!/usr/bin/env python
"""
å¿«é€Ÿè¿è¡Œè¯„ä¼°çš„è„šæœ¬
å¯ä»¥ç›´æ¥è¿è¡Œè¯„ä¼°è€Œä¸éœ€è¦pytest
"""

import sys
import argparse
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Any
import json

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent))

from tests.test_mongodb_deepeval import MongoTestDataExtractor, MongoTestCase
from tests.deepeval_config import (
    EXPECTED_TOOLS_MAP,
    METRIC_THRESHOLDS,
    CUSTOM_THRESHOLDS,
    get_expected_tools,
    format_test_result
)
from deepeval.metrics import (
    AnswerRelevancyMetric,
    FaithfulnessMetric,
    HallucinationMetric,
    BiasMetric
)
from deepeval.test_case import LLMTestCase


class QuickEvaluator:
    """å¿«é€Ÿè¯„ä¼°å™¨"""

    def __init__(self):
        self.extractor = MongoTestDataExtractor()
        self.metrics = self._initialize_metrics()

    def _initialize_metrics(self) -> Dict:
        """åˆå§‹åŒ–è¯„ä¼°æŒ‡æ ‡"""
        return {
            "relevancy": AnswerRelevancyMetric(
                threshold=METRIC_THRESHOLDS["answer_relevancy"],
                model="gpt-4o-mini"
            ),
            "faithfulness": FaithfulnessMetric(
                threshold=METRIC_THRESHOLDS["faithfulness"],
                model="gpt-4o-mini"
            ),
            "hallucination": HallucinationMetric(
                threshold=METRIC_THRESHOLDS["hallucination"],
                model="gpt-4o-mini"
            ),
            "bias": BiasMetric(
                threshold=METRIC_THRESHOLDS["bias"],
                model="gpt-4o-mini"
            )
        }

    def evaluate_recent(self, hours: int = 24, limit: int = 5, with_tools: bool = False) -> Dict:
        """è¯„ä¼°æœ€è¿‘çš„è¿è¡Œ

        Args:
            hours: æŸ¥è¯¢æœ€è¿‘Nå°æ—¶çš„æ•°æ®
            limit: æœ€å¤šè¿”å›Nä¸ªæµ‹è¯•ç”¨ä¾‹
            with_tools: æ˜¯å¦åªè¿”å›æœ‰å·¥å…·è°ƒç”¨çš„æµ‹è¯•ç”¨ä¾‹
        """
        print(f"\n{'='*60}")
        print(f"ğŸ“Š è¯„ä¼°æœ€è¿‘{hours}å°æ—¶çš„{limit}ä¸ªè¿è¡Œ")
        if with_tools:
            print(f"   ï¼ˆä»…åŒ…å«æœ‰å·¥å…·è°ƒç”¨çš„ç”¨ä¾‹ï¼‰")
        print(f"{'='*60}\n")

        # è·å–æµ‹è¯•ç”¨ä¾‹
        all_cases = self.extractor.extract_recent_cases(hours=hours, limit=limit * 3 if with_tools else limit)

        # è¿‡æ»¤ï¼šåªè¦æœ‰å·¥å…·è°ƒç”¨çš„
        if with_tools:
            test_cases = [c for c in all_cases if c.tool_calls][:limit]
        else:
            test_cases = all_cases[:limit]

        if not test_cases:
            print("âŒ æœªæ‰¾åˆ°æµ‹è¯•ç”¨ä¾‹")
            return {}

        print(f"æ‰¾åˆ° {len(test_cases)} ä¸ªæµ‹è¯•ç”¨ä¾‹")
        if with_tools:
            print(f"  å…¶ä¸­ {len([c for c in test_cases if c.context])} ä¸ªå·²æœ‰context\n")

        results = {}
        for i, case in enumerate(test_cases, 1):
            print(f"\n--- æµ‹è¯•ç”¨ä¾‹ {i}/{len(test_cases)} ---")
            result = self._evaluate_single_case(case)
            results[case.test_id] = result

        return results

    def evaluate_by_pattern(self, pattern: str, limit: int = 3) -> Dict:
        """æŒ‰æ¨¡å¼è¯„ä¼°"""
        print(f"\n{'='*60}")
        print(f"ğŸ” è¯„ä¼°åŒ¹é…æ¨¡å¼ '{pattern}' çš„{limit}ä¸ªè¿è¡Œ")
        print(f"{'='*60}\n")

        # è·å–æµ‹è¯•ç”¨ä¾‹
        test_cases = self.extractor.extract_by_pattern(pattern, limit)

        if not test_cases:
            print(f"âŒ æœªæ‰¾åˆ°åŒ¹é… '{pattern}' çš„æµ‹è¯•ç”¨ä¾‹")
            return {}

        results = {}
        for i, case in enumerate(test_cases, 1):
            print(f"\n--- æµ‹è¯•ç”¨ä¾‹ {i}/{len(test_cases)} ---")
            result = self._evaluate_single_case(case)
            results[case.test_id] = result

        return results

    def _evaluate_single_case(self, case: MongoTestCase) -> Dict:
        """è¯„ä¼°å•ä¸ªæµ‹è¯•ç”¨ä¾‹"""
        print(f"ID: {case.test_id}")
        print(f"è¾“å…¥: {str(case.input)[:100]}...")
        print(f"è¾“å‡º: {str(case.actual_output)[:100]}...")

        # ç¡®ä¿contextå¯ç”¨ï¼ˆç”¨äºFaithfulnessç­‰æŒ‡æ ‡ï¼‰
        if not case.context and case.tool_calls:
            print(f"  ğŸ”§ é‡æ„context: ä» {len(case.tool_calls)} ä¸ªå·¥å…·è°ƒç”¨")
            case.reconstruct_context()
            if case.context:
                print(f"  âœ… Contextå·²é‡æ„: {len(case.context)} æ¡")
            else:
                print(f"  âš ï¸ Contexté‡æ„å¤±è´¥")

        result = {
            "test_id": case.test_id,
            "input": case.input,
            "output_preview": case.actual_output[:200],
            "metrics": {},
            "tools": self._evaluate_tools(case),
            "performance": self._evaluate_performance(case),
            "context_available": len(case.context) if case.context else 0
        }

        # è½¬æ¢ä¸ºDeepEvalæµ‹è¯•ç”¨ä¾‹
        deepeval_case = case.to_deepeval_case()

        # è¯„ä¼°å„é¡¹æŒ‡æ ‡
        for metric_name, metric in self.metrics.items():
            try:
                metric.measure(deepeval_case)
                score = metric.score
                passed = metric.is_successful()
                reason = getattr(metric, 'reason', '')

                result["metrics"][metric_name] = {
                    "score": score,
                    "passed": passed,
                    "reason": reason
                }

                # æ‰“å°ç»“æœ
                status = "âœ…" if passed else "âŒ"
                print(f"  {status} {metric_name}: {score:.2%}")

            except Exception as e:
                print(f"  âš ï¸ {metric_name}: è¯„ä¼°å¤±è´¥ - {str(e)}")
                result["metrics"][metric_name] = {
                    "score": 0,
                    "passed": False,
                    "reason": str(e)
                }

        return result

    def _evaluate_tools(self, case: MongoTestCase) -> Dict:
        """è¯„ä¼°å·¥å…·ä½¿ç”¨"""
        actual_tools = []
        for tc in case.tool_calls:
            if "function" in tc:
                actual_tools.append(tc["function"]["name"])

        expected_tools = get_expected_tools(case.input)

        # è®¡ç®—å‡†ç¡®ç‡
        if not expected_tools:
            accuracy = 1.0
        else:
            correct = len(set(actual_tools) & set(expected_tools))
            accuracy = correct / len(expected_tools) if expected_tools else 0

        return {
            "actual": actual_tools,
            "expected": expected_tools,
            "accuracy": accuracy,
            "passed": accuracy >= CUSTOM_THRESHOLDS["tool_accuracy"]
        }

    def _evaluate_performance(self, case: MongoTestCase) -> Dict:
        """è¯„ä¼°æ€§èƒ½"""
        perf = {}

        if case.metrics_data:
            tokens = case.metrics_data.get("total_tokens", 0)
            duration = case.metrics_data.get("duration", 0)

            perf["tokens"] = tokens
            perf["duration"] = duration
            perf["tokens_passed"] = tokens <= CUSTOM_THRESHOLDS["token_limit"]
            perf["duration_passed"] = duration <= CUSTOM_THRESHOLDS["response_time"]

        return perf

    def generate_summary(self, results: Dict) -> None:
        """ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š"""
        if not results:
            return

        print(f"\n{'='*60}")
        print(f"ğŸ“ˆ è¯„ä¼°æ±‡æ€»")
        print(f"{'='*60}\n")

        # è®¡ç®—æ±‡æ€»ç»Ÿè®¡
        total = len(results)
        metrics_summary = {}

        for test_id, result in results.items():
            for metric_name, metric_result in result["metrics"].items():
                if metric_name not in metrics_summary:
                    metrics_summary[metric_name] = {
                        "passed": 0,
                        "failed": 0,
                        "scores": []
                    }

                if metric_result["passed"]:
                    metrics_summary[metric_name]["passed"] += 1
                else:
                    metrics_summary[metric_name]["failed"] += 1

                metrics_summary[metric_name]["scores"].append(metric_result["score"])

        # æ‰“å°æ±‡æ€»
        print(f"æ€»æµ‹è¯•ç”¨ä¾‹æ•°: {total}\n")

        print("æŒ‡æ ‡é€šè¿‡ç‡:")
        for metric_name, summary in metrics_summary.items():
            pass_rate = summary["passed"] / total * 100
            avg_score = sum(summary["scores"]) / len(summary["scores"])
            print(f"  {metric_name}:")
            print(f"    - é€šè¿‡ç‡: {pass_rate:.1f}% ({summary['passed']}/{total})")
            print(f"    - å¹³å‡åˆ†: {avg_score:.2%}")

        # å·¥å…·å‡†ç¡®ç‡
        tool_accuracies = [r["tools"]["accuracy"] for r in results.values()]
        if tool_accuracies:
            avg_tool_accuracy = sum(tool_accuracies) / len(tool_accuracies)
            print(f"\nå·¥å…·è°ƒç”¨å‡†ç¡®ç‡: {avg_tool_accuracy:.1%}")

        # æ€§èƒ½ç»Ÿè®¡
        durations = [r["performance"].get("duration", 0) for r in results.values() if r["performance"]]
        tokens = [r["performance"].get("tokens", 0) for r in results.values() if r["performance"]]

        if durations:
            print(f"\næ€§èƒ½ç»Ÿè®¡:")
            print(f"  å¹³å‡å“åº”æ—¶é—´: {sum(durations)/len(durations):.2f}ç§’")
            print(f"  æœ€å¤§å“åº”æ—¶é—´: {max(durations):.2f}ç§’")

        if tokens:
            print(f"  å¹³å‡Tokenä½¿ç”¨: {sum(tokens)/len(tokens):.0f}")
            print(f"  æœ€å¤§Tokenä½¿ç”¨: {max(tokens)}")

    def cleanup(self):
        """æ¸…ç†èµ„æº"""
        self.extractor.close()


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="è¿è¡ŒAgentè¯„ä¼°")

    parser.add_argument(
        "--mode",
        choices=["recent", "pattern", "report"],
        default="recent",
        help="è¯„ä¼°æ¨¡å¼"
    )
    parser.add_argument(
        "--hours",
        type=int,
        default=24,
        help="æŸ¥çœ‹æœ€è¿‘Nå°æ—¶çš„æ•°æ®ï¼ˆmode=recentæ—¶ï¼‰"
    )
    parser.add_argument(
        "--limit",
        type=int,
        default=5,
        help="è¯„ä¼°çš„æµ‹è¯•ç”¨ä¾‹æ•°é‡"
    )
    parser.add_argument(
        "--pattern",
        type=str,
        help="æœç´¢æ¨¡å¼ï¼ˆmode=patternæ—¶ï¼‰"
    )
    parser.add_argument(
        "--output",
        type=str,
        help="ä¿å­˜ç»“æœåˆ°æ–‡ä»¶"
    )
    parser.add_argument(
        "--with-tools",
        action="store_true",
        help="åªè¯„ä¼°æœ‰å·¥å…·è°ƒç”¨çš„æµ‹è¯•ç”¨ä¾‹"
    )

    args = parser.parse_args()

    # åˆ›å»ºè¯„ä¼°å™¨
    evaluator = QuickEvaluator()

    try:
        results = {}

        if args.mode == "recent":
            # è¯„ä¼°æœ€è¿‘çš„è¿è¡Œ
            results = evaluator.evaluate_recent(
                hours=args.hours,
                limit=args.limit,
                with_tools=args.with_tools
            )

        elif args.mode == "pattern" and args.pattern:
            # æŒ‰æ¨¡å¼è¯„ä¼°
            results = evaluator.evaluate_by_pattern(args.pattern, limit=args.limit)

        elif args.mode == "report":
            # ç”Ÿæˆå®Œæ•´æŠ¥å‘Š
            print("ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š...")
            recent_results = evaluator.evaluate_recent(hours=24, limit=10)
            results.update(recent_results)

        # ç”Ÿæˆæ±‡æ€»
        evaluator.generate_summary(results)

        # ä¿å­˜ç»“æœ
        if args.output and results:
            output_path = Path(args.output)
            output_path.write_text(json.dumps(results, indent=2, ensure_ascii=False))
            print(f"\nâœ… ç»“æœå·²ä¿å­˜åˆ°: {output_path}")

    finally:
        evaluator.cleanup()


if __name__ == "__main__":
    main()